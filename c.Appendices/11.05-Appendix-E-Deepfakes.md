
Appendix E: Deepfake Generation and Detection - A 2025 Technical Review


I. Introduction: The New Reality of Synthetic Media

The term "deepfake"—a portmanteau of "deep learning" and "fake"—refers to media that has been generated or synthetically altered using artificial intelligence (AI) techniques. Once a niche concept confined to research labs, deepfake technology has rapidly evolved into a powerful, dual-use tool that is reshaping the digital landscape. Its proliferation presents a profound societal challenge, blurring the lines between reality and artifice and fundamentally threatening the trust that underpins digital communication. This appendix provides a comprehensive technical review of the state of deepfake generation and detection as of 2025, exploring the architectural evolution of generative models, the escalating arms race between creation and detection, the real-world impact of this technology, and the multi-layered strategies required to mitigate its risks.

The Dual-Use Dilemma

Deepfake technology embodies a classic dual-use dilemma, possessing significant potential for both benevolent and malevolent applications. On one hand, its capabilities have opened new frontiers in creative industries and accessibility. In entertainment, deepfakes can facilitate realistic video dubbing of foreign films, create digital avatars for virtual reality, and even de-age actors or resurrect historical figures for educational purposes.1 The technology offers powerful tools for artists and content creators, enabling novel forms of expression and storytelling.1 In the realm of accessibility, voice cloning technologies can synthesize speech for individuals who have lost their ability to speak, restoring a fundamental aspect of their identity.1
On the other hand, the same technology is a formidable weapon in the hands of malicious actors. Deepfakes are now routinely used to fabricate messages from politicians, create non-consensual pornographic content, spread disinformation, and damage reputations.1 This dark side of deepfakes poses a direct threat to personal privacy, democratic processes, and even national security.1 The ease with which convincing forgeries can be created and disseminated has led to a crisis of authenticity, where the potential for deception undermines the integrity of the entire information ecosystem.

Commodification and Democratization

A critical factor accelerating the deepfake threat is the profound shift from resource-intensive, expert-driven processes to widely available, user-friendly tools. The complex and computationally expensive methods of just a few years ago have been "commoditized," making sophisticated AI capabilities accessible to a global audience of non-technical users.7 As of 2024, thousands of software tools and online services are available for AI-driven face swapping, lip-syncing, image generation, and voice cloning. Many of these tools allow users to generate synthetic media simply by describing the desired outcome in a text prompt or by providing a single photograph and a brief audio sample.7
This "democratization" of generative AI has dramatically lowered the barrier to entry for creating high-quality synthetic media. Consequently, cybercriminals, hacktivists, state-sponsored influence operations, and purveyors of fake news have rapidly incorporated these technologies into their attack frameworks.7 The commodification of deepfake generation represents a fundamental change in the economics and velocity of malicious campaigns. What was once a bespoke, high-effort activity has become a scalable, low-cost operation. This transition has led to a dramatic expansion of the threat surface area for businesses, governments, and individuals, as attackers no longer need to be AI experts to execute sophisticated deception campaigns. The result is a flood of synthetic content that threatens to overwhelm traditional, manual methods of verification and defense, making automated and scalable solutions an urgent necessity.8

The Societal Challenge

The core societal challenge posed by deepfakes is the generation of synthetic media that is, in many cases, indistinguishable from reality to human perception.4 Research has shown that people are increasingly unable to determine whether media is AI-generated or authentic.12 This creates a fertile ground for what is known as the "liar's dividend": a phenomenon where the mere existence of deepfake technology allows malicious actors to dismiss genuine, inconvenient evidence as a fabrication.13 As public awareness of deepfakes grows, so does skepticism toward all digital content, eroding the foundational trust necessary for informed public discourse, journalism, and legal evidence.6
This erosion of trust has severe consequences. It can undermine the credibility of democratic institutions, manipulate political debate, and tarnish personal and corporate reputations with fabricated scandals.6 The challenge is no longer merely about identifying individual fakes but about rebuilding and preserving the integrity of the digital information ecosystem itself. Addressing this requires a multi-faceted approach encompassing advanced technological detection, robust standards for content authenticity, effective legislation, and widespread digital literacy education.

II. The Evolving Architecture of Deepfake Generation

The rapid advancement in the realism and complexity of deepfakes is a direct result of the swift evolution of the underlying generative model architectures. Over the past decade, the field has progressed from foundational techniques capable of simple image manipulation to sophisticated systems that can generate coherent, high-definition, and even interactive 3D-aware videos from scratch. This progression represents not just an incremental improvement in quality but a fundamental shift in the nature of synthetic media—from faking 2D pixels to simulating a 4D reality of space and time.

A. Foundational Models: GANs and Variational Autoencoders (VAEs)

The first wave of convincing deepfakes was powered primarily by two types of deep learning models: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).
Generative Adversarial Networks (GANs), introduced in 2014, consist of two neural networks locked in a competitive, or adversarial, process.1 A "generator" network creates synthetic data (e.g., an image), while a "discriminator" network attempts to distinguish this fake data from real data. The generator's goal is to fool the discriminator, and the discriminator's goal is to correctly identify fakes. Through this continuous feedback loop, the generator becomes progressively better at producing highly realistic outputs that can deceive both the discriminator and, ultimately, human observers.16 GANs were the engine behind many early face-swapping applications and demonstrated a remarkable ability to synthesize photorealistic faces.1
Variational Autoencoders (VAEs) operate on a different principle. A VAE consists of an encoder, which learns to compress input data into a low-dimensional latent space (a compact representation of the data's key features), and a decoder, which learns to reconstruct the original data from this latent representation.15 By manipulating vectors within this learned latent space, VAEs can be used to generate new data variations. In the context of deepfakes, VAEs have been instrumental in tasks like face swapping, where the facial features of a source person are encoded and then decoded onto the facial structure of a target person.18
While newer architectures have surpassed them in many respects, GANs and VAEs remain relevant. They are still actively researched and often integrated into more complex generative pipelines, particularly for tasks like fine-grained facial attribute editing or one-shot face swapping.18

B. The Diffusion Revolution: A Paradigm Shift in Realism

Beginning around 2020, a new class of models known as Denoising Diffusion Probabilistic Models (DDPMs), or simply diffusion models, emerged and quickly became the state-of-the-art for high-fidelity media generation.20 The rise of diffusion models has been a primary driver of the current "explosive era of Artificial Intelligence Generated Content (AIGC)".23
Unlike GANs, which generate an image in a single step, diffusion models work through an iterative refinement process.16 The process begins with a "forward diffusion" stage, where a real image is gradually corrupted by adding small amounts of Gaussian noise over many time steps until it becomes pure, unstructured noise. The model then learns to reverse this process. During generation, the model starts with random noise and, guided by a prompt (e.g., text or an image), iteratively denoises it over a sequence of steps, gradually forming a coherent and detailed output.24
This iterative approach has proven to be more stable to train than GANs and is capable of producing images and videos with superior quality, diversity, and fidelity to the input prompt. This technological leap is responsible for the stunning capabilities of recent landmark text-to-video models. For example, OpenAI's Sora and Google's Veo can generate videos up to a minute long in high-definition (1080p) resolution, demonstrating unprecedented temporal coherence, object permanence, and understanding of physical plausibility.10 These models mark a significant departure from the short, often glitchy clips of the past, enabling the creation of complex, narrative-driven synthetic videos that were previously unattainable.

C. Transformers as the Engine for Coherent Generation

The scaling and performance of modern diffusion models have been made possible by another key architectural innovation: the Transformer. Originally developed for natural language processing, the Transformer architecture has proven to be exceptionally effective for generative tasks due to its "attention mechanism," which allows the model to weigh the importance of different parts of the input data when producing an output.
A breakthrough architecture known as the Diffusion Transformer (DiT) replaces the traditional U-Net convolutional network backbone in diffusion models with a Transformer.25 The DiT operates on "patches" of the latent representation of an image or video, treating them as a sequence of tokens, similar to how a language model processes words in a sentence.24 This approach has been shown to scale more effectively with increased model size and computational resources, leading to significant improvements in generation quality.
The DiT architecture is the engine behind leading models like Sora.25 Its attention mechanism is particularly crucial for video generation, as it can effectively model long-range dependencies across both space and time ("spacetime patches"). This enables the model to maintain temporal coherence over longer video sequences, ensuring that objects and characters behave consistently from one frame to the next—a major historical challenge for video synthesis.24

D. Entering the Third Dimension: NeRFs, 3D Gaussian Splatting, and 4D Synthesis

The latest frontier in deepfake generation moves beyond 2D video into the synthesis of interactive 3D and 4D content. This evolution represents another paradigm shift, moving from creating a fixed-perspective fake video to generating an entire synthetic reality that can be explored.
Neural Radiance Fields (NeRFs) are a revolutionary technique for synthesizing novel views of a 3D scene.26 A NeRF model learns a continuous, volumetric representation of a scene by training a neural network to map a 5D coordinate—comprising a 3D spatial location (
x,y,z) and a 2D viewing direction (θ,ψ)—to a color and volume density.27 By training on a collection of 2D images of a scene from known camera positions, a NeRF can render photorealistic new views from any angle, effectively creating a fully explorable 3D environment.28 When applied to creating deepfakes, NeRFs allow for the generation of a 3D avatar or scene that is view-consistent, meaning a user can change the camera perspective in real-time, a feat impossible with traditional 2D video techniques.26
While powerful, NeRFs can be computationally intensive and slow to render. 3D Gaussian Splatting (GS) has emerged as a popular alternative that achieves similar or better visual quality with significantly faster training and real-time rendering speeds.26 Instead of a neural network, GS represents a scene as a collection of 3D Gaussians, each with properties like position, shape, color, and opacity.27
Frameworks like ImplicitDeepfake are already combining these 3D techniques with traditional 2D deepfake methods. In this approach, a 2D face-swapping algorithm is applied to the input images used to train a NeRF or GS model, resulting in a plausible, high-quality 3D deepfake avatar that can be generated from just a single target image.18 Further extending this concept are
4D generative models, which explicitly incorporate a temporal dimension (t) alongside the 3D spatial coordinates (x,y,z). These models can learn to generate 3D-aware videos that are continuous in both space and time, allowing for simultaneous control over camera viewpoint and temporal progression.29
This evolution from 2D pixel manipulation to the simulation of 4D reality has profound implications. Detection methods that rely on finding 2D artifacts, such as inconsistent shadows or boundary blending errors, are likely to become obsolete against fakes rendered from a coherent underlying 3D model. Future detectors will face the much more difficult challenge of identifying subtle physical, geometric, or lighting inconsistencies within the synthesized 3D world itself.

E. The Synthetic Voice: Milestones in Audio Cloning

Parallel to the advancements in visual synthesis, AI-powered voice generation has achieved a level of realism that is equally impressive and concerning. The technology has evolved from robotic-sounding text-to-speech (TTS) systems to models that can clone a human voice with stunning accuracy from just a few seconds of audio.
Early pioneering models like DeepMind's WaveNet used deep neural networks to generate raw audio waveforms one sample at a time, resulting in a significant leap in naturalness compared to previous methods.30 However, the current state-of-the-art is defined by models like Microsoft's
VALL-E family, which represents a fundamental shift in approach.31
VALL-E treats TTS as a language modeling task. It first uses a neural audio codec to convert a speech waveform into a sequence of discrete "tokens," much like words in a text sentence. It then trains a large language model (a Transformer) to predict these audio tokens based on an input text and a short (e.g., 3-second) audio prompt of a target speaker's voice.30 This "in-context learning" capability allows VALL-E to perform
zero-shot voice cloning, meaning it can synthesize speech in the voice of a speaker it has never been trained on, using only the brief audio prompt.32
The latest iteration, VALL-E 2, has pushed the boundaries even further. By introducing techniques like Repetition Aware Sampling to improve stability and Grouped Code Modeling for greater efficiency, VALL-E 2 became the first system to achieve human parity in zero-shot TTS on standard academic benchmarks like LibriSpeech and VCTK.3 These models are not just replicating the timbre of a voice; they are capable of preserving the speaker's emotion, accent, and even the acoustic environment (e.g., background noise, reverberation) of the prompt audio, making the resulting synthetic speech incredibly convincing and difficult to distinguish from an authentic recording.30
Table 1: Evolution of Deepfake Generation Architectures








Architecture
Core Principle
Key Models/Examples
Strengths
Limitations/Artifacts
GANs (Generative Adversarial Networks)
An adversarial process between a generator and a discriminator network to produce realistic media.
StyleGAN, FaceSwap, DeepFaceLab
High-quality image synthesis, especially for faces. Widely used for face swapping.
Training instability; mode collapse; artifacts like unnatural teeth, hair, and inconsistent backgrounds.
VAEs (Variational Autoencoders)
An encoder compresses data into a latent space, and a decoder reconstructs it. Generation occurs by sampling from the latent space.
CodeSwap, SelfSwapper
Stable training; good for learning compressed data representations; used in identity-aware face swapping.
Often produces blurrier or less sharp images compared to GANs; less photorealistic.
Diffusion Models (DDPMs)
Learns to reverse a noise-adding process, iteratively denoising a random signal into a coherent output.
DALL-E 3, Midjourney, Stable Diffusion, Sora, Veo
State-of-the-art realism and diversity; stable training; excellent at text-to-image and text-to-video generation.
Computationally intensive and slow inference (though improving); can struggle with fine-grained details like hands.
Transformers
Uses attention mechanisms to model long-range dependencies in sequence data.
Diffusion Transformer (DiT), VALL-E 2
Enables scaling of generative models; crucial for temporal coherence in long videos and logical consistency in complex scenes.
High computational and memory requirements; primarily an architectural component, not a standalone generator.
NeRFs / 3D Gaussian Splatting
Learns a continuous 3D representation of a scene from 2D images, allowing for novel view synthesis.
ImplicitDeepfake, RigNeRF
Generates view-consistent, explorable 3D scenes, not just 2D videos; enables interactive fakes.
High data requirements for training; NeRFs can be slow to render; artifacts related to inconsistent lighting or geometry.


III. The Detection Dilemma: An Escalating Arms Race

The rapid progress in deepfake generation has triggered a corresponding effort to develop technologies capable of detecting them. However, this dynamic has devolved into a perpetual and asymmetrical arms race. As soon as a new detection method identifies a specific artifact or inconsistency, generative models are updated to eliminate that very flaw. This adversarial loop, combined with the sheer volume and diversity of synthetic media, means that detection technologies are fundamentally in a reactive posture, constantly struggling to keep pace with the ever-advancing frontier of generation.

A. The Generalization Gap: Why Lab-Tested Detectors Fail in the Wild

A central challenge in deepfake detection is the problem of generalization. For years, researchers have developed detectors that report near-perfect accuracy on controlled, academic datasets.12 However, a growing body of evidence shows that these models fail dramatically when deployed in the real world against the diverse and messy content circulating on the internet.4 These academic benchmarks are often outdated, lack diversity, and are not representative of the myriad generation techniques and platform-specific distortions (e.g., compression, resizing) found "in the wild".4
In-depth Analysis: The Deepfake-Eval-2024 Benchmark
The performance gap between lab and reality was starkly quantified by the Deepfake-Eval-2024 benchmark, a landmark 2024 study that has become a critical reference point for the field.4 Unlike its predecessors, this benchmark was specifically designed to test detectors against contemporary, real-world deepfakes.
Dataset Composition: The Deepfake-Eval-2024 dataset is a large, multi-modal collection comprising 45 hours of video, 56.5 hours of audio, and nearly 2,000 images.4 Crucially, all content was collected "in-the-wild" in 2024 from user submissions to deepfake detection platforms and from content moderation forums across 88 different websites.4 The dataset is exceptionally diverse, containing media in 52 different languages and featuring a wide array of modern manipulation techniques, including lip-sync, face swaps, and diffusion-based generation.4 This composition makes it a far more challenging and realistic testbed than older, synthetically generated datasets that often feature paid actors in sterile, controlled settings.34
The Performance Collapse: The study's central finding was a precipitous collapse in the performance of state-of-the-art, open-source detection models. When evaluated on Deepfake-Eval-2024, the Area Under the Curve (AUC)—a key metric of a model's discriminative ability—decreased by an average of 50% for video detectors, 48% for audio detectors, and 45% for image detectors compared to their published scores on older academic benchmarks.4 This stunning drop in accuracy provides definitive empirical evidence that many existing detection tools are not fit for purpose in the current threat environment.
Human vs. Machine: The study also found that while commercial detection models and open-source models that were fine-tuned on the new dataset showed superior performance, they still did not reach the accuracy of trained human deepfake forensic analysts.4 This suggests that human expertise in identifying subtle, contextual, and forensic inconsistencies remains a critical, and currently unmatched, component of deepfake analysis.
Error Analysis: A detailed error analysis revealed specific weaknesses in current detectors. Models particularly struggled with videos featuring selective facial manipulation (e.g., only altering the mouth) and non-facial manipulations.37 They performed poorly on videos generated by diffusion models and on images containing text overlays, which are common in social media memes.37 For audio, the presence of non-English speech, background music, or silence significantly degraded performance, highlighting that models are often overfitted to the clean, English-centric data of older benchmarks.39
The sobering results from Deepfake-Eval-2024 signal that the strategy of building "universal" passive detectors that can reliably spot any fake is becoming increasingly untenable. The diversity of generation methods, content types, and distribution channels creates a constantly shifting landscape of artifacts. A detector trained to spot the fingerprints of a GAN will likely fail against a diffusion model, and one trained on pristine video will fail against a heavily compressed clip from TikTok. This reality suggests that the future of detection lies not in a single "silver bullet" but in a hybrid, multi-layered, and probabilistic approach to assessing authenticity.

Table 2: Performance of SOTA Detectors on Academic vs. In-the-Wild Benchmarks










Modality
Academic Benchmark
SOTA Model AUC (Academic)
In-the-Wild Benchmark
SOTA Model AUC (In-the-Wild)
Performance Drop (%)
Video
FaceForensics++, etc.
~0.95 - 0.99
Deepfake-Eval-2024
~0.47
~50%
Audio
ASVspoof, etc.
~0.92 - 0.98
Deepfake-Eval-2024
~0.50
~48%
Image
Various Synthetic Sets
~0.90 - 0.97
Deepfake-Eval-2024
~0.52
~45%
Source: Based on findings from Chandra et al., 2024/2025 4












B. Passive Detection Techniques: A Moving Target

Passive detection involves analyzing a piece of media itself to find intrinsic evidence of forgery. These techniques can be broadly categorized by the types of artifacts they seek.
Biometric and Physiological Cues: Early detectors focused on subtle biological inconsistencies that generative models struggled to replicate perfectly. These included unnatural eye blinking patterns, a lack of subtle flickering, mismatched head movements, or poor temporal coherence between frames.33 However, generative models have rapidly improved in maintaining temporal consistency.19 A more advanced technique involves analyzing remote photoplethysmography (rPPG) signals, which measure minute changes in skin color caused by blood flow to detect a person's pulse. For a time, the absence of a realistic pulse was a reliable tell for deepfakes. However, a 2025 study demonstrated that state-of-the-art deepfakes can now successfully replicate a realistic heartbeat, either by inheriting it from a source video or adding it intentionally.40 The new frontier for this method is detecting a lack of
physiologically realistic spatial and temporal variations in blood flow across different regions of the face, which current fakes still struggle with.40
Algorithmic Fingerprints and Latent Space Analysis: Every generative algorithm leaves subtle, often imperceptible, "fingerprints" on the media it creates. These can manifest as anomalies in the frequency domain (e.g., from upsampling) or as inconsistencies in compression artifacts.33 While early methods were effective, more sophisticated generators have learned to mimic realistic compression patterns and avoid common frequency artifacts. A more novel and robust approach focuses on analyzing high-level features rather than low-level pixels. For instance, recent research has found that generated videos exhibit unnatural temporal variations in their
style latent vectors—the high-level codes that control facial appearance, expression, and geometry in a GAN.19 A framework proposed in 2024 introduces a
StyleGRU module, which uses a Gated Recurrent Unit (GRU) network trained with contrastive learning to specifically model the "flow" of these style vectors over time. It can effectively detect the suppressed variance and unnatural smoothness characteristic of generated facial animations, making it more robust to unseen manipulation techniques.19
Architectural Developments: The research community continues to explore different neural network architectures for the detection task. While Convolutional Neural Networks (CNNs) have traditionally dominated and often outperform other architectures in this specific domain, there is growing interest in adapting Vision Transformers (ViTs) for deepfake detection.41 ViTs have achieved state-of-the-art performance in general image classification, but have lagged in detection. To address this, frameworks like
FakeFormer have been proposed. FakeFormer extends the ViT architecture by introducing an explicit attention mechanism guided by artifact-vulnerable patches (e.g., facial boundaries, eyes, mouth), forcing the model to focus on the subtle, localized inconsistencies that are characteristic of forgeries.41

C. Proactive Defense: Shifting from Reaction to Prevention

Given the inherent challenges of passive detection, a paradigm shift is underway towards proactive defense strategies. Instead of trying to find evidence of forgery after the fact, these methods aim to embed proof of authenticity or markers of synthesis into content from the moment of its creation. This approach fundamentally alters the dynamics of the arms race, moving the burden of proof from the verifier (who must prove a piece of media is fake) to the creator (who has the option to prove their content is authentic). For trusted sources, this enables an "authenticity by default" model.
Content Provenance and the C2PA Standard: The most prominent initiative in this space is the Coalition for Content Provenance and Authenticity (C2PA), an open-standard-setting body founded by a consortium of major technology and media companies, including Adobe, Microsoft, Google, Intel, and the BBC.42
How it Works: C2PA provides a technical standard for creating what it calls "Content Credentials".42 This is a tamper-evident digital manifest that is cryptographically signed and embedded within a media file. This manifest acts as a "digital nutrition label," securely logging the content's provenance—who created it, when, and with what tools—as well as its complete edit history.45 Any alteration to the file or its manifest will break the cryptographic signature, immediately flagging it as tampered with.46
Adoption and Use Cases: The C2PA standard is gaining traction. News organizations like the BBC are using it to validate footage from conflict zones, attaching credentials to verified videos to alert viewers to misleading edits, such as dubbed audio.47 Some generative AI tools, like OpenAI's DALL-E, are now automatically embedding C2PA manifests in their output to clearly label the content as AI-generated, providing crucial context.44
Critical Limitations: Despite its promise, C2PA is not a panacea. Its primary weakness is that the embedded metadata can be easily stripped from a file through simple actions like taking a screenshot or re-uploading it to a platform that doesn't preserve metadata.45 Its effectiveness also depends on voluntary, widespread adoption by device manufacturers, software developers, and online platforms, which is still in its early stages.43 Furthermore, malicious actors creating bespoke deepfakes with tools outside the C2PA ecosystem will simply not include credentials, leaving their content unflagged.44
Forensic Watermarking and Data Poisoning: To address the fragility of metadata, more robust proactive techniques are being developed.
Invisible Watermarking: This approach embeds an imperceptible signal directly into the pixels or frequency domain of a media file. Technologies like DeepMind's SynthID and Steg AI use deep learning to create watermarks that are robust to common distortions like compression, cropping, and color adjustments.11 Unlike C2PA metadata, these watermarks are part of the content itself and are much harder to remove without significantly degrading the media's quality.50 This technique can serve as an embedded defense, allowing for real-time detection and traceability.50 In some implementations, the invisible watermark can even be used to store a link to a remote C2PA manifest, allowing for its recovery even if the file's metadata has been stripped.11
Data Poisoning: This is a more adversarial approach that aims to disrupt the deepfake generation process at its source. Tools like Nightshade subtly alter images online before they can be scraped for AI training datasets. These alterations are invisible to the human eye but cause generative models trained on this "poisoned" data to produce flawed, distorted, or unpredictable outputs, effectively sabotaging their ability to create convincing fakes.49
This shift towards proactive defense creates the potential for a two-tiered information ecosystem: one containing content with verifiable, cryptographically secured provenance, and another containing everything else. While this does not stop the creation of malicious fakes, it provides a powerful mechanism for trusted sources to establish a chain of custody for their content. Over time, the absence of a Content Credential or a verifiable watermark from a source that is expected to provide one—such as a major news agency or a government body—could itself become a significant red flag for audiences and analysts. The central question of verification begins to shift from "Is this fake?" to the more revealing "Why can't this be authenticated?".
Table 3: Comparative Analysis of Proactive Defense Mechanisms








Mechanism
Principle of Operation
Strengths
Weaknesses/Vulnerabilities
Key Proponents/Examples
C2PA Content Credentials
Embeds a cryptographically signed, tamper-evident metadata manifest tracking content origin and edit history.
Open standard with broad industry backing; provides rich contextual information; transparent to end-users.
Metadata can be easily stripped (e.g., via screenshot); relies on voluntary, opt-in adoption; does not prevent creation of un-credentialed fakes.
Adobe, Microsoft, Google, BBC, OpenAI (DALL-E)
Invisible Watermarking
Embeds a robust, imperceptible signal directly into the content's pixels or frequency domain using deep learning.
More robust to distortions than metadata; harder to remove without degrading content; can be used to recover stripped metadata.
Can be degraded by severe transformations; requires proprietary technology for embedding/reading; potential for adversarial removal attacks.
DeepMind (SynthID), Steg AI, various academic proposals
Data Poisoning
Adversarially modifies training data to cause generative models to fail or produce flawed outputs.
Disrupts the generation process itself; acts as a deterrent for data scrapers; protects artists and creators.
Requires widespread adoption to be effective; may be circumvented by more robust training techniques; a "scorched earth" approach.
University of Chicago (Nightshade, PhotoGuard)


IV. Deepfakes in the Real World: A 2024-2025 Case Study Analysis

The theoretical capabilities of deepfake technology have translated into tangible, high-impact threats across finance, politics, and personal security. An analysis of prominent incidents from 2024 and 2025 reveals not only the increasing technical sophistication of these forgeries but also the refined social engineering tactics used to deploy them. The most effective attacks are often those that exploit existing human and systemic vulnerabilities, using the deepfake as a catalyst to manipulate trust, urgency, and authority.

A. The New Face of Financial Crime

Deepfake technology has enabled a new and alarming evolution in financial fraud, moving from text-based phishing and simple voice scams to highly convincing, real-time audio-visual impersonations.
Case Study: The $25 Million Arup Video Conference Fraud: In a landmark case from early 2024, a finance worker at the global engineering firm Arup was tricked into transferring approximately $25.6 million to fraudsters.52 The attack was executed via a multi-person video conference call in which the employee believed they were speaking with the company's UK-based Chief Financial Officer and other senior staff members. In reality, all other participants on the call were deepfakes.9 This incident was a watershed moment, demonstrating the viability of using sophisticated, real-time, multi-person deepfakes to execute high-value corporate fraud. The attack's success relied not just on the visual realism of the fakes but on the social engineering element of a seemingly legitimate, urgent, and confidential business request from a figure of authority.
Case Study: Widespread Investment and Giveaway Scams: A more common but highly effective form of deepfake fraud involves the use of cloned voices and likenesses of public figures to promote scams. Throughout 2024, deepfake videos of high-profile entrepreneurs like Elon Musk were used in widespread campaigns on social media, promoting fraudulent cryptocurrency giveaways and investment schemes.55 These videos, often appearing as ads on platforms like Facebook and TikTok, have successfully defrauded victims of sums ranging from thousands to hundreds of thousands of dollars by leveraging the perceived credibility of the impersonated individual.55
Statistical Overview: These case studies are indicative of a rapidly escalating trend. According to Veriff's 2025 Identity Fraud Report, 1 in every 20 identity verification failures is now linked to deepfakes, and overall fraud attempts grew 21% year-over-year.8 A 2025 report from Pindrop, analyzing over a billion calls to contact centers, found a
680% year-over-year rise in deepfake activity and projected that deepfake-related fraud could surge by another 162% in 2025, with total contact center fraud losses potentially reaching $44.5 billion.57 In 2025 alone, the FBI noted that deepfake-enabled fraud resulted in over $200 million in losses.56

B. Weaponized Narratives: Deepfakes in Global Politics

The potential for deepfakes to disrupt democratic processes has been a subject of intense concern, and 2024 provided numerous examples of their use in political contexts around the world.
Case Study: The Joe Biden Robocall and the 2024 U.S. Election Cycle: In January 2024, just before the New Hampshire presidential primary, thousands of voters received a robocall featuring an AI-generated clone of President Joe Biden's voice.55 The message urged Democrats not to vote in the primary, a clear attempt at voter suppression.59 While the immediate impact on the election's outcome was deemed negligible, the incident served as a stark demonstration of how easily and cheaply voice cloning technology can be deployed to interfere in elections and spread political disinformation.
Global Election Interference: The Biden robocall was not an isolated event. A 2024 report documented 82 distinct deepfakes targeting public figures in 38 countries between July 2023 and July 2024, with a significant number aimed at influencing elections.60 In Turkey, President Erdoğan used a deepfake video to falsely link an opposition leader to a terrorist group. In India's general election, deepfakes of celebrities were used to create false endorsements for opposition parties and criticize Prime Minister Narendra Modi.13 In Slovakia, a fake audio clip discussing election manipulation went viral just days before a vote.60 These incidents illustrate the global nature of the threat and the diverse ways in which deepfakes are being weaponized as a tool of political warfare.
The "Apocalypse That Wasn't": While the threat is real, it is important to maintain a nuanced perspective. Many analysts predicted a "misinformation apocalypse" driven by AI that would overwhelm the 2024 elections, but this largely failed to materialize.9 A Meta report claimed that less than 1% of fact-checked misinformation during the 2024 election cycles was AI-generated.9 Furthermore, academic research has suggested that deepfake videos are not necessarily more deceptive than the same false information presented in text or audio formats, and their ability to uniquely sway voters remains unproven.58 This suggests that while deepfakes are a dangerous new tool in the disinformation arsenal, they are currently one among many, and their primary impact may be in contributing to a general climate of distrust rather than directly altering election outcomes.

C. The Human Cost: Harassment and Reputational Warfare

Beyond high-stakes financial and political arenas, one of the most pervasive and damaging uses of deepfake technology is for personal harassment, defamation, and the creation of non-consensual synthetic pornography.
Case Study: The Taylor Swift Incident: In early 2024, explicit, sexually graphic, and entirely fake images of the musician Taylor Swift were generated using AI and spread rapidly across social media platforms like X (formerly Twitter).10 The incident, which garnered tens of millions of views before the content was removed, sparked widespread outrage and highlighted the profound inadequacy of platform moderation policies in dealing with this form of synthetic abuse.59
Disproportionate Targeting: The Taylor Swift case is emblematic of a broader pattern. The vast majority of malicious deepfake content online is non-consensual pornography, and it disproportionately targets women.62 Public figures, including female politicians, journalists, and actresses, are frequent victims, in what amounts to a new, technologically-enhanced form of sexual harassment and reputational attack designed to silence, intimidate, and humiliate.60
Character Assassination: Deepfakes also serve as a potent tool for targeted character assassination and personal revenge. In one 2024 case, a school's athletic director, who was under investigation for theft, allegedly created and disseminated a fake audio clip of the school principal making racist and antisemitic remarks.55 The audio went viral, leading to the principal receiving death threats and being placed on administrative leave before forensic analysis exposed the recording as a fabrication.55 This incident illustrates how easily accessible deepfake tools can be weaponized to destroy careers and lives.

Table 4: High-Profile Deepfake Incidents and Their Technical Signatures (2024-2025)










Incident
Date
Modality
Suspected Generation Technique
Malicious Objective
Key Impact
Arup Corporate Fraud
Early 2024
Video + Audio
Real-time video/audio synthesis (multi-person)
Financial Fraud
$25.6 million loss; demonstrated viability of deepfakes for high-value corporate attacks.
Joe Biden Robocall
Jan. 2024
Audio
Zero-shot voice cloning (e.g., VALL-E style)
Election Interference / Voter Suppression
Triggered FCC investigation; highlighted ease of political disinformation campaigns.
Taylor Swift Images
Jan. 2024
Image
Diffusion models (text-to-image)
Harassment / Non-consensual pornography
Sparked global outrage; exposed gaps in platform moderation and legal frameworks.
Elon Musk Crypto Scams
2024
Video + Audio
Video dialogue synthesis; voice cloning
Financial Fraud
Millions of dollars in losses from individual investors; ongoing threat on social media.
MD School Principal
Jan. 2024
Audio
Voice cloning
Character Assassination / Revenge
Suspension of principal, death threats; criminal charges filed against the creator.
Sources: 52












V. Future Trajectories and A Multi-Layered Mitigation Strategy

The deepfake phenomenon is not a static threat but a dynamic and rapidly evolving challenge. As generative models continue to improve at an exponential rate, the nature of synthetic media will become more complex, immersive, and difficult to discern. Combating this threat effectively requires looking beyond any single "silver bullet" solution and embracing a holistic, multi-layered mitigation strategy that combines technology, policy, education, and industry-wide collaboration.

The Path to Real-Time, Multimodal, and Interactive Fakes

Extrapolating from current research and development trends allows for a projection of the next generation of deepfake capabilities. The future of synthetic media is likely to be defined by three key characteristics:
Real-Time Performance: The ability to generate and manipulate deepfakes in real-time during live video calls or streams is quickly moving from a high-end capability to a commoditized one.8 Tools like DeepFaceLive are already enabling this, and as computational efficiency improves, real-time audio and video impersonation will become a standard feature in the fraudster's toolkit.8
Multimodality: Future deepfakes will seamlessly integrate multiple synthetic modalities. Instead of just a fake face on a real video, attackers will deploy a fully synthetic person (fake face, fake body) in a synthetic environment, speaking with a synthetic voice. The convergence of advanced video, audio, and scene generation models will make these composite fakes far more convincing and harder to debunk.
Interactivity: The emergence of NeRFs, 3D Gaussian Splatting, and 4D generative models points toward a future of interactive fakes.28 Instead of passively watching a fake video, a user might be able to interact with a deepfake avatar in a 3D space, changing their viewing angle and eliciting novel responses. This would represent the ultimate evolution from "fake video" to "synthetic reality," with profound implications for virtual reality, gaming, and remote communication.

Beyond a Silver Bullet: The Need for a Holistic Defense

Given the complexity and rapid evolution of the threat, it is clear that no single technology or policy will be sufficient to solve the deepfake problem. An effective societal response must be a robust, defense-in-depth strategy that integrates multiple mutually reinforcing layers.
Technological Detection: Research must continue to push the boundaries of passive detection, moving away from brittle, artifact-specific methods toward more robust models that can generalize to unseen fakes. This includes focusing on high-level semantic and temporal inconsistencies, such as those captured by frameworks like StyleGRU, and developing new architectures like FakeFormer that are tailored to the detection task.19
Provenance Standards: The widespread adoption and, crucially, enforcement of content provenance standards like C2PA is essential.44 This requires a concerted effort from hardware manufacturers to build C2PA-compliant cameras into devices, from software companies to integrate credentialing into editing tools, and from social media platforms to preserve and display this metadata, flagging content that lacks verifiable origins.6
Proactive Forensics: Because provenance metadata can be stripped, it must be layered with more robust, content-inherent defenses. The integration of invisible forensic watermarking provides a resilient fallback for authentication, allowing for the verification of content even after it has been altered or its metadata has been removed.49
Robust Legislation: A clear and consistent legal framework is needed to deter and punish the malicious creation and distribution of deepfakes. While many jurisdictions are enacting laws targeting specific harms like non-consensual pornographic deepfakes or deceptive election communications, these efforts are often fragmented and lag behind the technology.14 The dual-use nature of the underlying technology makes broad regulation challenging, suggesting that laws should focus on penalizing malicious
intent and impact rather than the tools themselves. This legislative fragmentation and technological lag point to the necessity of a more agile, co-regulatory governance model, where industry standards and platform policies serve as the fast-moving first line of defense, backed by slower-moving but powerful legal frameworks.
Public Digital Literacy: Technology and policy alone are insufficient without an educated and critical populace. Public awareness campaigns and digital literacy initiatives are vital to teach citizens how to critically evaluate media, understand the nature of social engineering, and use verification tools.7 Fostering a healthy skepticism without promoting debilitating cynicism is a key educational challenge.

Conclusion: Navigating the "Liar's Dividend" and the Societal Challenge of Eroding Trust

The deepfake challenge extends far beyond the technical realm of bits and pixels. At its core, it is a challenge to the very foundation of shared reality and trust in the digital age. The most insidious long-term danger of this technology may not be the success of any individual fake but the corrosive effect it has on our collective ability to believe what we see and hear. By sowing doubt about the authenticity of all media, deepfakes pay a "liar's dividend," allowing the purveyors of genuine disinformation to dismiss factual evidence as just another forgery.13
Navigating this new reality requires a paradigm shift in how we approach digital content. We must move from a default assumption of trust to a more critical stance of "trust but verify." The suite of mitigation strategies outlined in this appendix—from advanced detection and proactive watermarking to provenance standards and public education—represents the necessary building blocks for a more resilient information ecosystem. However, they are not a permanent solution but rather the tools for managing an ongoing socio-technical problem. The arms race between generation and detection will continue, and the fight to preserve digital trust will require a sustained, collaborative, and adaptive effort from technologists, policymakers, media organizations, and citizens alike for the foreseeable future.
Works cited
Deep Learning for Deepfakes Creation and Detection: A Survey - ResearchGate, accessed on July 23, 2025, https://www.researchgate.net/publication/336055871_Deep_Learning_for_Deepfakes_Creation_and_Detection_A_Survey
The case for content authenticity in an age of disinformation, deepfakes and NFTs | Adobe, accessed on July 23, 2025, https://blog.adobe.com/en/publish/2021/10/22/content-authenticity-in-age-of-disinformation-deepfakes-nfts
VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2406.05370v1
A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2503.02857v4
A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2503.02857v1
Understanding the Impact of AI-Generated Deepfakes on Public Opinion, Political Discourse, and Personal Security in Social Media - IEEE Computer Society, accessed on July 23, 2025, https://www.computer.org/csdl/magazine/sp/2024/04/10552098/1XApkaTs5l6
The State Of Deepfakes 2024, accessed on July 23, 2025, https://5865987.fs1.hubspotusercontent-na1.net/hubfs/5865987/SODF%202024.pdf
Real-time deepfake fraud in 2025: AI-driven scams | Veriff.com, accessed on July 23, 2025, https://www.veriff.com/identity-verification/news/real-time-deepfake-fraud-in-2025-fighting-back-against-ai-driven-scams
Deepfakes proved a different threat than expected. Here's how to defend against them, accessed on July 23, 2025, https://www.weforum.org/stories/2025/01/deepfakes-different-threat-than-expected/
Video Deepfakes are Too Real Now. Here's How to Detect Them. - Reality Defender, accessed on July 23, 2025, https://www.realitydefender.com/insights/video-deepfakes-are-too-real-now
Content Verification and Deepfake Detection - Steg.AI, accessed on July 23, 2025, https://steg.ai/products/content-authentication/
A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2503.02857v2
Gauging the AI Threat to Free and Fair Elections | Brennan Center ..., accessed on July 23, 2025, https://www.brennancenter.org/our-work/analysis-opinion/gauging-ai-threat-free-and-fair-elections
How Do Deepfakes Affect Media Authenticity? - Identity.com, accessed on July 23, 2025, https://www.identity.com/deepfake-ai-how-verified-credentials-enhance-media-authenticity/
Deepfake Generation and Detection: A Benchmark and Survey - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2403.17881v1
Faster Than Lies: Real-time Deepfake Detection using Binary Neural Networks, accessed on July 23, 2025, https://openaccess.thecvf.com/content/CVPR2024W/DFAD/papers/Lanzino_Faster_Than_Lies_Real-time_Deepfake_Detection_using_Binary_Neural_Networks_CVPRW_2024_paper.pdf
State-of-the-art AI-based Learning Approaches for Deepfake Generation and Detection, Analyzing Opportunities, Threading through - SciSpace, accessed on July 23, 2025, https://scispace.com/pdf/state-of-the-art-ai-based-learning-approaches-for-deepfake-5kub2e7lmcu8.pdf
flyingby/Awesome-Deepfake-Generation-and-Detection - GitHub, accessed on July 23, 2025, https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection
Exploiting Style Latent Flows for Generalizing Deepfake Video ..., accessed on July 23, 2025, https://arxiv.org/pdf/2403.06592
[2403.17881] Deepfake Generation and Detection: A Benchmark and Survey - arXiv, accessed on July 23, 2025, https://arxiv.org/abs/2403.17881
Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook, accessed on July 23, 2025, https://www.researchgate.net/publication/386335316_Deepfake_Media_Generation_and_Detection_in_the_Generative_AI_Era_A_Survey_and_Outlook
Deepfake Media Forensics: State of the Art and Challenges Ahead - ResearchGate, accessed on July 23, 2025, https://www.researchgate.net/publication/382797388_Deepfake_Media_Forensics_State_of_the_Art_and_Challenges_Ahead
DF40: Toward Next-Generation Deepfake Detection, accessed on July 23, 2025, https://proceedings.neurips.cc/paper_files/paper/2024/file/34239f60eca7ce9bee5280aaf81362d8-Paper-Datasets_and_Benchmarks_Track.pdf
Diffusion Models for Video Generation | Lil'Log, accessed on July 23, 2025, https://lilianweng.github.io/posts/2024-04-12-diffusion-video/
Video Generation Models Explosion 2024 - Yen-Chen Lin, accessed on July 23, 2025, https://yenchenlin.me/blog/2025/01/08/video-generation-models-explosion-2024/
[2402.06390] Deepfake for the Good: Generating Avatars through Face-Swapping with Implicit Deepfake Generation - arXiv, accessed on July 23, 2025, https://arxiv.org/abs/2402.06390
Deepfake for the Good: Generating Avatars through Face-Swapping with Implicit Deepfake Generation - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2402.06390v2
RigNeRF: A New Deepfakes Method That Uses Neural Radiance Fields - Unite.AI, accessed on July 23, 2025, https://www.unite.ai/rignerf-a-new-deepfakes-method-that-uses-neural-radiance-fields/
[2206.14797] 3D-Aware Video Generation - ar5iv, accessed on July 23, 2025, https://ar5iv.labs.arxiv.org/html/2206.14797
AI Voice Cloning in the U.S.: Innovation or Identity Theft Waiting to Happen? - GoodFirms, accessed on July 23, 2025, https://www.goodfirms.co/blog/ai-voice-cloning-us-innovation-identity-theft
Understanding AI Voice Cloning: What, Why, and How | Resemble AI, accessed on July 23, 2025, https://www.resemble.ai/understanding-ai-voice-cloning/
VALL-E - Microsoft, accessed on July 23, 2025, https://www.microsoft.com/en-us/research/project/vall-e-x/
GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer, accessed on July 23, 2025, https://arxiv.org/html/2307.07036v2
Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024, accessed on July 23, 2025, https://www.researchgate.net/publication/389581656_Deepfake-Eval-2024_A_Multi-Modal_In-the-Wild_Benchmark_of_Deepfakes_Circulated_in_2024
What to Expect from Deepfake Threats and How Likely are We to Develop Effective Detection Tools? - KuppingerCole, accessed on July 23, 2025, https://www.kuppingercole.com/blog/celik/what-to-expect-from-deepfake-threats-and-how-likely-are-we-to-develop-effective-detection-tools
[2503.02857] Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 - arXiv, accessed on July 23, 2025, https://arxiv.org/abs/2503.02857
Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024, accessed on July 23, 2025, https://powerdrill.ai/discover/summary-deepfake-eval-2024-a-multi-modal-in-the-wild-cm7xu20a09k8g07rswtcpfdss
Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 | AI Research Paper Details - AIModels.fyi, accessed on July 23, 2025, https://www.aimodels.fyi/papers/arxiv/deepfake-eval-2024-multi-modal-wild-benchmark
[Literature Review] Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024 - Moonlight | AI Colleague for Research Papers, accessed on July 23, 2025, https://www.themoonlight.io/en/review/deepfake-eval-2024-a-multi-modal-in-the-wild-benchmark-of-deepfakes-circulated-in-2024
Deepfakes now come with a realistic heartbeat, making them harder to unmask - Frontiers, accessed on July 23, 2025, https://www.frontiersin.org/news/2025/04/30/frontiers-imaging-deepfakes-feature-a-pulse
FakeFormer: Efficient Vulnerability-Driven Transformers for Generalisable Deepfake Detection - arXiv, accessed on July 23, 2025, https://arxiv.org/html/2410.21964v2
C2PA | Verifying Media Content Sources, accessed on July 23, 2025, https://c2pa.org/
New Standard Aims to Protect Against Deepfakes - Truepic, accessed on July 23, 2025, https://www.truepic.com/blog/new-standard-aims-to-protect-against-deepfakes
How C2PA, Watermarking, and Nightshade Are Shaping the Battle Against Deepfakes, accessed on July 23, 2025, https://www.netizen.net/news/post/6377/how-c2pa-watermarking-and-nightshade-are-shaping-the-battle-against-deepfakes
Can Metadata Standards Like C2PA Fight Deepfakes? - Built In, accessed on July 23, 2025, https://builtin.com/artificial-intelligence/fighting-deepfakes
Fighting Deepfakes With Content Credentials and C2PA - CMS Wire, accessed on July 23, 2025, https://www.cmswire.com/digital-experience/fighting-deepfakes-with-content-credentials-and-c2pa/
Media Embrace Content Credentials to Fight Deepfakes - Fstoppers, accessed on July 23, 2025, https://fstoppers.com/artificial-intelligence/media-embrace-content-credentials-fight-deepfakes-693004
Deepfake Detection: Provenance Vs. Inference - Reality Defender, accessed on July 23, 2025, https://www.realitydefender.com/insights/provenance-and-inference
Technologies for Authenticating Media in the Context of Deepfakes - IEEE-USA InSight, accessed on July 23, 2025, https://insight.ieeeusa.org/articles/technologies-for-authenticating-media-in-the-context-of-deepfakes/
(PDF) Enhancing Deepfake Detection: Proactive Forensics Techniques Using Digital Watermarking - ResearchGate, accessed on July 23, 2025, https://www.researchgate.net/publication/387042696_Enhancing_Deepfake_Detection_Proactive_Forensics_Techniques_Using_Digital_Watermarking
CMC | Enhancing Deepfake Detection: Proactive Forensics Techniques Using Digital Watermarking - Tech Science Press, accessed on July 23, 2025, https://www.techscience.com/cmc/v82n1/59264
Top 10 Examples of Deepfake Across The Internet - HyperVerge, accessed on July 23, 2025, https://hyperverge.co/blog/examples-of-deepfakes/
Cybercrime: Lessons learned from a $25m deepfake attack - The World Economic Forum, accessed on July 23, 2025, https://www.weforum.org/stories/2025/02/deepfake-ai-cybercrime-arup/
Deepfake detection in 2024: Why it matters and how to fight it? - Pi-labs, accessed on July 23, 2025, https://pi-labs.ai/deepfake-detection-in-2024-why-it-matters-and-how-to-fight-it/
Top 5 Cases of AI Deepfake Fraud From 2024 Exposed | Blog | Incode, accessed on July 23, 2025, https://incode.com/blog/top-5-cases-of-ai-deepfake-fraud-from-2024-exposed/
AI Fraud Tops $200 Million in 2025 as TruthScan Debuts New Detection Suite - Nasdaq, accessed on July 23, 2025, https://www.nasdaq.com/articles/ai-fraud-tops-200-million-2025-truthscan-debuts-new-detection-suite
Deepfake Fraud Could Surge 162% in 2025 | Pindrop, accessed on July 23, 2025, https://www.pindrop.com/article/deepfake-fraud-could-surge/
Political deepfake videos no more deceptive than other fake news, research finds, accessed on July 23, 2025, https://source.washu.edu/2024/08/political-deepfake-videos-no-more-deceptive-than-other-fake-news-research-finds/
Top 10 Terrifying Deepfake Examples - Arya.ai, accessed on July 23, 2025, https://arya.ai/blog/top-deepfake-incidents
2024 Deepfakes and Election Disinformation Report: Key Findings ..., accessed on July 23, 2025, https://www.recordedfuture.com/research/targets-objectives-emerging-tactics-political-deepfakes
The apocalypse that wasn't: AI was everywhere in 2024's elections ..., accessed on July 23, 2025, https://ash.harvard.edu/articles/the-apocalypse-that-wasnt-ai-was-everywhere-in-2024s-elections-but-deepfakes-and-misinformation-were-only-part-of-the-picture/
Narrative Attack and Deepfake Scandals Expose AI's Threat to Celebrities, Executives, and Influencers | Blackbird.AI, accessed on July 23, 2025, https://blackbird.ai/blog/celebrity-deepfake-narrative-attacks/
iFakeDetector: Real Time Integrated Web-based Deepfake Detection System - IJCAI, accessed on July 23, 2025, https://www.ijcai.org/proceedings/2024/1016.pdf
Tracker: State Legislation on Deepfakes in Elections - Public Citizen, accessed on July 23, 2025, https://www.citizen.org/article/tracker-legislation-on-deepfakes-in-elections/
