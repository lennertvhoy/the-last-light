
Appendix L: The Case for Tool AI — A Grounded Perspective on Artificial Intelligence


Introduction: Navigating the Chasm Between Hype and Reality

The public discourse surrounding artificial intelligence (AI) is often a study in extremes. On one side stands a narrative of imminent, transformative change, populated by concepts like Artificial General Intelligence (AGI), the "Singularity," and existential risk (x-risk). This view, fueled by speculative philosophy and media amplification, often portrays AI as an impending artificial life form, a successor intelligence that could render humanity obsolete or extinct. On the other side stands a more pragmatic, science-driven perspective, one grounded in the realities of engineering, cognitive science, and economic application. This is the perspective of "Tool AI."
The Tool AI framework does not deny the power or potential of artificial intelligence. Rather, it reframes it. From this viewpoint, AI is not an embryonic consciousness but the latest and most sophisticated category of tool yet developed by humankind. Its purpose is not to replicate human minds but to augment human capabilities, solve specific, well-defined problems, and serve as a powerful engine for industrial and scientific progress. This appendix provides an overview of the primary proponents of this grounded perspective, who are united in their skepticism of the hype surrounding AGI and its associated existential anxieties.
The four figures profiled herein—Yann LeCun, Andrew Ng, Melanie Mitchell, and Rodney Brooks—are not Luddites or technology deniers. They are, in fact, among the world's leading pioneers and practitioners in the fields of machine learning, robotics, and cognitive science. Their collective skepticism is not a rejection of AI's promise but a rigorous critique of the direction of the hype and a call for a more sober, scientifically grounded approach to its development and deployment. They argue that the most significant challenges and opportunities with AI are not found in science-fiction scenarios but in the immediate, tangible problems of building systems that are reliable, understandable, and genuinely useful. Their arguments, explored in detail below, are essential for any "thinking human" seeking to navigate the chasm between the hype and the reality of artificial intelligence, a choice that is central to the thesis of this book.1

Section 1: Yann LeCun: The Engineer's Case for World Models

As a Turing Award laureate and the Chief AI Scientist at Meta, Yann LeCun occupies a central position in the AI landscape. His skepticism is not that of an outsider but of a chief architect who finds the prevailing blueprints for AGI to be fundamentally flawed. LeCun's critique is deeply technical, targeting the limitations of the very models that have fueled the current AI boom, and he offers a detailed, alternative vision for achieving human-level intelligence.

1.1 The Inadequacy of Language-Only Models

LeCun's core argument is that Large Language Models (LLMs), for all their linguistic prowess, are built on a foundation of sand. He posits that systems trained exclusively on text can never achieve true intelligence because the problem lies not with the AI, but with the "limited nature of language" itself.2 Language, he and co-author Jacob Browning argue, is an inherently "low-bandwidth" medium for transmitting information.2 It is rife with ambiguities—homonyms, pronouns, and contextual dependencies—that humans effortlessly resolve using a vast, shared, and crucially
non-linguistic understanding of how the world works.2
Current LLMs are trained to "fill in the blanks" or "produce one word after the other" based on statistical patterns in their massive textual training data.3 This methodology, while powerful for generating plausible language, leads to what LeCun terms a "shallow understanding".2 An LLM can learn to explain a concept linguistically but cannot necessarily use it practically; for example, it can describe the steps of long division without being able to perform the calculation, or list offensive words and then proceed to use them insensitively.2 This disconnect is the root cause of their unreliability. They are prone to "hallucinations"—more accurately termed confabulations—because they do not possess genuine reasoning or planning abilities; they are simply generating statistically likely sequences of tokens.3
LeCun emphasizes that this is not a problem that can be solved by simply scaling up current models. He argues that autoregressive language models will not scale to human-level intelligence and that a "new paradigm" is required.4 To illustrate the informational poverty of text, he contrasts the processing speed of the human brain for language (around 12 bytes per second) with the bandwidth of sensory input from observation and interaction (around 20 megabytes per second).3 This vast disparity suggests that true intelligence is built upon a much richer, higher-bandwidth data stream than language alone can provide.

1.2 A Path Towards Autonomous Machine Intelligence

In response to the limitations of LLMs, LeCun has articulated a comprehensive alternative vision in his 2022 position paper, "A Path Towards Autonomous Machine Intelligence".5 The paper addresses the central question: "How could machines learn as efficiently as humans and animals?".5 He notes that an adolescent can learn to drive a car in about 20 hours of practice, a feat of learning efficiency that is orders of magnitude beyond current machine learning systems, which require an immense number of trials to cover even rare situations.6
The key to this efficiency, LeCun hypothesizes, is the innate ability of humans and animals to learn "world models"—internal, predictive models of how the world works.5 These models allow an agent to understand concepts like object permanence, gravity, and the basic laws of physics through observation long before they are learned through language.6 To build machines with this capability, LeCun proposes a modular, fully differentiable cognitive architecture.6
A central component of this architecture is the Joint Embedding Predictive Architecture (JEPA).4 Unlike generative models that try to predict every pixel in the next frame of a video—an incredibly complex and often wasteful task—JEPA learns to create abstract representations of the world and then makes predictions within that abstract representation space.4 By doing so, the model can ignore irrelevant details (like the rustling of leaves on a tree) and focus on predicting the essential, high-level consequences of actions, which is the foundation of effective reasoning and planning.7
This architectural proposal is accompanied by a set of radical recommendations for the field. LeCun advocates for abandoning generative models in favor of joint-embedding architectures like JEPA, abandoning probabilistic models for more flexible energy-based models, and, most controversially, abandoning reinforcement learning (RL) as a primary training method.10 He argues that RL is extremely inefficient and should only be used as a secondary tool to fine-tune the world model or the agent's intrinsic cost function when its plans fail to match reality.10

1.3 Reframing "AGI" and the Alignment Problem

LeCun's public statements on AGI and AI safety are often seen as contradictory or dismissive, but they reflect a deep-seated disagreement with the terminology and framing used by the "AI doomer" community.11 He is famous for stating that "there is no such thing as AGI" while simultaneously acknowledging that machines will "eventually surpass human intelligence in all domains".11 This is not a logical inconsistency but a terminological and philosophical one. He rejects the term "AGI" because he believes it promotes a flawed, anthropomorphic vision of a single, conscious, monolithic entity. He finds the term to be ill-defined and even "insane," preferring more precise descriptors like "human-level AI".12
Despite his dismissal of the AGI narrative, LeCun actively engages with the underlying safety concerns, framing the issue as, "How to solve the alignment problem?".11 As detailed in Appendix B, this problem involves the immense technical difficulty of specifying human values to a machine (Outer Alignment) and ensuring the machine robustly adopts those goals (Inner Alignment). His proposed solution, however, is not based on formal verification or mathematical proofs of correctness, which he views as impractical. Instead, his is an engineering approach. The architecture he proposes is designed to be "safe by design".11 It is driven by a set of intrinsic objectives encoded in a "cost" module, which calculates the level of discomfort for an agent. The agent's entire planning process is dedicated to finding sequences of actions that minimize this cost. By designing this objective function appropriately, LeCun argues, we can build AI systems that are inherently steerable, controllable, and non-confrontational.
Critics argue that this approach is naive and reflects a "massive failure in imagination" regarding more complex risk models.11 His comparison of aligning AI to aligning humans and governments—through laws and social norms—is seen as particularly weak, given the frequent and flagrant failures of alignment in human societies.11 This highlights a potential blind spot in his framework, which focuses on engineering control while potentially underestimating emergent, strategic risks that fall outside the predefined cost function.

1.4 The Open-Source Imperative

A cornerstone of LeCun's vision for a safe and beneficial AI future is his unwavering advocacy for open-source development. He argues that the foundational platforms on which future AI assistants will be built must be "open source and widely available".13 His reasoning is both democratic and practical: no single company, especially one based on the US West Coast or in China, can possibly build a foundational model that adequately understands and reflects the full spectrum of the world's languages, cultures, and value systems.3
This stance is also, undoubtedly, a strategic one for Meta, positioning the company as a champion of openness in a field increasingly dominated by closed, proprietary models from competitors like Google and OpenAI. Some observers have framed this as a business tactic, allowing Meta to "steer clear of all that confusing AI safety hogwash" and accelerate development.12 However, LeCun's stated rationale is consistently centered on the belief that a decentralized, collaborative approach is the only way to create AI that serves all of humanity.
This perspective reveals a different conception of AI safety itself. Rather than a technical problem to be solved once by a single team of researchers in a lab, safety becomes an ongoing, distributed process managed by a global community. By enabling diverse groups to inspect, critique, and fine-tune open models, the risk of a single, powerful, and misaligned AI imposing a narrow set of values on the world is mitigated. This vision of safety is inherently political and economic, rooted in democratization and the prevention of concentrated power, a stark contrast to the centralized, technical alignment approach favored by many in the x-risk community. LeCun's goal is to prevent a future where our interaction with the digital world is mediated by systems controlled by a "handful of companies".13

Section 2: Andrew Ng: The Economic Pragmatist's Playbook

Andrew Ng, a co-founder of Google Brain and Coursera, and founder of Landing AI, brings the lofty conversation about artificial intelligence crashing down to earth. His perspective is that of an economic pragmatist, an educator, and an entrepreneur who sees AI not as a philosophical puzzle but as a transformative economic force. For Ng, the entire debate about AGI and existential risk is a dangerous and self-serving distraction from the real work at hand: applying AI to create tangible value and addressing its immediate societal consequences.

2.1 "AI is the New Electricity"

The key to understanding Andrew Ng's entire worldview is his central analogy: "AI is the new electricity".14 First articulated in keynotes and talks around 2017, this metaphor reframes AI from a mysterious, sentient force into a familiar, utilitarian concept.14 Ng argues that just as the electrification of society 100 years ago fundamentally transformed every major industry—from manufacturing and transportation to healthcare and agriculture—AI is now poised to have a similarly pervasive and revolutionary impact.16
The core of the analogy is the idea that AI is a General-Purpose Technology (GPT). Its power lies not in its potential for consciousness but in its broad utility. Like electricity, AI is an essentially neutral tool; its value is unlocked only through its application to solve countless specific problems across every conceivable sector.18 This framing immediately shifts the focus of the AI endeavor. The goal is no longer to build a single, god-like "AGI," but rather to empower millions of developers and entrepreneurs to build thousands of valuable, AI-powered applications that improve specific processes and create economic value.15 This analogy is a strategic tool designed to demystify AI for business leaders outside of the tech industry. It communicates that one does not need to build a power plant (a foundation model) to benefit from the technology; one simply needs to learn how to use the electricity (the AI tools) to run one's existing machinery more effectively. This democratizes the concept of AI, shifting the locus of innovation from the few elite labs building models to the many domain experts solving real-world problems.

2.2 Deconstructing the AGI Hype

Ng is one of the most direct and vocal critics of what he sees as a manufactured hype cycle around AGI. He repeatedly and bluntly states that "AGI has been overhyped" and dismisses claims that a single new AI model will cause mass unemployment or wipe out entire industries as "just not true" and "ridiculous".18
Crucially, he argues that this is not simply a case of over-enthusiasm but a deliberate, economically motivated narrative. He explicitly claims that these "hype narratives" are strategically employed by some technology companies to "raise money or appear more powerful than they actually are".18 This provides an insider's critique of a specific feedback loop within the Silicon Valley ecosystem, where grand narratives of exponential disruption are rewarded with massive venture capital funding and media attention, creating a self-reinforcing cycle of hype. One online commenter astutely observed this pattern in action: "It was 'AGI SOON AGI SOON AGI SOON' for years to build up hype and generate VC funds, then they hit a internal wall and realize that they probably won't hit AGI, now that VC groups and average users are recognizing the limitations of this tech... tech companies are saying 'AGI was overhyped'".21 This perfectly illustrates the dynamic Ng is critiquing, warning the broader public and business community not to get caught up in a narrative bubble driven by the unique financial incentives of the tech-VC world.

2.3 The Real-World Imperative: From Model-Building to Application

Flowing directly from his critique of hype is Ng's core message: the real power in the AI era lies in application, not just creation. He argues that "the real game-changer... won't be who builds the smartest machine, but who learns to use existing tools effectively".18 In his view, the most powerful people in the coming decades will be those who know how to use AI to solve practical problems, not necessarily those who can build new models from scratch.19
He consistently advises entrepreneurs and developers to focus on solving real-world needs in sectors like healthcare and education rather than chasing "speculative breakthroughs" that may never materialize.19 His recent work and keynotes have focused on promoting "agentic AI workflows," which are practical design patterns—such as reflection, tool use, and planning—that leverage existing models to solve complex business problems, from document analysis to visual inspection in manufacturing.20 This focus on the application layer is the defining characteristic of his work with companies like Landing AI and DeepLearning.AI, which are dedicated to helping businesses and individuals use AI tools today.22

2.4 Identifying the Real Risks: Job Displacement over Killer Robots

Ng is deeply skeptical of speculative, long-term risks, sharply contrasting them with the tangible, near-term societal challenges posed by AI. He is famously dismissive of fears of "evil AI killer robots," comparing such anxieties to "worrying about overpopulation on the planet Mars".16 He sees "no clear path to how AI can become sentient" and argues that if it ever does, it might take hundreds or thousands of years.16
More pointedly, he contends that this "evil AI hype" serves as a convenient smokescreen, a way to "whitewash a much more serious issue, which is job displacement".16 Ng believes that "AI software will be in direct competition with a lot of people for jobs," and that this is a problem the tech industry needs to "own up to".16 Rather than debating hypothetical future risks, he argues that society must act now to address this impending economic disruption. Drawing a parallel to how the automation of agriculture led to the development of the modern K-12 and university system, he calls for a fundamental rethinking of education and the creation of a robust social safety net. He proposes that governments should help the unemployed by providing the structure and resources to study and reskill, enabling them to re-enter a workforce reshaped by AI.16

Section 3: Melanie Mitchell: The Cognitive Scientist's Cautionary Tale

Melanie Mitchell, a professor at the Santa Fe Institute, offers a perspective rooted in cognitive science and a deep appreciation for the profound complexity of human intelligence. Her skepticism is not primarily about economic incentives or engineering architectures, but about the fundamental gap between what AI systems can do and what they can understand. She warns that the greatest danger we face is not from artificial superintelligence, but from a misplaced faith in the abilities of profoundly "stupid" machines.

3.1 The Specter of "Artificial Stupidity"

Mitchell's central argument is that when it comes to near-term worries, "superintelligence should be far down the list. In fact, the opposite of superintelligence is the real problem".24 The key issue is the inherent "brittleness" of even the most accomplished AI systems.24 These systems excel at pattern recognition within the narrow confines of their training data but can fail in unexpected and nonsensical ways when faced with situations that deviate even slightly from what they have seen before.24
This brittleness gives rise to what she and others have termed "artificial stupidity": subtle, unpredictable failures that betray a complete lack of common sense.27 She provides numerous examples: a self-driving car that fails to identify a pedestrian in an unusual pose 28; a state-of-the-art vision system that learns to associate the concept of "animal" with the "blurry green background" common in its training photos rather than with the animal itself 27; or a deep reinforcement learning agent trained to play the game Breakout that, after mastering the game, fails completely when the paddle is moved up by just a few pixels because it never learned the
concept of a paddle as an object.29 This view is memorably summarized in a quote she often cites from AI researcher Pedro Domingos: "People worry that computers will get too smart and take over the world, but the real problem is that they're too stupid and they've already taken over the world".25

3.2 The Human Tendency to Overestimate

The danger of artificial stupidity is magnified by a corresponding flaw in human psychology: our innate tendency to overestimate the capabilities of AI. Mitchell warns that "the most worrisome aspect of AI systems in the short term is that we will give them too much autonomy without being fully aware of their limitations and vulnerabilities".24 This happens because we instinctively "anthropomorphize AI systems," projecting human-like qualities of understanding, intention, and trustworthiness onto them where none exist.24 The fluent language of an LLM or the superhuman skill of a game-playing AI seduces us into believing the system possesses a general competence that it simply does not have.
This creates a perilous sociotechnical failure mode. The risk is not just that the AI is brittle (a technical problem), but that our flawed human psychology leads us to deploy these brittle systems in high-stakes environments like medicine, finance, and transportation, trusting them far more than their actual capabilities warrant. The catastrophe, in this view, is not caused by a malevolent superintelligence, but by a misplaced trust in a superficially competent but fundamentally unintelligent tool.
Mitchell also situates the current wave of enthusiasm within the historical "boom and bust cycle" of the field, characterized by alternating "AI Springs" of immense optimism and funding, followed by "AI Winters" of disappointment and budget cuts when grand promises fail to materialize.28 She recalls being advised not to even use the term "artificial intelligence" on her job applications during the AI winter of the 1990s, a stark contrast to today's environment where the term is ubiquitous.30 This historical perspective serves as a crucial reality check on current claims, suggesting that the present hype may be just another peak before an inevitable trough of disillusionment.

3.3 The Barrier of Meaning

At the heart of Mitchell's critique is her concept of the "barrier of meaning".27 This is her explanation for
why AI systems are so brittle and stupid. Drawing on the work of philosopher Gian-Carlo Rota, she argues that even the most advanced AI has not yet crashed this barrier; that is, the systems do not "actually understand" the situations they encounter in any meaningful, human-like way.32 They can process language without comprehending it, and recognize images without grasping their significance.27
This lack of understanding is most evident in their profound deficit of common sense—the vast, intuitive body of knowledge about the physical and social world that humans use to navigate nearly every situation.28 This deficit is exposed by their poor performance on linguistic puzzles like the Winograd Schema Challenge, which requires disambiguating a pronoun based on real-world knowledge (e.g., "The trophy would not fit in the brown suitcase because
it was too large." What does "it" refer to?). The difficulty AI has with such a simple task led researcher Oren Etzioni to quip, "When AI can't determine what 'it' refers to in a sentence, it's hard to believe that it will take over the world".27 For Mitchell, this lack of genuine, grounded understanding is the single greatest obstacle to achieving robust, reliable, and trustworthy AI.

3.4 The Path Forward Through Analogy and Abstraction

Mitchell's proposed path forward is not focused on a specific architecture but on cultivating the core cognitive abilities she sees as foundational to human intelligence. She argues that the key to surmounting the barrier of meaning lies in developing AI that can make robust abstractions and form fluid analogies.36 It is this ability to see the abstract essence of a situation and map it onto new, different situations that allows for true generalization and transfer of knowledge—a hallmark of human learning and a critical weakness of current AI.37
She suggests that for AI to acquire this kind of deep, flexible understanding, it may need to learn more like a human child. This points toward the necessity of embodiment and developmental learning, where an AI system, perhaps housed in a robot, could experience and interact with the physical and social world directly.28 This aligns her with the thinking of Rodney Brooks, suggesting that intelligence cannot be divorced from physical experience. The goal is to move beyond systems that learn from static datasets to systems that learn through active, embodied exploration, thereby building up the rich, grounded, common-sense foundation required for true understanding.

Section 4: Rodney Brooks: The Roboticist's Grounding in Reality

Rodney Brooks, former director of the MIT Computer Science and Artificial Intelligence Laboratory and co-founder of iRobot, is the intellectual bedrock of the modern AI skeptic tradition. For over four decades, he has waged a consistent and influential campaign against the notion of disembodied intelligence. His philosophy, forged in the practical challenges of robotics, provides a powerful and enduring argument that true intelligence cannot be separated from physical interaction with the real world.

4.1 Intelligence Without Representation: The Foundational Critique

Brooks's most foundational contribution came in the late 1980s and early 1990s, when the dominant paradigm in AI was the symbol system hypothesis. This classical approach decomposed intelligence into a sequence of abstract, functional modules: perception would create a symbolic model of the world, a central reasoning engine would plan based on that model, and an actuator module would execute the plan.39 Brooks argued this entire approach was "fundamentally flawed" and biologically implausible.39
In his seminal 1990 paper, "Elephants Don't Play Chess," he argued that AI research was too focused on the "expert" behaviors of a small human elite (like playing chess) while ignoring the far more fundamental and difficult problem of basic mobility and survival that all animals master.40 He proposed an alternative based on the
physical grounding hypothesis: the idea that a system's representations must be directly grounded in the physical world through perception and action.41 The most elegant and efficient model of the world, he famously asserted, is "the world is its own best model".39
He put this philosophy into practice with his subsumption architecture.41 This was a bottom-up, layered system that abandoned the central planning model. Instead, it was built from simple, reactive, behavior-generating modules. A robot's lowest layer might be a simple behavior: "if a sensor detects an obstacle, turn away." A higher layer could add another behavior: "wander randomly." This higher layer could "subsume" the lower one, for example, by suppressing the wandering behavior when an obstacle was detected.42 Brooks demonstrated that complex, seemingly goal-directed behavior—like a robot that explores a room while avoiding objects—could emerge from the interaction of these simple, independent, and physically grounded modules, all without a central, symbolic representation of the room.42

4.2 From Symbolic AI to "Masterful Bullshitters"

Brooks's decades-long critique finds new life and relevance in the age of LLMs. He sees a direct philosophical lineage from the ungrounded symbols of classical AI to the ungrounded text tokens of modern generative models. His assessment of LLMs is withering: he calls them "masterful bullshitters".44 The term is not just a pejorative; it is a precise technical critique. Like a human bullshitter, an LLM is concerned with producing a plausible-sounding output, not with its truthfulness or connection to reality. They "don't know what's true," Brooks argues, but have simply learned from a vast corpus of text "what words sort of work together".44
He argues that humans are uniquely vulnerable to this form of deception because we are "seduced by language".45 We see fluent linguistic output and instinctively attribute understanding, intention, and intelligence to its source. Brooks warns that we must not be "deceived by LLMs' facile use of language into believing they have magical capabilities".44 At their core, he contends, these systems are sophisticated auto-correlation engines that lack a model of reality and, crucially, any grasp of causality.45 This critique is identical in spirit to his original argument against symbolic AI: the system is manipulating tokens (whether logical symbols or words) that have no inherent meaning to the machine itself.

4.3 The Slow Pace of Real-World Change

A key component of Brooks's skepticism is his pushback against the pervasive assumption of exponential progress, a core tenet of the "Singularity" narrative. He argues that exponentials "can't operate forever" and that many technological trends that appear exponential are in fact S-curves of growth that eventually mature and level off.45 He has even coined an acronym for the social pressure to accept these narratives: "FOBAWTPALSL" (Fear of Being a Wimpy Techno-Pessimist and Looking Stupid Later), which describes the tendency for people to uncritically embrace hype to avoid being seen as technology deniers.44
His most powerful argument against runaway exponential growth is grounded in the constraints of the physical world. While computation may follow Moore's Law for a time, the real world does not. He points out that we are already approaching the physical limits of energy efficiency for a robot lifting an object, meaning the cost of physical robotic labor will not drop exponentially forever.44 Furthermore, deploying technology in the real world is subject to immense friction. He often cites the internet's slow, multi-decade transition from the IPv4 protocol to IPv6 as a prime example. Even though IPv6 is a purely digital and vastly superior technology, its adoption has been hampered by the immense cost and coordination required to update the world's existing infrastructure.44 An AGI, no matter how intelligent, cannot magically overcome the material, economic, and systemic inertia of the physical world. This provides a powerful reality check that grounds the abstract concept of a "fast takeoff" singularity in the messy, slow, and friction-filled reality of the global economy.

4.4 The Primacy of Human Agency

Underpinning all of Brooks's work is a consistent design philosophy for technology. He maintains that the goal of AI and robotics should be to "enhance human capabilities rather than attempt to replicate them".44 He argues that people only truly accept and adopt new technologies when they feel they retain a sense of control and
agency.44 Users want the ability to understand what the system is doing and, most importantly, to step in and override it when it is not performing as expected.45
He points to the persistent challenges of fully autonomous, or "driverless," cars as a key example of this principle in action.44 A major barrier to their acceptance is the ambiguity of control and the lack of a clear, intuitive way for the human "driver" to maintain agency over the vehicle's actions. For Brooks, a successful AI system is not one that replaces the human, but one that becomes a reliable and predictable tool that plugs into the human's own model of the world, ultimately leaving the human in charge.44

Section 5: Convergences, Divergences, and the Wider Skeptical Landscape

While Yann LeCun, Andrew Ng, Melanie Mitchell, and Rodney Brooks each bring a unique perspective to the table, their arguments weave together to form a powerful and coherent counter-narrative to the dominant hype surrounding AGI. Analyzing their points of convergence and divergence, and situating them within a broader intellectual landscape, reveals the depth and rigor of the case for Tool AI.

5.1 Comparative Framework Table

To clarify the distinct yet overlapping positions of these four thinkers, the following table provides a high-level summary of their core critiques, proposed solutions, and stances on AGI and existential risk.
Table 1: A Comparative Framework of Prominent AI Skeptics
Proponent
Yann LeCun
Andrew Ng
Melanie Mitchell
Rodney Brooks


5.2 Common Threads: The Anti-Cartesian Consensus

The most powerful common thread uniting these four thinkers is their shared rejection of a disembodied, purely computational view of intelligence. In essence, they form an "anti-Cartesian" consensus, pushing back against the idea of a mind divorced from a body and a world. Each, in their own way, argues that intelligence is not merely the manipulation of abstract symbols in a vacuum.
Rodney Brooks is the originator of this view in modern AI, with his decades-long insistence on physical grounding and embodiment as prerequisites for intelligence.41 Yann LeCun echoes this by rejecting language-only models and demanding that future AI learn from high-bandwidth sensory data to build predictive world models, just as an animal does.2 Melanie Mitchell argues that the "barrier of meaning" and the acquisition of common sense can only be overcome through embodied, developmental learning that grounds concepts in worldly experience.28 And while Andrew Ng's focus is more economic, his "electricity" analogy inherently treats AI as a tool to be applied
in the world, deriving its value from its real-world effects, not from its existence as a disembodied brain in a vat. This shared conviction that intelligence must be situated and grounded forms the philosophical core of the Tool AI perspective.

5.3 Points of Contention: Divergent Paths and Priorities

Despite their philosophical alignment, these thinkers propose different solutions and prioritize different risks. While they agree on the fundamental problem of disembodied AI, their paths forward diverge. Brooks has historically advocated for a bottom-up, behavior-based robotics approach like the subsumption architecture.42 LeCun, coming from a deep learning background, proposes a more complex, top-down (though still grounded) architecture in JEPA, which aims to learn abstract world models through self-supervision.7 Mitchell, the cognitive scientist, is less focused on a specific architecture and more on identifying the necessary cognitive capabilities—like analogy and abstraction—that any successful architecture must achieve.37 Ng remains largely agnostic about architecture, focusing instead on the economic and application layer where existing tools can create value.20
Their primary concerns also differ. Ng is focused on the immediate, tangible, and certain economic risk of job displacement.16 Mitchell is concerned with the near-term sociotechnical risk of humans misapplying brittle, "stupid" systems in critical domains due to our tendency to overestimate them.24 LeCun's concerns are both technical and political: ensuring future systems are controllable by design and that their development is democratized through open source to prevent the concentration of power.13 Brooks, the veteran observer, focuses on deflating the hype itself, reminding everyone of the physical and systemic constraints that make real-world progress far slower and more difficult than popular narratives suggest.45

5.4 Situating the Skeptics: The Wider Landscape

The views of these four proponents are enriched when placed in the context of other prominent AI critics who raise different but complementary challenges.
Gary Marcus, a cognitive scientist and entrepreneur, shares Mitchell's focus on AI's brittleness and lack of true understanding. However, his proposed solution is different. He argues that deep learning is "data hungry, shallow, and brittle" and must be supplemented with techniques from classical, symbol-manipulating AI.46 His call for neuro-symbolic "hybrid models" represents a different path forward, one that seeks to explicitly integrate the rule-based reasoning of classical AI with the pattern-recognition strengths of deep learning.46 This contrasts with LeCun's vision, which aims to achieve reasoning abilities from a purely deep learning-based, self-supervised foundation.
Judea Pearl, another Turing Award winner, offers perhaps the most fundamental critique of the entire modern machine learning paradigm. He argues that current AI, including deep learning, is "stuck on rung one" of his three-rung "ladder of causation".48 Systems are adept at finding correlations in data (Level 1: Seeing), but they are unable to reason about interventions (Level 2: Doing) or ask counterfactual "what if" questions (Level 3: Imagining).50 Pearl contends that without a built-in capacity for causal reasoning, machines are merely performing sophisticated "curve fitting" and can never achieve genuine, human-like intelligence.49 This critique introduces a crucial dimension—causality—that is largely unaddressed by the other skeptics and poses a profound challenge to the entire field.

Conclusion: Beyond the Hype—Building Valuable and Reliable AI

The collective arguments of the Tool AI proponents present a formidable and necessary counterpoint to the speculative narratives that so often dominate conversations about the future. Their skepticism is not a pessimistic rejection of technology but a pragmatic and constructive call for intellectual honesty and scientific rigor. It is a demand that the field of artificial intelligence ground itself in solving the difficult, fundamental problems of the present before indulging in fantasies about the future.
The recurring themes are clear and compelling. True intelligence is not a disembodied algorithm but is deeply intertwined with sensory experience, physical interaction, and a grounded understanding of the world. The most significant hurdles are not about achieving superhuman speed but about instilling basic common sense. The most pressing risks are not from hypothetical, malevolent superintelligences, but from the very real brittleness of our current systems and our own psychological biases that lead us to trust them inappropriately.
Ultimately, the case for Tool AI is a case for a more mature, responsible, and productive vision for the field. It suggests that the proper goal of AI research should not be the quasi-mythological quest for an artificial god, but the patient, difficult, and deeply valuable engineering of a diverse ecosystem of reliable, transparent, and beneficial tools. The aim should be to build systems that solve real human problems, augment human intelligence, and create tangible economic and social value. This grounded approach, the evidence suggests, is the true path to realizing the immense and transformative potential of artificial intelligence.
Works cited
en.wikipedia.org, accessed on July 23, 2025, https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Guide_for_Thinking_Humans
AI And The Limits Of Language - Noema Magazine, accessed on July 23, 2025, https://www.noemamag.com/ai-and-the-limits-of-language/
Yann LeCun Emphasizes the Promise of AI - NYAS - The New York Academy of Sciences, accessed on July 23, 2025, https://www.nyas.org/ideas-insights/blog/yann-lecun-emphasizes-the-promise-ai/
Why Can't AI Make Its Own Discoveries? — With Yann LeCun ..., accessed on July 23, 2025, https://www.youtube.com/watch?v=qvNCVYkHKfg&pp=0gcJCfwAo7VqN5tD
Yann LeCun: A Path Towards Autonomous Machine Intelligence | Shaped Blog, accessed on July 23, 2025, https://www.shaped.ai/blog/yann-lecun-a-path-towards-autonomous-machine-intelligence
A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27 - OpenReview, accessed on July 23, 2025, https://openreview.net/pdf?id=BZ5a1r-kVsf
A Path Towards Autonomous Machine Intelligence - Temple CIS, accessed on July 23, 2025, https://cis.temple.edu/tagit/presentations/A%20Path%20Towards%20Autonomous%20Machine%20Intelligence.pdf
A Path Towards Autonomous Machine Intelligence: Exploring Hierachical Predictive Architectures | by Lawrence Knight | Medium, accessed on July 23, 2025, https://medium.com/@LawrencewleKnight/a-path-towards-autonomous-machine-intelligence-exploring-hierachical-predictive-architectures-48ba2ca950af
Yann LeCun: From Machine Learning to Autonomous Intelligence - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=VRzvpV9DZ8Y
A Path Towards Autonomous Machine Intelligence with Dr. Yann LeCun - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=EvSe0ktD95k
Yann LeCun on AGI and AI Safety - Effective Altruism Forum, accessed on July 23, 2025, https://forum.effectivealtruism.org/posts/LSzHmdCdsFieMXLcL/yann-lecun-on-agi-and-ai-safety
Yann LeCun on AGI and AI Safety - LessWrong, accessed on July 23, 2025, https://www.lesswrong.com/posts/Zfik4xESDyahRALKk/yann-lecun-on-agi-and-ai-safety
The Shape of AI to Come! Yann LeCun at AI Action Summit 2025 - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=xnFmnU0Pp-8
Andrew Ng on why Artificial Intelligence is the new electricity - EECS ..., accessed on July 23, 2025, https://eecs.berkeley.edu/news/andrew-ng-why-artificial-intelligence-new-electricity/
AI is the New Electricity - Dr. Andrew Ng - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=fgbBtnCvcDI
Andrew Ng: Why AI Is the New Electricity | Stanford Graduate School of Business, accessed on July 23, 2025, https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity
Why AI Is the 'New Electricity' – Knowledge@Wharton - Gerd Leonhard, accessed on July 23, 2025, https://futuristgerd.com/2017/11/why-ai-is-the-new-electricity-knowledgewharton/
Andrew Ng: True AI Power Lies in Usage, Not in Chasing AGI, accessed on July 23, 2025, https://www.thehansindia.com/technology/tech-news/andrew-ng-true-ai-power-lies-in-usage-not-in-chasing-agi-987243
'Just ridiculous,' says Google Brain founder on hype about AI taking away all jobs, shares tips how anyone can become powerful - The Economic Times, accessed on July 23, 2025, https://m.economictimes.com/news/new-updates/just-ridiculous-says-google-brain-founder-on-hype-about-ai-taking-away-all-jobs-shares-tips-how-anyone-can-become-powerful/articleshow/122633320.cms
Andrew Ng on the Rise of AI Agents: Redefining Automation and Innovation - Medium, accessed on July 23, 2025, https://medium.com/@muslumyildiz17/andrew-ng-on-the-rise-of-ai-agents-redefining-automation-and-innovation-440565ce633b
Google Brain founder says AGI is overhyped, real power lies in knowing how to use AI and not building it - Reddit, accessed on July 23, 2025, https://www.reddit.com/r/ArtificialInteligence/comments/1lzmu7i/google_brain_founder_says_agi_is_overhyped_real/
Andrew Ng - AI Keynote Speaker, accessed on July 23, 2025, https://www.aurumbureau.com/speaker/andrew-ng/
Andrew Ng: Artificial Intelligence is the New Electricity - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=21EiKfQYZXc
Quote by Melanie Mitchell: “In any ranking of near-term worries ..., accessed on July 23, 2025, https://www.goodreads.com/quotes/10078945-in-any-ranking-of-near-term-worries-about-ai-superintelligence-should
Artificial Intelligence Quotes by Melanie Mitchell - Goodreads, accessed on July 23, 2025, https://www.goodreads.com/work/quotes/67780615-artificial-intelligence-a-guide-for-thinking-humans
[2012.06058] Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable - arXiv, accessed on July 23, 2025, https://arxiv.org/abs/2012.06058
Review of Artificial Intelligence: A Guide for Thinking Humans ..., accessed on July 23, 2025, https://medium.com/@adnanmasood/review-of-artificial-intelligence-a-guide-for-thinking-humans-an-insiders-appraisal-of-melanie-5a489a6680f1
Melanie Mitchell: 'The big leap in artificial intelligence will come when it is inserted into robots that experience the world like a child' | Technology, accessed on July 23, 2025, https://english.elpais.com/technology/2024-04-14/melanie-mitchell-the-big-leap-in-artificial-intelligence-will-come-when-it-is-inserted-into-robots-that-experience-the-world-like-a-child.html
Mindscape 68 | Melanie Mitchell on Artificial Intelligence and the Challenge of Common Sense - YouTube, accessed on July 23, 2025, https://www.youtube.com/watch?v=F9Il2Q0mCDI
Transcript of Episode 33 – Melanie Mitchell on the Elements of AI - The Jim Rutt Show, accessed on July 23, 2025, https://jimruttshow.blubrry.net/the-jim-rutt-show-transcripts/transcript-of-episode-33-melanie-mitchell-on-the-elements-of-ai/
Events: Artificial Intelligence and the “Barrier” of Meaning | Santa Fe Institute, accessed on July 23, 2025, https://www.santafe.edu/events/artificial-intelligence-and-barrier-meaning
On Crashing the Barrier of Meaning in AI - Melanie Mitchell, accessed on July 23, 2025, https://melaniemitchell.me/PapersContent/AIMagazine2020.pdf
Artificial Intelligence and the “Barrier of Meaning” PROFESSOR MELANIE MITCHELL - Trinity College Dublin, accessed on July 23, 2025, https://www.tcd.ie/Neuroscience/RPPF/assets/pdfs/melanie_mitchell_poster.pdf
68 | Melanie Mitchell on Artificial Intelligence and the Challenge of Common Sense, accessed on July 23, 2025, https://www.preposterousuniverse.com/podcast/2019/10/14/68-melanie-mitchell-on-artificial-intelligence-and-the-challenge-of-common-sense/
Podcast: Ep. 1: What is Intelligence | Santa Fe Institute, accessed on July 23, 2025, https://www.santafe.edu/culture/podcasts/ep-1-what-is-intelligence?tab=transcript
Melanie Mitchell on Can Artificial Intelligence Beat Human Thinking | MHC Ep 203, accessed on July 23, 2025, https://www.youtube.com/watch?v=fmhqrm1er48
The Computer Scientist Training AI to Think With Analogies - Quanta Magazine, accessed on July 23, 2025, https://www.quantamagazine.org/melanie-mitchell-trains-ai-to-think-with-analogies-20210714/
Artificial Intelligence and the Barrier of Meaning, accessed on July 23, 2025, https://www.tcd.ie/Neuroscience/RPPF/assets/pdfs/melanie_mitchell.pdf
Elephants Don't Play Chess - People - MIT, accessed on July 23, 2025, https://people.csail.mit.edu/brooks/papers/elephants.pdf
Rodney Brooks - Wikipedia, accessed on July 23, 2025, https://en.wikipedia.org/wiki/Rodney_Brooks
Elephants Don't Play Chess - JAWS, accessed on July 23, 2025, https://msujaws.wordpress.com/2010/11/18/elephants-dont-play-chess/
Elephants Don't Play Chess, accessed on July 23, 2025, https://www.cse.unr.edu/~monica/Courses/CS493-790/Presentations/Yan1.ppt
Examining the Validity, Verity, and Relevance of Rodney A. Brooks's Argument Against the Necessity of Representation in Intelligent Systems | The Classic Journal, accessed on July 23, 2025, https://theclassicjournal.uga.edu/index.php/2025/05/09/examining-the-validit/
The Myth Buster: Rodney Brooks Breaks Down the Hype Around AI ..., accessed on July 23, 2025, https://www.robust.ai/blog/newsweekaiseries
The Myth Buster: Rodney Brooks Breaks Down the Hype Around AI ..., accessed on July 23, 2025, https://www.newsweek.com/rodney-brooks-ai-impact-interview-futures-2034669
In defense of skepticism about deep learning | by Gary Marcus ..., accessed on July 23, 2025, https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-6e8bfd5ae0f1
deep learning a critical appraisal.formatted.pages - arXiv, accessed on July 23, 2025, https://arxiv.org/abs/1801.00631
At 87, Pearl is still able to change his mind - LessWrong, accessed on July 23, 2025, https://www.lesswrong.com/posts/uFqnB6BG4bkMW23LR/at-87-pearl-is-still-able-to-change-his-mind
To Build Truly Intelligent Machines, Teach Them Cause and Effect ..., accessed on July 23, 2025, https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/
The Book of Why: Exploring the missing piece of artificial ..., accessed on July 23, 2025, https://bdtechtalks.com/2019/12/09/judea-pearl-the-book-of-why-ai-causality/
