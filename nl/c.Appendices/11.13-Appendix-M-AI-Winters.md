# Bijlage M: Een Gedetailleerde Geschiedenis van AI-winters

Introductie: De Seizoenen van een Wetenschap

De geschiedenis van kunstmatige intelligentie (AI) is geen lineaire mars van vooruitgang. In plaats daarvan is het een verhaal van dramatische cycli, van vurige zomers van optimisme gevolgd door bittere, desolate winters. De term "AI-winter," voor het eerst gearticuleerd in een openbaar debat tijdens de jaarlijkse bijeenkomst van de American Association for Artificial Intelligence (AAAI) in 1984, is komen te definiëren wat deze periodes van terugtrekking kenmerken, gekenmerkt door verminderde financiering, afnemende publieke interesse en een afkoeling van wetenschappelijke ambitie.1 Deze winters zijn geen lulls; ze zijn vormend, vaak getriggerd door een voorspelbare en terugkerende cyclus. Het patroon begint met onderzoekers die ambitieuze, soms grandioze, beloften doen, gevoed door vroege en indrukwekkende resultaten in beperkte omgevingen. Deze claims worden vervolgens versterkt door de media, investeerders en overheidsfinancierders, wat leidt tot een "piek van opgeblazen verwachtingen".1 Wanneer de technologie onvermijdelijk niet aan deze hoge doelen voldoet, hetzij door onvoorziene complexiteit, fundamentele theoretische beperkingen, of de immense kloof tussen laboratorium "microwerelden" en de werkelijkheid, volgt een "daling van teleurstelling". Deze teleurstelling manifesteert zich als ernstige bezuinigingen op de financiering, de ineenstorting van commerciële ondernemingen en een algemeen verlies van vertrouwen in de belofte van het veld.4
Deze bijlage stelt dat AI-winters, hoewel pijnlijk, niet simpelweg mislukkingen zijn, maar cruciale, paradigma-veranderende gebeurtenissen. Elke winter heeft een afrekening afgedwongen met de fundamentele aannames van zijn tijdperk. De eerste winter stelde de toereikendheid van pure logica en zoekmethoden ter discussie, waardoor het veld gedwongen werd om de rommelige, hardnekkige problemen van kennis in de echte wereld onder ogen te zien. De tweede winter onthulde de broosheid van handgecodeerde expertise, wat een verschuiving naar automatisch leren uit gegevens noodzakelijk maakte. Het begrijpen van deze cyclische dynamiek—van hype, teleurstelling en uiteindelijk heroriëntatie—is essentieel voor het contextualiseren van de ongekende bloei van de huidige AI-zomer en voor het bewust kiezen hoe om te gaan met de beloften en gevaren ervan.3
Het is echter belangrijk op te merken dat deze geschiedenis niet zonder eigen debatten is. Het bestaan en de timing van de "eerste AI-winter" zijn betwist. Sommige technologiehistorici, zoals Thomas Haigh, beweren dat de jaren '70, hoewel gekenmerkt door hooggeprofileerde kritiek, in feite een periode van gestage institutionele en intellectuele groei voor de AI-onderzoeks gemeenschap waren. In dit licht is het verhaal van een "eerste winter" een retrospectieve constructie, een verhaal dat is verteld om in een handig cyclisch patroon te passen. De echte, diepe winter, zo wordt betoogd, begon pas met de ineenstorting van de expertensystemenbubbel aan het eind van de jaren '80.7 Deze bijlage zal deze nuance navigeren, de traditioneel geaccepteerde "twee winters" narratief presenteren als een kader, terwijl het deze belangrijke historiografische tegenargumenten erkent en analyseert. Het verhaal van AI-winters is dus niet alleen een geschiedenis van een technologie, maar een geschiedenis van de beloften die we erover doen, de grenzen die we ontdekken, en de lessen die we gedwongen zijn te leren.

Deel I: De Eerste Winter – Een Afrekening met de Complexiteit van de Echte Wereld (c. 1966–1980)

De eerste grote neergang in AI was niet slechts een financieringscrisis, maar een diepgaande intellectuele. Het markeerde het moment waarop het initiële paradigma van AI—een rationalistische droom van intelligentie als formele logica en heuristische zoekmethoden—botste met de hardnekkige, combinatoriële complexiteit van de echte wereld. Deze botsing was geen stille academische discussie, maar een openbaar spektakel, gedocumenteerd in twee verwoestende officiële rapporten die systematisch de vroege beloften van het veld ontmantelden en een decennium van teleurstelling katalyseerden.

De "Gouden Jaren" van Belofte (1956-1974)

De periode onmiddellijk volgend op het Dartmouth Summer Research Project on Artificial Intelligence in 1956 werd gekenmerkt door een diepgaande en wijdverspreide optimisme, wat sommigen een "naïeve euforie" hebben genoemd.9 Het oprichtingsvoorstel van het project stelde de ambitieuze toon, voortbouwend op de "vermoeden dat elk aspect van leren of enige andere eigenschap van intelligentie in principe zo precies kan worden beschreven dat een machine kan worden gemaakt om het te simuleren".10 Gedurende bijna twee decennia leek deze conjectuur met verbazingwekkende snelheid waar te worden.
Vroege programma's, ontwikkeld in de opkomende AI-laboratoria van MIT, Stanford en Carnegie Mellon, produceerden resultaten die voor de meeste waarnemers simpelweg "verbazingwekkend" waren.12 Computers demonstreerden capaciteiten die voorheen als het exclusieve domein van de menselijke geest werden beschouwd. Deze omvatten geavanceerde probleemoplossers zoals Allen Newell en Herbert Simon's Logic Theorist en de daaropvolgende General Problem Solver (GPS), die probeerde een universeel algoritme te vangen voor het oplossen van problemen door middel van heuristische zoekmethoden.12 Deze werden gevolgd door Herbert Gelernter's Geometry Theorem Prover (1958) en James Slagle's SAINT (Symbolic Automatic Integrator), een programma geschreven in 1961 dat symbolische integratieproblemen kon oplossen van een calculus-examen voor eerstejaars.12
De vooruitgang in natuurlijke taalverwerking was even indrukwekkend. Daniel Bobrow's programma STUDENT uit 1964 kon algebraïsche woordproblemen van de middelbare school oplossen door Engelse zinnen in vergelijkingen te vertalen.12 De meest beroemde was echter Joseph Weizenbaum's chatbot ELIZA uit 1966. Door eenvoudige patroonherkenning en herformuleringstechnieken te gebruiken om een Rogeriaanse psychotherapeut te simuleren, kon ELIZA gesprekken voeren die zo realistisch waren dat sommige gebruikers beroemdelijk in de veronderstelling verkeerden dat ze met een echte mens communiceerden.12 Dit fenomeen, waarbij menselijke gebruikers een grotere intelligentie en begrip toeschrijven aan een computerprogramma dan het daadwerkelijk bezit, werd bekend als het "ELIZA-effect" en diende als een vroege waarschuwing over de verleidelijke kracht van vloeiende taalgeneratie.11
Deze reeks successen voedde intens optimistische voorspellingen van de pioniers van het veld. In 1957 verklaarde Herbert Simon dat machines konden "denken, leren en creëren," en slechts een jaar later voorspelden hij en Newell dat "binnen tien jaar een digitale computer de wereldkampioen schaken zal zijn" en "een belangrijke nieuwe wiskundige stelling zal ontdekken en bewijzen".17 Dit vertrouwen, dat op de rand van hoogmoed balanceerde, was besmettelijk. Het trok enorme financiële steun aan, vooral van de Amerikaanse overheid via de Defense Advanced Research Projects Agency (DARPA). Gemotiveerd door de Koude Oorlog noodzaak om een technologische voorsprong op de Sovjetunie te behouden, begon DARPA in 1963 AI-onderzoek te financieren, waarbij miljoenen dollars in de leidende academische laboratoria werden gepompt en de primaire motor werd van de vroege groei van AI.12

De Vertaaldebacle: Het ALPAC Rapport van 1966

Machinevertaling (MT) was een van de vroegste en meest zwaar gefinancierde toepassingen van AI. De Koude Oorlog creëerde een dringende strategische behoefte voor de Amerikaanse inlichtingendienst om enorme hoeveelheden Russische wetenschappelijke en technische documenten automatisch te vertalen.4 Het veld kreeg een enorme boost van het Georgetown-IBM-experiment in 1954, een openbare demonstratie die een zorgvuldig samengestelde set van 49 Russische zinnen in het Engels vertaalde. Hoewel de woordenschat van het systeem beperkt was tot slechts 250 woorden en de grammaticaregels zeer specifiek waren, was het evenement een triomf voor de public relations, met sensationele koppen zoals "Elektronische hersenen vertalen Russisch" en "Robotbrein vertaalt Russisch naar King's English".2 Deze demonstratie creëerde een golf van opwinding en opende de sluizen voor overheidsfinanciering.
Echter, in het volgende decennium botste het aanvankelijke optimisme met de diepgaande complexiteiten van de menselijke taal. Onderzoekers ontdekten al snel dat vertaling niet simpelweg een kwestie was van woorden uit een woordenboek vervangen en ze opnieuw ordenen volgens syntactische regels. Al in 1960 betoogde de invloedrijke taalkundige Yehoshua Bar-Hillel dat volledig automatische, hoogwaardige vertaling in principe onmogelijk was. Hij wees erop dat het oplossen van ambiguïteit in taal echte, alledaagse kennis vereist, een capaciteit die machines volledig ontbeerden.23 Zijn beroemde voorbeeld, "De doos was in de pen," vereist kennis van de relatieve afmetingen van dozen en speelkooien om correct te worden begrepen. Dit probleem leidde tot nu legendarische (hoewel misschien apocriefe) verkeerde vertalingen, zoals de bijbelse zin "de geest is gewillig, maar het vlees is zwak" die naar verluidt "de vodka is goed, maar het vlees is rot" werd na een retourreis van Engels naar Russisch en terug.18
Tegen het midden van de jaren '60 groeide de scepsis van de overheidsinstanties die MT-onderzoek financierden over het gebrek aan vooruitgang. In 1964 vormden ze de Automatic Language Processing Advisory Committee (ALPAC), voorgezeten door John R. Pierce van Bell Labs, om een grondige evaluatie van het veld uit te voeren.24 Het uiteindelijke rapport van de commissie, gepubliceerd in 1966, was een doodsteek voor de MT-onderzoeks gemeenschap.24
Het ALPAC-rapport was een nuchtere, pragmatische en veroordelende beoordeling. Het concludeerde dat machinevertaling langzamer, minder nauwkeurig en aanzienlijk duurder was dan menselijke vertaling.5 De commissie vond geen bewijs van een dringende vraag naar vertalingen die niet al werd vervuld door de bestaande voorraad menselijke vertalers; in feite waren er op dat moment geen openstaande overheidsvertalerposities.25 Kritisch gezien nam het rapport een zeer strikte definitie van succes aan: "volledig automatische hoogwaardige vertaling," of FAHQT, wat betekende dat er nuttige output moest worden geproduceerd zonder enige menselijke nabewerking. Volgens deze standaard stelde het rapport onomwonden vast: "er is geen machinevertaling van algemene wetenschappelijke tekst geweest, en er is geen onmiddellijke vooruitzicht."25
In plaats van voortgezette financiering voor wat het als een mislukte onderneming beschouwde, adviseerde ALPAC dat de overheidssteun moest worden omgeleid. Het pleitte voor een focus op fundamenteel onderzoek in computationele taalkunde om de fundamentele aard van taal beter te begrijpen, en voor de ontwikkeling van praktische machine-ondersteunde tools om de productiviteit van menselijke vertalers te verhogen, zoals geautomatiseerde glossaria en betere bewerkingssoftware.24 De impact van het rapport was onmiddellijk en verwoestend. De Amerikaanse overheid accepteerde de aanbevelingen en elimineerde vrijwel alle financiering voor academisch MT-onderzoek. De bezuinigingen waren zo ernstig dat het veld bijna twee decennia in een diepe freeze ging in de Verenigde Staten, wat een krachtige en afschrikwekkende boodschap naar de bredere wetenschappelijke gemeenschap stuurde: AI kon niet voldoen aan zijn meest hooggeprofileerde beloften.4

De Britse Inquisitie: Het Lighthill Rapport van 1973

Terwijl het ALPAC-rapport een enkele toepassing van AI chirurgisch ontmantelde, kwam enkele jaren later een tweede, bredere kritiek op in het Verenigd Koninkrijk. In 1972, bezorgd over de onenigheid binnen zijn AI-onderzoeks gemeenschap en op zoek naar een objectieve evaluatie van de voortgang van het veld, gaf de British Science Research Council (SRC) opdracht aan Sir James Lighthill om een beoordeling uit te voeren.30 Lighthill was een zeer gerespecteerde toegepaste wiskundige, een figuur van immense wetenschappelijke autoriteit, maar hij had geen eerdere ervaring met kunstmatige intelligentie—een feit dat werd gezien als een garantie voor zijn objectiviteit.32 Zijn rapport uit 1973, formeel getiteld
Artificial Intelligence: A General Survey, gaf een diep pessimistisch oordeel dat effectief een AI-winter in Groot-Brittannië zou triggeren.30
Lighthill's analytische benadering was om AI-onderzoek in drie verschillende categorieën te verdelen, die hij A, B en C noemde 6:
Categorie A (Geavanceerde Automatisering): Dit omvatte de praktische toepassingszijde van AI, waaronder taken zoals probleemoplossing en informatieophaling. Lighthill concludeerde dat het werk in deze categorie slechts beperkte successen had behaald, en alleen binnen zeer beperkte, kunstmatige omgevingen die bekend staan als "microwerelden." Hij was bijzonder kritisch over het feit dat deze systemen enorme, handgecodeerde kennisbases nodig hadden om te functioneren, wat ze onpraktisch maakte voor echte wereldproblemen.30
Categorie C (Computer-gebaseerd onderzoek naar het centrale zenuwstelsel): Dit verwees naar het gebruik van computers om neurofysiologische en psychologische processen te simuleren, in wezen computationele neurowetenschap en psychologie. Lighthill was ondersteunend aan deze lijn van onderzoek, en beschouwde het als een waardevolle wetenschappelijke onderneming.6
Categorie B (Robotica): Dit was de "brug" categorie, bedoeld om het toepassingsgerichte werk van Categorie A te verbinden met de wetenschappelijke modellering van Categorie C. Lighthill beoordeelde dit gebied als een bijna totale mislukking. Hij citeerde de teleurstellende prestaties van robotica-projecten van die tijd, zoals de Freddy II-robot van de Universiteit van Edinburgh, en merkte op dat schaakprogramma's nog steeds niet beter waren dan menselijke amateurs.30
De centrale en meest schadelijke kritiek van het Lighthill Rapport was de focus op de mislukking van AI om het probleem van "combinatoriële explosie" op te lossen.30 Dit is de fundamentele wiskundige realiteit dat naarmate problemen complexer worden, het aantal mogelijke toestanden of paden naar een oplossing in een astronomisch tempo groeit. Lighthill betoogde krachtig dat hoewel AI-technieken misschien effectief leken te werken op de triviale "speelgoedproblemen" die populair waren in onderzoeks laboratoria—zoals de "blocks world" microworld aan MIT, waar een gesimuleerde robotarm eenvoudige geometrische vormen manipuleerde—deze methoden volledig zouden falen wanneer ze werden opgeschaald naar de complexiteit van de echte wereld.4 Deze kritiek weerklonk het fundamentele probleem van wereldkennis dat de machinevertaling had verdoemd; het was een diep, theoretisch probleem voor het hele paradigma van vroege AI.
De conclusies van het Lighthill Rapport waren zo controversieel dat ze leidden tot een beroemde televisie-debat aan de Royal Institution in Londen op 9 mei 1973. Het evenement stelde Sir James Lighthill tegenover een team van toonaangevende AI-onderzoekers, waaronder Donald Michie uit Edinburgh, Richard Gregory en de Amerikaanse pionier John McCarthy.30 Ondanks een geestdriftige verdediging van hun veld, was de impact van het rapport beslissend. Het "veroorzaakte een enorme vertrouwenscrisis" in AI binnen de Britse academische wereld en de overheid.31 Het rapport vormde de primaire basis voor de beslissing van de Britse overheid om de financiering drastisch te verlagen en de steun voor AI-onderzoek in alle maar een paar universiteiten te beëindigen, waardoor de AI-gemeenschap in het VK in zijn eigen diepe winter werd gedompeld.5

De Koude Neerdaling

De ALPAC- en Lighthill-rapporten waren de belangrijkste katalysatoren die officieel het begin van de eerste AI-winter markeerden, maar ze waren niet de enige factoren. Het intellectuele klimaat was al aan het verschuiven. Een significante, hoewel vaak verkeerd begrepen, gebeurtenis was de publicatie in 1969 van het boek Perceptrons door Marvin Minsky en Seymour Papert. Het boek bood een rigoureus wiskundig bewijs van de fundamentele beperkingen van enkele-laags perceptrons, een vroeg type neuraal netwerk. Het toonde beroemd aan dat ze niet in staat waren om eenvoudige functies zoals de XOR-logische operatie te leren. Hoewel de kritiek specifiek was voor een eenvoudige architectuur, werd het breed geïnterpreteerd als een veroordeling van de gehele connectionistische (neuraal netwerk) benadering, wat effectief het onderzoek en de financiering meer dan een decennium lang afleidde van dit paradigma.6
Bovendien was de ruwe rekencapaciteit die nodig was om zelfs de symbolische AI-benaderingen van die tijd na te streven een belangrijke bottleneck. De computers van de jaren '60 en vroege jaren '70 hadden simpelweg niet de geheugen- en verwerkingssnelheid om de combinatoriële explosie aan te kunnen die Lighthill had geïdentificeerd.1 Dit technologische plafond betekende dat zelfs veelbelovende ideeën niet konden worden opgeschaald.
Ten slotte veranderden de politieke winden in de Verenigde Staten. De Mansfield Amendment van 1973 vereiste dat het Ministerie van Defensie alleen onderzoek financierde met directe, missie-georiënteerde toepassingen. Dit drukte DARPA om zich te verschuiven van het soort blue-sky, langetermijn fundamenteel onderzoek dat zijn vroege AI-financiering had gekarakteriseerd.12 Als gevolg hiervan begon DARPA zijn brede academische AI-subsidies te verminderen, wat werd geïllustreerd door de teleurstellende resultaten en daaropvolgende stopzetting van het ambitieuze Speech Understanding Research-programma aan de Carnegie Mellon Universiteit.5 De samenvloeiing van deze factoren—veroordelende officiële rapporten, fundamentele theoretische kritiek, hardwarebeperkingen en verschuivende overheidsprioriteiten—creëerde de perfecte storm. De gouden eeuw van AI was voorbij, en de eerste winter was begonnen.5
De eerste AI-winter was meer dan alleen een technologische tegenslag; het vertegenwoordigde een crisis van epistemologie voor het jonge veld. De kernmislukking was niet slechts een gebrek aan voldoende rekencapaciteit, maar een diepgaande miscalculatie over de aard van intelligentie zelf. Vroege onderzoekers opereerden grotendeels onder een rationalistische filosofie, waarbij ze intelligentie beschouwden als een systeem van formele, logische regels die ontdekt, gearticuleerd en in een machine geprogrammeerd konden worden.18 Ze geloofden dat complexe, intelligente gedragingen konden voortkomen uit relatief eenvoudige algoritmen die opereerden in zorgvuldig gedefinieerde en beperkte "microwerelden," zoals de gesimuleerde blokken op een tafel.7 De ALPAC- en Lighthill-rapporten waren de scherpe, empirische weerlegging van deze wereldvisie. Ze toonden onmiskenbaar aan dat taken in de echte wereld zoals taalvertaling en robotica geen op zichzelf staande logische puzzels waren. In plaats daarvan waren deze taken diep "gevestigd" in een complexe wereld en vereisten ze immense hoeveelheden impliciete, contextuele, alledaagse kennis die niet gemakkelijk kon worden geformaliseerd of geprogrammeerd. De "combinatoriële explosie" was de wiskundige symptoom van deze diepere filosofische fout. De winter was dus niet alleen een bezuiniging op de financiering; het was een gedwongen paradigma verschuiving, die het veld wegduwde van de elegante puurheid van logica en naar het rommelige, moeilijke probleem van kennisrepresentatie.
Hoewel de ALPAC- en Lighthill-rapporten vaak worden voorgesteld als de primaire oorzaken van de winter, is het nauwkeuriger om ze te zien als zowel een oorzaak als een symptoom van een groeiende teleurstelling. Overheidsinstanties zoals DARPA hadden miljoenen dollars in AI-onderzoek gepompt op basis van de gedurfde beloften van zijn pioniers.12 Tegen het midden van de jaren '60 voor machinevertaling en de vroege jaren '70 voor algemene AI, werd het pijnlijk duidelijk voor deze financiers dat de vooruitgang was gestagneerd en de beloofde doorbraken nergens in zicht waren.4 De opdracht voor deze kritische rapporten was waarschijnlijk gemotiveerd door een dringende behoefte om voortdurende, enorme uitgaven te rechtvaardigen in het licht van steeds magere resultaten. De rapporten gaven de officiële, autoritatieve rechtvaardiging voor een financieringscorrectie die al politiek en economisch noodzakelijk aan het worden was. Ze creëerden de winter niet uit het niets; ze gaven het een naam en een formele rechtvaardiging.
Deze cyclus van teleurstelling werd versterkt door een fenomeen dat bekend zou komen te staan als het "AI-effect." Dit verwijst naar de neiging dat de doelpalen van "ware" intelligentie verschuiven zodra een machine een bepaalde taak beheerst. Zodra een complexe capaciteit met succes is geautomatiseerd, wordt deze vaak afgedaan als "slechts berekening" in plaats van echte intelligentie.4 Schaken, ooit beschouwd als een hoogtepunt van menselijke intellect, werd gewoon een "boomzoekprobleem" zodra computers zoals Deep Blue het op grootmeester-niveau konden spelen.35 Dit effect betekende dat AI-onderzoekers in een constante strijd waren tegen hun eigen successen. Het publiek en financiers, wiens verwachtingen werden gevormd door sciencefictioncreaties zoals HAL 9000 uit 2001: A Space Odyssey (een film waarvoor Marvin Minsky zelf als technisch adviseur diende), verwachtten sentiente, bewuste machines.8 Toen het veld systemen zoals ELIZA leverde, die uiteindelijk gewoon slimme sets van patroonherkenningsregels waren, werd het gevoel van teleurstelling vergroot, ongeacht de oprechte technische prestatie die het vertegenwoordigde.12

Tabel 1: Fundamentele Rapporten van de Eerste AI-winter

Kenmerk
ALPAC Rapport (1966) 24
Lighthill Rapport (1973) 30
Gecommissioneerd door
Amerikaanse overheid (ALPAC-commissie)
Britse Science Research Council (SRC)
Primaire focus
Machinevertaling (MT), specifiek Russisch-naar-Engels
Algemene staat van AI-onderzoek in het VK
Belangrijkste negatieve bevindingen

- MT is langzamer, duurder en minder nauwkeurig dan menselijke vertaling. - Geen bewijs van een grote onvervulde behoefte aan vertaling. - Vooruitgang in het decennium sinds het Georgetown-experiment was minimaal. - "Geen machinevertaling van algemene wetenschappelijke tekst... en geen onmiddellijke vooruitzicht."
- AI heeft zijn "grandioze doelstellingen" niet bereikt. - AI heeft het probleem van "combinatoriële explosie" niet opgelost en kan niet opschalen buiten "microwerelden." - Onderzoek in Categorie 'B' (robotica, bruggenbouw tussen automatisering en cognitieve wetenschap) was een mislukking.
Kernkritiek
Pragmatisch/Economisch: AI faalde in een kosten-batenanalyse voor een specifieke, risicovolle overheidsopdracht.
Theoretisch/Fundamenteel: AI-methoden waren fundamenteel gebrekkig en konden de complexiteit van de echte wereld niet aan.
Directe consequentie
Drastische vermindering van de Amerikaanse overheidfinanciering voor MT-onderzoek gedurende bijna twee decennia.
Stopzetting van de financiering voor AI-onderzoek aan de meeste Britse universiteiten, wat een "AI-winter" in het VK triggerde.

Deel II: De Tweede Winter – De Ineenstorting van de Commerciële Droom (c. 1987–1993)

Na een dooi in de vroege jaren '80, gedreven door de commerciële belofte van een nieuw AI-paradigma, zette een tweede, wreedere winter in. Deze keer was de mislukking niet primair in het academische laboratorium, maar in de meedogenloze wereld van de markt. Het was een winter die werd getriggerd door de plotselinge ineenstorting van een gespecialiseerde hardware-industrie en de pijnlijke realisatie dat het vastleggen en codificeren van menselijke expertise een veel moeilijker en duurder voorstel was dan iemand had gedacht. De tweede winter was de barst van AI's eerste grote commerciële bubbel.

De AI Lente: De Opkomst van de Expertensystemen (1980-1987)

De jaren '80 zagen een opmerkelijke renaissance voor kunstmatige intelligentie, grotendeels gedreven door een strategische rebranding. Om de stigma's van de eerste winter te ontvluchten, werd het veld vaak gepresenteerd onder nieuwe namen zoals "kennisgebaseerde systemen" of, het meest succesvol, "expertensystemen".7 Deze systemen vertegenwoordigden een van de eerste werkelijk succesvolle vormen van commerciële AI-software, die de technologie uit het laboratorium en de bedrijfswereld inbracht.36
Het kernidee, gepromoot door figuren zoals Edward Feigenbaum van Stanford, was een vertrek van de grote ambitie om algemene intelligentie te creëren. In plaats daarvan waren expertensystemen gericht op het oplossen van problemen door de gespecialiseerde kennis en heuristische "als-dan" regels van menselijke experts binnen een zeer smal domein vast te leggen en te codificeren.37 Vroege academische systemen zoals MYCIN, dat bacteriële bloedinfecties diagnosticeerde, en Dendral, dat organische chemische structuren identificeerde, demonstreerden het potentieel van deze kennisgebaseerde aanpak.7
De commerciële bloei werd aangewakkerd door een handvol spectaculaire successen die bewezen dat de technologie tastbare financiële rendementen kon opleveren. Het uithangbord voor dit nieuwe tijdperk was het XCON (eXpert CONfigurer) systeem, ontwikkeld aan de Carnegie Mellon Universiteit voor Digital Equipment Corporation (DEC).2 XCON automatiseerde de uiterst complexe en foutgevoelige taak van het configureren van klantbestellingen voor DEC's VAX-computersystemen, waarbij ervoor werd gezorgd dat alle noodzakelijke componenten compatibel en inbegrepen waren. Het systeem was een enorm succes, dat DEC naar verluidt een geschat bedrag van $40 miljoen per jaar bespaarde door verhoogde nauwkeurigheid en efficiëntie.2
Het succes van XCON en andere vroege systemen leidde tot een ware goudkoorts. Tegen 1985 gaven bedrijven over de hele wereld meer dan een miljard dollar uit aan AI, waarvan het merendeel werd geïnvesteerd in het creëren van interne expertensystemenafdelingen om verschillende bedrijfsprocessen te automatiseren.2 Er werd geschat dat tweederde van de Fortune 500-bedrijven actief gebruik maakte van de technologie.37 Deze fervor gaf geboorte aan een volledig ecosysteem van AI-startups, waaronder softwarebedrijven die "expertensysteem shells" verkochten zoals Teknowledge en Intellicorp (wiens product KEE was), die frameworks aanboden om het bouwen van deze systemen gemakkelijker te maken.2

De Lisp Machine Bubbel Barst

Een cruciaal element van deze bloei was de hardware waarop het draaide. De dominante programmeertaal voor AI-onderzoek in de Verenigde Staten was LISP (LISt Processing), een krachtige symbolische taal die door John McCarthy was uitgevonden.2 Echter, de unieke kenmerken van LISP, zoals dynamische typing en automatische garbage collection, maakten het berucht inefficiënt op de standaard computerarchitecturen van die tijd, die waren geoptimaliseerd voor talen zoals FORTRAN.4
Deze prestatiekloof creëerde een lucratieve nichemarkt voor zeer gespecialiseerde hardware die bekend stond als "Lisp Machines." Dit waren computers met op maat ontworpen processors en architecturen die specifiek waren geoptimaliseerd om LISP-code efficiënt uit te voeren. Een hele industrie, geleid door bedrijven zoals Symbolics, LISP Machines Inc. (LMI) en Xerox, groeide rond het bouwen en verkopen van deze dure, high-performance werkstations aan de bedrijven en universiteiten die aan de voorhoede van de expertensystemenrevolutie stonden.2
De markt voor deze gespecialiseerde hardware stortte met verbluffende snelheid in 1987 in, wat het begin van de tweede AI-winter markeerde.2 De ineenstorting werd gedreven door een klassiek geval van technologische disruptie vanuit twee richtingen:
De Opkomst van Algemeen Doel Hardware: De "PC-revolutie" en de opkomst van krachtige en steeds betaalbaardere engineeringwerkstations van bedrijven zoals Sun Microsystems boden een overtuigend alternatief. Deze algemene machines, gebouwd op commodity-componenten, haalden snel de prestaties van de gespecialiseerde Lisp-machines in. Tegen 1987 waren high-end desktopcomputers van Apple en IBM even krachtig als de duurdere Lisp-machines, en boden ze een eenvoudigere en populairdere architectuur.2
De Opkomst van Draagbare Software: Tegelijkertijd ontwikkelden softwarebedrijven zoals Lucid Inc. en Franz Inc. zeer geoptimaliseerde en krachtige LISP-compilers en omgevingen die op elke standaard UNIX-gebaseerde workstation konden draaien. Dit doorbrak de cruciale afhankelijkheid van op maat gemaakte hardware; bedrijven konden nu hun AI-toepassingen ontwikkelen en draaien op dezelfde Sun- of DEC-werkstations die hun andere ingenieurs gebruikten, waardoor de noodzaak voor een aparte, kostbare hardware-ecosysteem werd geëlimineerd.2
De waardepropositie voor Lisp-machines verdampte bijna van de ene op de andere dag. Een hele industrie die een waarde van een half miljard dollar had, werd effectief in een enkel jaar vervangen.2 Tegen het begin van de jaren '90 was de markt verwoest. De meeste van de pionierende Lisp-machinebedrijven, waaronder Symbolics, LMI en Lucid, waren failliet gegaan of hadden het veld verlaten, en werden een schoolvoorbeeld van een gespecialiseerde technologieplatform die werd overtroffen door de onophoudelijke vooruitgang van algemene computing.2

De Broosheid van Expertise

Terwijl de hardwaremarkt implodeerde, onthulden de expertensystemen zelf diepgewortelde en uiteindelijk fatale gebreken. Tegen het begin van de jaren '90 werd het duidelijk dat zelfs de meest succesvolle systemen, zoals XCON, onhoudbaar werden. De kernproblemen waren inherent aan de kennisgebaseerde aanpak:
De Kennisverwervingsbottleneck: Het proces van het extraheren van de nodige kennis van menselijke experts en deze zorgvuldig codificeren in duizenden "als-dan" regels was ongelooflijk traag, moeilijk en duur. Het was een belangrijke bottleneck die de schaalbaarheid van de aanpak beperkte.6
Broosheid: De systemen waren fundamenteel rigide en inflexibel. Omdat ze niet konden leren of redeneren vanuit eerste principes, waren ze "broos"—ze zouden spectaculair falen of bizarre, onzinnige antwoorden produceren wanneer ze werden geconfronteerd met enige situatie, hoe klein ook, die buiten hun enorme maar eindige set van vooraf geprogrammeerde regels viel.2 Ze konden simpelweg de inherente rommeligheid en variabiliteit van de echte wereld niet aan.44
De Onderhoudsnachtmerrie: Het bijwerken en onderhouden van deze complexe regelgebaseerde systemen bleek een Herculeaanse taak te zijn. Het toevoegen van een enkele nieuwe regel kon onvoorziene en kettingreacties met duizenden bestaande regels hebben, waardoor de kennisbasis fragiel en bijna onmogelijk te debuggen of betrouwbaar te wijzigen was.2 De ironie was dat systemen die waren ontworpen om expertise te automatiseren hun eigen teams van hoogbetaalde menselijke experts nodig hadden om ze te onderhouden. Op het hoogtepunt had DEC naar verluidt een toegewijd personeel van 59 mensen nodig om zijn interne expertensystemen draaiende te houden.7
Het Kwalificatieprobleem: De expertensystemenaanpak viel ten prooi aan een probleem in logica en filosofie dat jaren eerder was geïdentificeerd: het kwalificatieprobleem. Het is praktisch onmogelijk om alle noodzakelijke voorwaarden (kwalificaties) te formuleren voor een bepaalde regel om correct van toepassing te zijn in de echte wereld. Dit maakte redeneren over iets anders dan de meest beperkte domeinen computationeel onhandelbaar en foutgevoelig.2

Het Einde van Grote Ambities

De commerciële klap werd weerspiegeld door de hooggeprofileerde mislukkingen van grootschalige, door de overheid gefinancierde AI-initiatieven die met veel tamtam aan het begin van het decennium waren gelanceerd.
Japan's Fifth Generation Computer Systems Project (1981-1992): Dit ambitieuze, door de overheid geleide project van $850 miljoen was een belangrijke katalysator voor de AI-bloei, en wekte angsten in het Westen over een Japanse overname in computing.7 Het doel was om bestaande technologie te overtreffen door computers te creëren die in staat waren tot mensachtige redenering, natuurlijke taalbegrip en massaal parallelle verwerking. Tegen de tijd dat het project in 1992 eindigde, was het er niet in geslaagd zijn primaire AI-doelen te bereiken en was de gekozen hardware-architectuur grotendeels vervangen door ontwikkelingen in de VS, wat het markeerde als een grote strategische teleurstelling.5
DARPA's Strategic Computing Initiative (SCI) (1983-1993): Direct geïnspireerd door het Japanse project, was de SCI een miljard-dollar DARPA-programma dat aan het Amerikaanse Congres werd verkocht met beloften van tastbare militaire toepassingen, zoals autonome tanks en assistenten voor piloten.7 Het doel van de initiatief was om expertensystemen, natuurlijke taalverwerking en machinevisie te integreren om een machine te creëren die "kan zien, horen, spreken en denken als een mens".42 Tegen het einde van de jaren '80 was het echter duidelijk dat de onderliggende AI-technologieën niet volwassen genoeg waren om te slagen. DARPA, geconfronteerd met bezuinigingen en teleurstellende resultaten, begon de financiering "diep en wreed" te snijden. Deze zet had een kettingreactie-effect, waardoor belangrijke aannemers zoals Symbolics, die sterk afhankelijk waren van SCI-financiering, ernstig werden beschadigd en de ineenstorting van de industrie versnelde.5
De samenvloeiing van de ineenstorting van de Lisp-machine-markt, de wijdverspreide teleurstelling van bedrijven met broze en kostbare expertensystemen, en de publieke mislukking van deze enorme overheidsprojecten dompelde AI onder in zijn tweede, en in veel opzichten diepere, winter.2
De tweede AI-winter vertegenwoordigde een mislukking van de commercialisatiestrategie, een klassieke technologiebubbel die barstte onder het gewicht van zijn eigen hype. In tegenstelling tot de eerste winter, die voornamelijk een crisis van academische theorie was, was de tweede een zakelijke mislukking. Een oprecht nuttige, zij het beperkte, technologie—regelgebaseerde systemen voor smalle domeinen—werd oversold als een panacee die alle vormen van menselijke expertise kon vervangen.2 Deze overtuigende pitch, gecombineerd met de angst voor Japanse technologische suprematie, leidde ertoe dat durfkapitalisten en bedrijven enorme sommen geld in het veld pompten, waardoor een onhoudbare markt voor AI-software en -hardware ontstond.2 De Lisp-machine-industrie was een "picks and shovels" spel op deze goudkoorts, maar het lot ervan was onlosmakelijk verbonden met een enkel, propriëtair softwareparadigma. Toen goedkopere, flexibele algemene hardware—de PC- en workstationrevolutie—opkwam, werd het hele gespecialiseerde hardware-ecosysteem bijna van de ene op de andere dag verouderd.2 Dit was geen mislukking van AI-theorie in isolatie, maar een brute markcorrectie gedreven door het fundamentele economische principe dat algemene, commodity-technologie bijna altijd triomfeert boven dure, gespecialiseerde oplossingen.
Onder deze commerciële mislukking lag echter een diepere technische continuïteit met de eerste winter. De centrale uitdaging die expertensystemen verdoemde—de "kennisverwervingsbottleneck"—was een directe afstammeling van de "combinatoriële explosie" die door het Lighthill Rapport was geïdentificeerd.6 Het Lighthill Rapport had de combinatoriële explosie aangestipt als de onvermogen van AI om verder te schalen dan microwerelden vanwege het astronomische aantal mogelijkheden dat inherent is aan problemen in de echte wereld.30 De kennisverwervingsbottleneck was hetzelfde probleem in een nieuwe gedaante. In plaats van dat het systeem een oneindige ruimte van mogelijke acties moest doorzoeken, moest de menselijke "kennisingenieur" handmatig een effectief oneindige set regels anticiperen en codificeren om alle real-world contingenties te dekken—de essentie van het "kwalificatieprobleem".2 Beide winters stammen voort uit dezelfde worteloorzaak: de immense, impliciete en moeilijk te verwoorden aard van kennis in de echte wereld. De eerste winter onthulde dat machines deze regels niet automatisch konden ontdekken door middel van zoekmethoden; de tweede winter onthulde dat mensen ze zelfs niet met de hand konden opschrijven op een manier die efficiënt, robuust of onderhoudbaar was.
De ineenstorting van de expertensystemenbubbel was zo ernstig en zo openbaar dat de term "AI" zelf meer dan een decennium toxisch werd in de zakenwereld. Bedrijven die miljoenen hadden geïnvesteerd in "AI-laboratoria" en expertensysteemsoftware zagen weinig rendement op hun investering en sloten deze initiatieven.2 Het "AI"-merk werd synoniem met dure, overgehypete en mislukte projecten.4 Echter, het onderliggende onderzoek naar alternatieve, datagestuurde benaderingen stopte niet. Om financiering veilig te stellen en de stigma's die aan de term "AI" waren verbonden te vermijden, hernoemden onderzoekers en bedrijven strategisch hun werk. Waardevolle vooruitgang ging door onder andere namen zoals "machine learning," "patroonherkenning," "data mining," en "informatics".4 Dit was een cruciale terugtrekking die de zaden voor de volgende AI-zomer in relatieve stilte kon zaaien, maar het fragmentiseerde ook het veld en vertraagde de terugkeer van "AI" als een respectabele commerciële term voor bijna twee decennia.

Tabel 2: Een Verhaal van Twee Winters - Katalysatoren en Gevolgen

Dimensie
Eerste AI Winter (c. 1966–1980)
Tweede AI Winter (c. 1987–1993)
Primaire Hype
Algemene Probleemoplossing, Machinevertaling, Mensachtige Redenering 12
"Kenisgebaseerde" Expertensystemen 2
Belangrijkste Katalysatoren
ALPAC Rapport (1966): Verklaarde machinevertaling een mislukking. 24

Lighthill Rapport (1973): Bekritiseerde de theoretische fundamenten van AI. 30
Ineenstorting van de Lisp Machine Markt (1987): Gespecialiseerde hardware werd verouderd. 2

Mislukking van Expertensystemen: Bewezen broos, duur en moeilijk te onderhouden. 2
Kern Technologische Hurdle
De Combinatoriële Explosie: Onvermogen van logica-gebaseerde zoekmethoden om verder te schalen dan "microwerelden." 30
De Kennisverwervingsbottleneck: Onmogelijk om de enorme, impliciete regels van real-world domeinen handmatig te codificeren. 6
Primaire Arena van Mislukking
Academische Onderzoeks Labs & Overheidsgefinancierde Projecten
Corporate Marketplace & Commerciële Ondernemingen
Belangrijkste Overheidsinitiatieven
Vroege DARPA-financiering voor fundamenteel onderzoek 19
DARPA's Strategic Computing Initiative (SCI), Japan's Fifth Generation Project 40
Gevolg & Les
Een verschuiving weg van de "algemene intelligentie" droom naar meer gerichte problemen. De realisatie dat intelligentie enorme kennis vereist.
Een verschuiving weg van handgecodeerde kennis naar automatisch leren uit gegevens. De realisatie dat propriëtaire, gespecialiseerde platforms kwetsbaar zijn voor algemene commodity-technologie.

Deel III: De Stille Dooi en de Data-gedreven Tsunami (c. 1993–2012)

De tweede AI-winter dwong een diepgaande en noodzakelijke heroriëntatie van het veld. De grote, top-down ambities van symbolische AI, die probeerden intelligentie te creëren door middel van logica en handgecodeerde regels, maakten plaats voor een pragmatische, bottom-up benadering die geworteld was in statistieken en machine learning. Deze periode, een "stille dooi" die duurde van de vroege jaren '90 tot de vroege jaren 2010, werd gekenmerkt door minder kop-grabbende hype en meer door het gestage, methodische werk van het bouwen van solide wiskundige en computationele fundamenten. Onderzoekers concentreerden zich op het oplossen van specifieke, goed gedefinieerde problemen met meetbaar succes. Dit tijdperk legde de cruciale basis voor de huidige AI-zomer, die culmineerde in een reeks baanbrekende prestaties die de onmiskenbare kracht van een nieuw, data-gedreven paradigma demonstreerden.

Een Paradigma Verschuiving: Van Logica naar Statistieken

De herhaalde mislukkingen van de jaren '70 en '80 leidden tot een wijdverspreide afwijzing van wat bekend werd als "Good Old-Fashioned AI" (GOFAI), een paradigma gebaseerd op symbolische manipulatie en Booleaanse (Waar/Fout) logica.34 Een nieuwe consensus ontstond: intelligente systemen konden niet in een vacuüm worden gebouwd. Ze moesten geworteld zijn in gegevens uit de echte wereld en in staat zijn om ambiguïteit en onzekerheid aan te kunnen.34
De jaren '90 en vroege jaren 2000 zagen de opkomst van statistische machine learning.5 De focus van het veld verschoof dramatisch. In plaats van de ongrijpbare droom na te jagen van het creëren van veelzijdige, denkende machines, concentreerden onderzoekers zich op het bouwen van systemen die specifieke, geïsoleerde problemen konden oplossen met een hoge mate van prestatie en wetenschappelijke verantwoordelijkheid.14 Deze nieuwe pragmatisme werd mogelijk gemaakt door twee krachtige seculiere trends: de explosie van gegevens die beschikbaar werden gesteld door het internet en de onophoudelijke, exponentiële groei in de kracht van commodity computers.34
Een belangrijke ontwikkeling die de geest van dit tijdperk belichaamde, was de Support Vector Machine (SVM). Ontwikkeld door Vladimir Vapnik en zijn collega's bij AT&T Bell Laboratories in de jaren '90, is de SVM een krachtige supervised learning-algoritme voor classificatie.50 Het kernprincipe van een SVM is om de optimale hypervlak—een lijn, vlak of hoger-dimensionale equivalent—te vinden die de grootste mogelijke marge of "straat" creëert tussen de gegevenspunten van verschillende klassen.53 Door deze marge te maximaliseren, is het algoritme waarschijnlijker om goed te generaliseren naar nieuwe, ongeziene gegevens. Bovendien, door een wiskundige techniek die bekend staat als de "kernel trick," konden SVM's efficiënt complexe, niet-lineaire problemen aan door de gegevens impliciet in een hoger-dimensionale ruimte te mappen waar een lineaire scheiding mogelijk wordt.52 SVM's werden jarenlang een dominant hulpmiddel in de toolkit van de machine learning-praktijk, waarbij ze state-of-the-art resultaten behaalden op taken zoals tekstclassificatie, beeldherkenning en bio-informatica, en vertegenwoordigden de nieuwe focus op rigoureuze, wiskundige en data-gedreven methoden.52

Mijlpaal 1: Deep Blue (1997) – De Triomf van Brute Kracht

In mei 1997 werd een belangrijke mijlpaal in de geschiedenis van AI bereikt toen IBM's schaakspelende supercomputer, Deep Blue, de regerende wereldkampioen schaken, Garry Kasparov, versloeg in een zes-game match onder standaard toernooi-regels.54 Het evenement was een wereldwijde media-sensatie, die een doel bereikte dat sinds de oprichting van het veld als een maatstaf voor kunstmatige intelligentie had gediend en een van Herbert Simon's decennia-oude voorspellingen vervulde.21
Echter, Deep Blue was geen product van het opkomende statistische machine learning-paradigma. Integendeel, het was de ultieme uitdrukking—de apotheose—van de oude symbolische, "brute kracht" benadering.54 Deep Blue was een massaal parallelle IBM RS/6000 SP supercomputer, aangevuld met 480 op maat ontworpen VLSI "schaakchips" die hardwired waren om schaakspecifieke berekeningen uit te voeren.55 Deze gespecialiseerde hardware stelde het in staat om een verbazingwekkende 200 miljoen schaakposities per seconde te evalueren.54 Zijn "intelligentie" was een combinatie van deze immense zoekcapaciteit—een sterk geoptimaliseerde implementatie van een boomzoekalgoritme met behulp van minimax en alpha-beta pruning—en een geavanceerde evaluatiefunctie. Deze functie, samen met een openingsboek dat meer dan 4.000 posities en 700.000 grootmeesterpartijen bevatte, was zorgvuldig handmatig afgesteld door een team van computerwetenschappers en menselijke schaakgrootmeesters.35
De overwinning van Deep Blue was symbolisch immens. Het bewees dat een machine de beste menselijke speler kon verslaan in een spel dat lange tijd als een bastion van intellectuele diepgang en creativiteit werd beschouwd. Toch waren de methoden ervan sterk gespecialiseerd, op maat gemaakt voor een enkele taak, en niet gemakkelijk generaliseerbaar naar andere problemen.55 In zekere zin vertegenwoordigde Deep Blue de magnifieke piek van één paradigma van AI net op het moment dat een ander, krachtiger paradigma op het punt stond over te nemen.

Mijlpaal 2: AlexNet (2012) – De Deep Learning Revolutie

Als de moderne AI-zomer een enkele, definitieve startpunt heeft, is het 30 september 2012. Op die dag won een convolutioneel neuraal netwerk (CNN) genaamd AlexNet, gemaakt door Alex Krizhevsky, Ilya Sutskever en Geoffrey Hinton aan de Universiteit van Toronto, niet alleen de jaarlijkse ImageNet Large Scale Visual Recognition Challenge (ILSVRC)—het verpletterde de concurrentie.60 AlexNet bereikte een top-5 foutpercentage van 15,3%, wat betekent dat het de label van een afbeelding binnen zijn top vijf gissingen 84,7% van de tijd correct identificeerde. Dit was een verbazingwekkende sprong in prestaties, meer dan 10 procentpunten beter dan het foutpercentage van de runner-up van 26,2%, dat was gebaseerd op meer traditionele computer vision-technieken.60
Het succes van AlexNet was zo diepgaand dat mede-AI-pionier Yann LeCun het beschreef als een "onmiskenbaar keerpunt in de geschiedenis van computer vision".60 Deze revolutie was niet het resultaat van een enkele uitvinding, maar eerder de krachtige convergentie van drie belangrijke technologische factoren die jarenlang onafhankelijk waren gerijpt 60:
Big Data: De kritische faciliterende factor was het bestaan van de ImageNet-dataset. Gecreëerd door een team onder leiding van Stanford-professor Fei-Fei Li, was ImageNet een enorme, gratis en zorgvuldig door mensen gelabelde corpus van miljoenen afbeeldingen met hoge resolutie in duizenden categorieën. Het bood de hoogwaardige trainingsgegevens op een schaal die volledig onbeschikbaar was geweest in eerdere tijdperken van AI-onderzoek.60
Krachtige Hardware: Het trainen van een diep neuraal netwerk met 60 miljoen parameters was een computationeel immense taak. Het succes van AlexNet werd mogelijk gemaakt door het gebruik van Graphics Processing Units (GPU's). Krizhevsky trainde het netwerk op twee Nvidia GTX 580 consumenten-gaming GPU's, waarbij hij hun massaal parallelle architectuur gebruikte om de matrixvermenigvuldigingen die centraal stonden in de training van neurale netwerken te versnellen. Dit maakte het mogelijk om het 8-laagse netwerk in een redelijke tijd te trainen, een taak die onhandelbaar zou zijn geweest op de CPU's van die tijd.60
Algoritmische Verbeteringen: AlexNet integreerde en populariseerde met succes verschillende belangrijke algoritmische technieken die standaardpraktijk werden in deep learning. Het gebruikte de Rectified Linear Unit (ReLU) als activatiefunctie, die veel sneller trainde dan de traditionele sigmoid- of tanh-functies door het "vervagende gradient" probleem te mitigeren. Het gebruikte ook dropout, een regularisatietechniek waarbij willekeurige neuronen tijdens elke trainingsstap worden genegeerd om te voorkomen dat het model overfitting op de trainingsgegevens. Ten slotte gebruikte het on-the-fly data-augmentatie (zoals het bijsnijden en omdraaien van afbeeldingen) om de grootte van de trainingsset kunstmatig uit te breiden.60
De impact van AlexNet was onmiddellijk en transformerend. Het bewees onomstotelijk de superioriteit van deep learning voor complexe perceptietaken. Binnen enkele jaren was het hele veld van computer vision overgestapt van het moeizame proces van handmatige functie-engineering (waarbij experts algoritmen ontwierpen om randen, texturen, enz. te detecteren) naar end-to-end deep learning, waarbij het netwerk de relevante kenmerken rechtstreeks uit de ruwe pixelgegevens leert. Dit enkele evenement ontketende de huidige explosie in AI-onderzoek, ontwikkeling en commerciële investeringen.60

Mijlpaal 3: AlphaGo (2016) – De Dageraad van Creatieve AI

Als Deep Blue een overwinning vertegenwoordigde van brute kracht en AlexNet een overwinning van data-gedreven patroonherkenning, dan vertegenwoordigde DeepMind's AlphaGo een nieuwe grens in kunstmatige intelligentie. In maart 2016, in een reeks wedstrijden die door miljoenen mensen over de hele wereld werden bekeken, versloeg AlphaGo Lee Sedol, een 18-voudig wereldkampioen en een van de grootste Go-spelers in de geschiedenis, met een beslissende score van 4-1.65 Het resultaat zorgde voor schokgolven in zowel de AI- als de Go-gemeenschappen. Het oude spel Go, met zijn diepgaande strategische diepgang en een astronomisch aantal mogelijke bordposities (verre overschrijdend het aantal atomen in het universum), werd algemeen beschouwd als de "grote uitdaging" voor AI. De meeste experts geloofden dat een machine die in staat was om een top menselijke professional te verslaan, minstens een decennium verwijderd was.67
AlphaGo's architectuur was een briljante en nieuwe synthese van meerdere AI-technieken. In zijn hart combineerde het diepe neurale netwerken met een geavanceerd zoekalgoritme dat bekend staat als Monte Carlo tree search (MCTS).67 Het systeem gebruikte twee hoofd neurale netwerken:
Een "policy network" dat was getraind om de meest veelbelovende volgende zetten te voorspellen, waardoor de enorme zoekruimte effectief werd verkleind.
Een "value network" dat was getraind om een gegeven bordpositie te evalueren en de uiteindelijke winnaar van het spel te voorspellen.66
Cruciaal was dat AlphaGo's leerproces veel verder ging dan simpelweg het imiteren van menselijke experts. Het systeem werd eerst "gebootstrapped" door te trainen op een database van 30 miljoen zetten uit menselijke expertpartijen. Zodra het een redelijk niveau van bekwaamheid had bereikt, werd het verder getraind via een proces van versterkingsleren. DeepMind zette duizenden instanties van AlphaGo tegen elkaar in een enorme interne toernooi. Door tegen zichzelf te spelen en van zijn eigen fouten te leren, kon AlphaGo nieuwe strategieën ontdekken en een dieper begrip van het spel ontwikkelen dan ooit een mens had bereikt.66
Deze capaciteit werd het meest beroemd gedemonstreerd in de tweede wedstrijd tegen Lee Sedol met de nu legendarische "Zet 37." AlphaGo speelde een zet op de vijfde lijn die zo ongebruikelijk en schijnbaar amateuristisch was dat menselijke commentatoren het aanvankelijk als een fout afwezen. Pas later in het spel werd de subtiele genialiteit ervan duidelijk, omdat het een cruciale zet bleek te zijn in het veiligstellen van AlphaGo's overwinning. Dit moment kristalliseerde het idee dat AI niet alleen menselijke kennis kon beheersen, maar ook oprecht nieuwe, creatieve en mooie inzichten kon genereren die de grenzen van menselijk begrip konden uitbreiden.67
De overwinning van AlphaGo werd als veel significanter beschouwd dan die van Deep Blue. Terwijl de methoden van Deep Blue sterk gespecialiseerd waren, werd de combinatie van deep learning en versterkingsleren van AlphaGo gezien als een veel algemenere benadering van probleemoplossing, wat tastbare vooruitgang signaleerde richting het langetermijndoel van Artificial General Intelligence (AGI).67 Het evenement werd een "Sputnik-moment" voor regeringen over de hele wereld, met name China, dat zijn nationale investering in AI-onderzoek dramatisch verhoogde in de nasleep ervan.67
De doorbraken van de jaren 2010 waren op geen enkel eerder moment in de geschiedenis mogelijk omdat ze de gelijktijdige rijping van drie onafhankelijke technologische curves vereisten. AlexNet is het canonieke voorbeeld van deze convergentie.60 Ten eerste, zonder de creatie van enorme, gratis beschikbare en zorgvuldig gelabelde datasets zoals ImageNet (de datacurve), zou er niets zijn geweest om deze modellen op de vereiste schaal te trainen.61 Ten tweede, zonder de rijping van de parallelle verwerkingskracht van GPU's, die voornamelijk waren ontwikkeld voor de consumentenvideogamemarkt (de hardwarecurve), zouden de computationele kosten van training onhandelbaar zijn geweest.62 Ten derde, zonder de decennia van langzame, stille vooruitgang in neurale netwerkalgoritmen—waaronder de uitvinding van backpropagation, de ontwikkeling van convolutionele architecturen en de verfijning van regularisatietechnieken zoals dropout (de algoritmische curve)—zou de beschikbare data en hardware verspild zijn.62 De "revolutie" van 2012 was in feite een onvermijdelijke samenvloeiing, een moment waarop alle drie de noodzakelijke componenten tegelijkertijd een kritische drempel van capaciteit en toegankelijkheid overschreden.
Hoewel vaak gepresenteerd als een volledige breuk met het verleden, bevatten de overwinningen van Deep Blue, AlexNet en AlphaGo elk een duidelijke afstamming van de symbolische AI-paradigma's die hen voorafgingen. Deep Blue was bijna puur GOFAI—een massaal zoekalgoritme dat opereerde over een symbolische, regelgebaseerde ruimte.55 AlphaGo, hoewel revolutionair in zijn gebruik van deep learning, gebruikte nog steeds een Monte Carlo tree search-algoritme in zijn kern, een directe afstammeling van de heuristische zoekmethoden die in de jaren '50 waren gepionierd.66 Zijn belangrijkste innovatie was om deze klassieke zoekmethode te begeleiden met moderne neurale netwerken. Zelfs AlexNet, het icoon van het nieuwe statistische paradigma, loste uiteindelijk een classificatieprobleem op, een kern taak die al een focus was van vroeg onderzoek naar patroonherkenning. Dit toont een duidelijke evolutie aan, geen volledige vervanging. De kracht van moderne AI komt precies voort uit deze integratie: het combineren van de zoek- en representatie-ideeën van het symbolische tijdperk met de krachtige leer- en generalisatiecapaciteiten van de statistische en connectionistische tijdperken.
Elke van deze belangrijke mijlpalen diende ook om het publieke narratief rond AI en de relatie met menselijke intelligentie te herdefiniëren. De overwinning van Deep Blue werd gepresenteerd als een klassiek "man versus machine" gevecht, een strijd van rauwe berekening waarbij de brute kracht van de machine uiteindelijk de menselijke genialiteit overweldigde.54 Het succes van AlexNet ging minder over een directe wedstrijd en meer over het demonstreren van nut; het toonde aan dat machines een fundamentele sensorische taak (visie) beter konden uitvoeren dan elk eerder geautomatiseerd systeem, waardoor de deur werd geopend naar talloze praktische toepassingen.62 De overwinning van AlphaGo was de meest diepgaande. Vanwege zijn schijnbaar "creatieve" en "intuitieve" zetten werd het niet alleen gepresenteerd als een machine die kon berekenen, maar als een die kon begrijpen en innoveren op een manier die mensen daadwerkelijk nieuwe dingen kon leren.67 Het narratief verschuift van AI als een eenvoudig hulpmiddel voor berekening naar AI als een potentiële bron van nieuwe inzichten, waarbij de lijnen tussen berekening en creativiteit vervagen en serieuze discussies over de weg naar AGI opnieuw worden aangewakkerd.

Tabel 3: Mijlpalen van het Moderne AI-tijdperk

Mijlpaal
Jaar
Belangrijke Technologie Gedemonstreerd
Betekenis & Impact
IBM's Deep Blue
1997
Massaal Parallelle Symbolische AI / Brute-Kracht Zoek: Aangepaste hardware voor snelle evaluatie van schaakposities. 54
Symbolische Overwinning: Eerste machine die een regerende wereldkampioen schaken versloeg in een standaardwedstrijd. Bewees de kracht van gespecialiseerde, brute-kracht berekening voor een goed gedefinieerd probleem. Vertegenwoordigde de piek van "Good Old-Fashioned AI." 35
AlexNet
2012
Diepe Convolutionele Neurale Netwerken (CNN's): Combinatie van een diepe architectuur, GPU-versnelling, grootschalige gegevens (ImageNet) en algoritmische verbeteringen (ReLU, Dropout). 60
De Deep Learning Revolutie: Bereikte een enorme sprong in de nauwkeurigheid van beeldherkenning, verschuivend het veld van handmatige functie-engineering naar end-to-end leren. Ontstak de huidige AI-bloei en investering. 60
DeepMind's AlphaGo
2016
Diepe Versterkingsleren: Een nieuwe combinatie van diepe neurale netwerken (beleid en waarde netwerken) met Monte Carlo Tree Search, getraind via zelfspel. 66
Creatieve & Generaliseerbare AI: Versloeg een top menselijke Go-speler, een taak die als een decennium verwijderd werd beschouwd. Demonstreerde de mogelijkheid om nieuwe, "creatieve" strategieën te leren en ontdekken die verder gaan dan menselijke kennis. Signaleerde vooruitgang richting meer algemene leer systemen. 67

Deel IV: De Huidige Zomer – Navigeren door Ongeëvenaarde Hype en Het Gezicht van Nieuwe Grenzen

De doorbraken van de jaren 2010 hebben de huidige "AI-zomer" ingeluid, een tijdperk van ongekende investeringen, publieke aandacht en technologische mogelijkheden. Deze golf wordt gedefinieerd door de opkomst van generatieve modellen, fundamentmodellen, en vooral, grote taalmodellen (LLM's), die opmerkelijke vaardigheden hebben aangetoond in het verwerken en produceren van mensachtige tekst, afbeeldingen en code. Echter, deze explosieve bloei gaat gepaard met een groeiende en intense discussie over de lange termijn duurzaamheid ervan. Sommige van de meest gerespecteerde pioniers van het veld, veteranen van eerdere winters, waarschuwen nu voor nieuwe, fundamentele grenzen aan de horizon en de mogelijkheid van een derde, ingrijpende AI-winter.

De Generatieve AI-explosie

Het huidige AI-landschap wordt gedomineerd door generatieve modellen, met name de transformer-gebaseerde architecturen die systemen zoals OpenAI's GPT-serie aandrijven.68 Deze modellen, getraind op enorme hoeveelheden van het internet, kunnen vloeiende en coherente tekst genereren, talen vertalen, software schrijven en realistische afbeeldingen creëren op basis van eenvoudige prompts. Dit heeft geleid tot een andere enorme golf van hype, met wijdverspreide voorspellingen dat deze technologie elke industrie en elk aspect van de samenleving fundamenteel zal revolutioneren.5
Deze opwinding heeft zich vertaald in investeringen op een schaal die alles wat in eerdere AI-cycli is gezien overtreft. In 2024 overschreed de wereldwijde durfkapitaalfinanciering voor AI-bedrijven de $100 miljard, waardoor AI de leidende sector voor investeringen wereldwijd werd, een stijging van meer dan 80% ten opzichte van 2023.69 In de eerste helft van 2025 alleen al hebben Amerikaanse AI-startups een verbluffende $104,3 miljard opgehaald, terwijl de door VC gesteunde exits in totaal slechts $36 miljard bedroegen, wat wijst op een enorme instroom van kapitaal in het ecosysteem.70 Dit niveau van financiële betrokkenheid, gedreven door zowel particuliere investeerders als de strategische imperatieven van technologiegiganten, overschrijdt ver de miljard-dollar bloei van de jaren '80 expertensystemenperiode.

De Spook van een Derde Winter: Argumenten voor een Vertraag

Ondanks de onmiskenbare vooruitgang en financiële fervor, begint een groeiende koor van prominente onderzoekers, analisten en AI-veteranen de duurzaamheid van het huidige paradigma in twijfel te trekken. Ze beweren dat de strategie van simpelweg het opschalen van modellen, gegevens en rekencapaciteit tegen fundamentele grenzen aanloopt, en dat het veld mogelijk op weg is naar een andere winter, of in ieder geval een significante en pijnlijke vertraging. De belangrijkste argumenten voor deze pessimistische kijk zijn onder andere:
De Data Bottleneck: De huidige benadering van het trainen van steeds grotere LLM's op enorme hoeveelheden tekst nadert een harde fysieke limiet: de eindige voorraad van hoogwaardige, door mensen gegenereerde gegevens die beschikbaar zijn op het publieke internet.71
Gedetailleerde analyses, zoals een prominente studie uit 2022 van onderzoekers van EpochAI, projecteren dat de voorraad hoogwaardige tekstgegevens volledig uitgeput zou kunnen zijn voor trainingsdoeleinden ergens tussen 2026 en 2032.74 Deze tijdlijn wordt versneld als modellen "overgetraind" worden op dezelfde gegevens meerdere keren. Trainen op gegevens van lagere kwaliteit of op synthetische gegevens die door andere AI's zijn gegenereerd, heeft aangetoond dat het leidt tot afnemende rendementen, een verlies van diversiteit, en een fenomeen dat bekend staat als "model ineenstorting" of "Habsburg AI," waarbij het model begint zijn eigen artefacten en fouten te leren, wat zijn prestaties degradeert.74
Afnemende Opbrengsten van Opschaling: Jarenlang verbeterde de prestaties van LLM's voorspelbaar met toenames in modelgrootte, datasetgrootte en rekencapaciteit, een fenomeen dat bekend staat als "neural scaling laws." Echter, er is groeiend bewijs dat deze wetten afnemende rendementen opleveren.77 Elke volgende verdubbeling van rekencapaciteit en gegevens produceert steeds kleinere winsten in capaciteit. Bovendien worden de financiële en milieukosten van het trainen van deze enorme modellen onhoudbaar. De energie die nodig is om een state-of-the-art model te trainen is enorm, en de vraag naar gespecialiseerde hardware zoals GPU's overtreft ver de aanbod, wat een belangrijke economische bottleneck creëert.76
De Fundamentele Kloof Tussen LLM's en Ware Intelligentie: Een kernkritiek van veel toonaangevende onderzoekers is dat het huidige LLM-paradigma, gebaseerd op auto-regressieve next-token voorspelling, fundamenteel beperkt is en niet kan leiden tot Artificial General Intelligence (AGI).
Critici zoals roboticus Rodney Brooks beweren dat LLM's verfijnde "meesterlijke leugenaars" zijn. Ze zijn meesterlijk in het leren van de statistische correlaties tussen woorden en het imiteren van de vorm van menselijke taal, maar ze missen elk waarachtig onderliggend model van de wereld, begrip van causaliteit of gezond verstand. Zoals gedetailleerd in Bijlage L, is Brooks' kritiek dat LLM's niet gegrond zijn, tokens manipuleren die geen inherente betekenis voor de machine hebben, een directe parallel met de ongegronde symbolen van de klassieke AI.
Yann LeCun, de Chief AI Scientist van Meta, is een andere prominente skepticus van de LLM-only aanpak. Hij betoogt dat deze modellen, die alleen zijn getraind op het "lage-bandwidth" medium van tekst, nooit ware intelligentie kunnen bereiken. Zoals uitgelegd in Bijlage L, stelt hij dat ze een waarachtig begrip van de wereld missen en vatbaar zijn voor "confabulaties" omdat ze geen oprechte redeneer- of planningsvaardigheden bezitten. Hij gelooft dat verdere vooruitgang een fundamenteel nieuw paradigma vereist dat verder gaat dan eenvoudige opschaling, en werkt actief aan alternatieve architecturen zoals JEPA om voorspellende wereldmodellen te bouwen.
De Verbrede "Hype versus Realiteit" Kloof: Zoals in elke vorige cyclus, zijn de beloften die worden gedaan over de huidige technologie ver vooruit op de werkelijke, vaak broze, capaciteiten. De hype rond AGI die "om de hoek" staat of LLM's die een breed scala aan professionele banen vervangen, stelt verwachtingen die waarschijnlijk niet op korte termijn zullen worden waargemaakt. Dit creëert een aanzienlijk risico van teleurstelling bij investeerders en het publiek wanneer de beloofde enorme productiviteitswinsten niet materialiseren, wat mogelijk leidt tot een scherpe correctie in financiering en interesse.3

Waarom Deze Keer Anders Zou Kunnen Zijn: Argumenten Tegen een Winter

Aan de andere kant van het debat beweren velen dat hoewel de huidige hype misschien overmatig is, een ernstige en langdurige winter op de schaal van eerdere gebeurtenissen onwaarschijnlijk is. De fundamenten van de huidige AI-zomer, beweren ze, zijn veel solider.
Tastbare Commerciële Waarde en Wijdverspreide Adoptie: In tegenstelling tot de AI van eerdere tijdperken, die grotendeels beperkt was tot academische laboratoria of niche-ondernemingen, is de AI van vandaag diep geïntegreerd in de wereldeconomie en het consumentenleven. Het drijft kernfuncties aan van producten die door miljarden mensen worden gebruikt, zoals zoekmachines, sociale media feeds en smartphone persoonlijke assistenten. Het biedt ook duidelijke en meetbare rendementen op investeringen (ROI) voor een breed scala aan specifieke zakelijke taken, van fraudedetectie tot automatisering van klantenservice.1 Deze wijdverspreide, praktische nut creëert een stabiele en aanhoudende vraag die simpelweg niet bestond in de jaren '70 of '80.
Massale en Verankerde Investeringen: De schaal van investeringen in het huidige AI-ecosysteem is van een orde van grootte groter en meer verankerd dan in enige eerdere cyclus. De grootste en meest winstgevende bedrijven ter wereld—Google, Meta, Microsoft, Apple, Amazon—hebben hun toekomstige strategische richting op AI gebaseerd. Ze zijn betrokken in een felle technologische en talentenwapenwedloop, bouwen hele bedrijfsmodellen rond AI en investeren jaarlijks tientallen miljarden dollars in onderzoek, ontwikkeling en infrastructuur.44 Dit niveau van strategische toewijding van wereldwijde economische grootmachten, samen met belangrijke overheidsinitiatieven, maakt een volledige terugtrekking van financiering en een volledige onderzoeksinval zeer onwaarschijnlijk.89
Aanpasbare Modellen en Volwassen Infrastructuur: De AI-modellen van vandaag zijn veel flexibeler en aanpasbaarder dan de rigide, handgecodeerde expertensystemen van de jaren '80. Technieken zoals fine-tuning en transfer learning stellen een enkel groot fundamentmodel in staat om te worden aangepast aan een breed scala van downstream-taken met relatief weinig moeite.88 Bovendien is de cloud-gebaseerde infrastructuur voor het trainen en implementeren van deze modellen (zoals Google's TPUs en Amazon's AWS) volwassen, schaalbaar en breed toegankelijk, waardoor de drempel voor het ontwikkelen van nieuwe toepassingen wordt verlaagd.47

De Stemmen van de Pioniers: Een Spectrum van Opvattingen

Het debat over de toekomst van AI wordt misschien het beste vastgelegd door de verschillende opvattingen van drie van zijn meest invloedrijke pioniers, elk een "peetvader" van het veld op zijn eigen manier.
Geoffrey Hinton (De Bezorgde Profeet): Na tientallen jaren pionierswerk aan neurale netwerken is Hinton een van de meest prominente en gerespecteerde stemmen van voorzichtigheid geworden. Hij heeft beroemd zijn positie bij Google in 2023 neergelegd zodat hij vrijer kon spreken over de existentiële risico's die hij gelooft dat AI met zich meebrengt.90 Hij schat nu dat er een kans van 10-20% is dat AI kan leiden tot de uitsterving van de mensheid, en betoogt dat we nog nooit eerder hebben moeten omgaan met het creëren van "dingen die intelligenter zijn dan wijzelf" en geen bewezen plan hebben om ze te controleren.90 Naast het existentiële risico waarschuwt hij ook dat onder de huidige economische systemen de enorme productiviteitswinsten van AI waarschijnlijk de ongelijkheid in rijkdom zullen verergeren door werknemers te vervangen zonder een sociaal vangnet te bieden, waarbij de winsten naar de rijken vloeien in plaats van naar degenen die hun banen verliezen.83
Yann LeCun (De Pragmaticus Realist): LeCun, een andere Turing Award-winnende pionier van deep learning, is diep sceptisch over de huidige hype rond LLM's en de opvatting dat AGI nabij is. Zoals gedetailleerd in Bijlage L, betoogt hij krachtig dat auto-regressieve LLM's tegen een muur aanlopen en niet in staat zijn om ware intelligentie te bereiken omdat ze zijn getraind op lage-bandwidth tekstgegevens en niet kunnen redeneren, plannen of interne "wereldmodellen" bouwen. Hij ziet de weg naar menselijke AI als lang en vereist fundamenteel nieuwe architecturen, zoals de Joint-Embedding Predictive Architecture (JEPA) die hij ontwikkelt bij Meta. Terwijl hij een correctie in de overgeblazen verwachtingen voor LLM's verwacht, gelooft hij niet dat een ernstige, veld-brede AI-winter waarschijnlijk is, gezien de bewezen nut van de technologie in veel gebieden.
Rodney Brooks (De Gegronde Skepticus): Een veteranen-robotica en mede-oprichter van iRobot, heeft Brooks meerdere AI-hypecycli zien komen en gaan, en hij bekijkt de huidige met een gezonde dosis scepsis. Zoals opgemerkt in Bijlage L, betoogt hij dat intelligentie fysieke verankering vereist en dat onbelichaamde LLM's gewoon "meesterlijke leugenaars" zijn. Hij heeft de afkorting "FOBAWTPALSL" (Fear of Being a Wimpy Techno-Pessimist and Looking Stupid Later) bedacht om de kudde-mentaliteit en het gebrek aan kritisch denken te beschrijven die hij ziet als de drijvende kracht achter de huidige bloei. Zijn kernkritiek is dat AI-systemen, en LLM's in het bijzonder, niet "gegrond" zijn in de fysieke realiteit; ze missen causale begrip en zijn daarom fundamenteel onbetrouwbaar. Hij voorspelt dat een AI-winter of in ieder geval een significante vertraging op komst is, aangezien de beloften van exponentiële vooruitgang onvermijdelijk botsen met de veel langzamere, lineaire realiteiten van het implementeren van complexe en betrouwbare systemen in de echte wereld.
Het intense debat over een mogelijke derde AI-winter kan worden gezien als een proxy-oorlog tussen twee concurrerende filosofieën van intelligentie. Het "pro-winter" of sceptische kamp, vertegenwoordigd door figuren zoals LeCun en Brooks, argumenteert vanuit een constructivistische invalshoek. Ze geloven dat ware intelligentie de engineering van specifieke cognitieve architecturen vereist die interne modellen van de wereld kunnen bouwen, causaliteit kunnen begrijpen en robuust redeneren.81 Vanuit dit perspectief zijn LLM's verfijnde maar uiteindelijk oppervlakkige patroonherkenners die de grenzen hebben bereikt van wat kan worden bereikt zonder dit diepere, gestructureerde begrip. In tegenstelling hiermee argumenteert het "anti-winter" of optimistische kamp vanuit een pragmatische, emergentistische invalshoek. Ze beweren dat het enorme commerciële succes en de verrassend veelzijdige capaciteiten die zijn voortgekomen uit simpelweg het opschalen van gegevens en berekeningen bewijs zijn van een levensvatbaar pad vooruit, zelfs als de onderliggende mechanismen nog niet volledig worden begrepen.44 Dit debat weerklinkt de oude symbolische versus connectionistische verdeeldheden van het verleden, maar op een veel grotere en ingrijpender schaal. Het is een fundamentele onenigheid over de vraag of intelligentie iets is dat zorgvuldig moet worden ontworpen of iets dat gewoon kan ontstaan.
De economische structuur van het moderne AI-ecosysteem creëert ook een nieuwe dynamiek. In de jaren '80 bestond de AI-industrie uit talrijke kleinere, gespecialiseerde startups zoals Symbolics, die zeer kwetsbaar waren voor marktschokken en technologische verschuivingen.2 Tegenwoordig wordt de grens van AI-onderzoek en -ontwikkeling overweldigend gedomineerd door een handvol technologiebedrijven van een biljoen dollar.70 Deze enorme concentratie van kapitaal, talent en rekencapaciteit biedt een krachtige buffer tegen een totale financieringsinval. Deze bedrijven hebben de financiële veerkracht om R&D door een neergang te ondersteunen en hebben AI zo diep in hun kernstrategieën geïntegreerd dat een volledige terugtrekking bijna ondenkbaar is.44 Echter, deze oligopolie zou ook kunnen leiden tot een andere, subtielere soort winter: een "winter van innovatie." Als deze technologiegiganten allemaal convergeren op hetzelfde dominante paradigma (bijvoorbeeld het opschalen van transformer-architecturen) en hun marktmacht hen in staat stelt om startups die meer radicale, concurrerende benaderingen nastreven te verwerven of te marginaliseren, zou het veld kunnen stagnatie ondervinden door een gebrek aan intellectuele diversiteit, zelfs in de afwezigheid van een grote financieringscrisis.
Ten slotte evolueert het dreigende "data-schaarste" probleem van een puur technische limiet naar een complex juridisch en ethisch strijdtoneel. De eerste golven van LLM's werden getraind op gegevens die van het "publieke" internet waren gehaald, een ethisch grijze maar grotendeels onbetwiste bron.61 Terwijl deze bron van gegevens opdroogt, wenden bedrijven zich tot nieuwe bronnen: synthetische gegevens die door andere AI's zijn gegenereerd, gelicentieerde privé-datasets en de enorme, niet-publieke gegevens van het "deep web".75 Elk van deze paden presenteert diepgaande uitdagingen. Trainen op puur synthetische gegevens brengt het risico van "model-in-een-storting" en een degradatie van kwaliteit met zich mee.76 Het gebruik van gelicentieerde of privé-gegevens is enorm kostbaar en roept aanzienlijke privacyzorgen op.94 En het gebruik van auteursrechtelijk beschermd materiaal in trainingsdatasets is al het onderwerp van belangrijke, industrie-vormende rechtszaken van kunstenaars, auteurs en mediabedrijven die de economie en legaliteit van AI-ontwikkeling fundamenteel kunnen veranderen.80 Daarom is het "einde van data" niet alleen

---
*Vertaald door AI. Controleer de originele Engelse versie bij twijfel.*