# Bijlage V: De Geschiedenis en Traject van Kunstmatige Intelligentie

I. Inleiding: Van Abstracte Logica naar Wereldwijde Kracht

Het veld van Kunstmatige Intelligentie (AI) heeft een rijke en complexe geschiedenis, gekenmerkt door periodes van intense optimisme, significante doorbraken en frustrerende "AI-winter". Het traject is geen eenvoudige, lineaire voortgang naar steeds slimmere machines. Integendeel, het is een dynamisch en vaak omstreden verhaal van concurrerende paradigma's, filosofische debatten en technologische beperkingen. Het begrijpen van deze geschiedenis is cruciaal voor het contextualiseren van de huidige ontwikkelingen, het waarderen van de schaal van recente prestaties en het anticiperen op de diepgaande uitdagingen die voor ons liggen. 
Het verhaal van AI kan worden begrepen door de lens van een centrale intellectuele spanning die sinds de oprichting ervan bestaat: het debat tussen symbolische en connectionistische benaderingen. De symbolische school, die de vroege decennia van het veld domineerde, stelt dat intelligentie voortkomt uit de manipulatie van symbolen volgens expliciete, logische regels. Het is een top-down benadering, geworteld in de overtuiging dat kennis kan worden geformaliseerd en in een machine kan worden geprogrammeerd. In tegenstelling hiermee stelt de connectionistische school dat intelligentie een emergente eigenschap is van een netwerk van eenvoudige, onderling verbonden eenheden, analoog aan de neuronen in de menselijke hersenen. Deze bottom-up benadering betoogt dat kennis niet wordt geprogrammeerd, maar wordt geleerd uit patronen in gegevens.1 Deze fundamentele onenigheid is niet louter technisch, maar filosofisch, en weerspiegelt het langdurige debat tussen rationalisme, dat de nadruk legt op aangeboren kennis en rede, en empirisme, dat de voorkeur geeft aan kennis die is verkregen uit zintuiglijke ervaring.1 De geschiedenis van AI kan worden gezien als een grote pendule die heen en weer zwaait tussen deze twee polen, waarbij de successen en mislukkingen van elk tijdperk de basis leggen voor het volgende. 
Deze geschiedenis wordt ook gekenmerkt door een terugkerende cyclus van hype en desillusie. Periodes van snelle vooruitgang en gedurfde beloften—de "AI-zomers"—leiden vaak tot opgeblazen verwachtingen en massale investeringen, om vervolgens gevolgd te worden door "AI-winter" wanneer de diepgaande moeilijkheid van de problemen en de beperkingen van de huidige technologie duidelijk werden, waardoor financiering verdampte.5 Deze cycli waren niet louter mislukkingen, maar noodzakelijke afrekeningen die het veld dwongen om zijn fundamentele aannames onder ogen te zien en nieuwe tools te ontwikkelen. Het huidige tijdperk, aangedreven door de convergentie van enorme datasets, krachtige parallelle computing en verfijnde connectionistische algoritmen, lijkt veel van de lang gekoesterde beloften van het veld waar te maken, maar brengt ook een nieuwe reeks maatschappelijke, ethische en geopolitieke uitdagingen met zich mee op wereldwijde schaal. Deze bijlage zal die reis volgen van zijn filosofische oorsprongen naar de huidige status als een transformerende wereldwijde kracht. Het begrijpen van deze geschiedenis is kiezen voor context boven speculatie, een vitale stap in het bewust navigeren van de toekomst. 
Tabel 1: Een Chronologische Tijdlijn van Belangrijke AI-mijlpalen

Jaar(len)
Mijlpaal/Evenement
Belangrijke Figuren/Organisaties
Betekenis
1943
McCulloch-Pitts Neuron
Warren McCulloch, Walter Pitts
Eerste wiskundige model van een kunstmatige neuron; toonde aan dat netwerken van eenvoudige eenheden logische functies konden berekenen.6
1950
"Computing Machinery and Intelligence"
Alan Turing
Introduceerde het "Imitation Game" (Turing Test) als criterium voor machine-intelligentie en kaderde het filosofische debat.7
1956
Dartmouth Zomer Onderzoeksproject
John McCarthy, Marvin Minsky, Claude Shannon, e.a.
Bedenker van de term "Kunstmatige Intelligentie" en vestigde het als een formeel onderzoeksveld.8
1956
Logic Theorist
Allen Newell, Herbert A. Simon, Cliff Shaw
Het eerste AI-programma; toonde geautomatiseerd redeneren door wiskundige stellingen te bewijzen.9
1958
LISP Programmeertaal
John McCarthy
Werd de dominante programmeertaal voor AI-onderzoek gedurende tientallen jaren vanwege de symbolische verwerkingscapaciteiten.10
1964-1967
ELIZA
Joseph Weizenbaum
Vroeg natuurlijk taalverwerkingsprogramma dat een psychotherapeut simuleerde, wat de "Eliza-effect" benadrukte.11
1966-1972
Shakey de Robot
SRI International
De eerste mobiele robot die perceptie, redeneren (met behulp van STRIPS-planner) en fysieke actie integreerde.12
1973
Lighthill Rapport
Sir James Lighthill
Een kritisch rapport dat de mislukking van AI om de "combinatorische explosie" te overwinnen benadrukte, leidend tot zware bezuinigingen op de financiering in het VK en bijdragend aan de eerste AI-winter.13
1970s
MYCIN
Stanford Universiteit
Een expert systeem dat infectieziekten diagnosticeerde, wat het potentieel van kennisgebaseerde AI toonde en zekerheidfactoren gebruikte om onzekerheid te hanteren.14
1980
Chinese Kamer Argument
John Searle
Een zeer invloedrijk filosofisch gedachte-experiment dat de claim uitdaagt dat symbolische manipulatie voldoende is voor begrip ("Sterke AI").15
1986
Backpropagation Gepopulariseerd
David Rumelhart, Geoffrey Hinton, Ronald Williams
Een paper die het backpropagation-algoritme populair maakte, waardoor het praktisch werd om meerlaagse neurale netwerken te trainen, wat de connectionistische benadering nieuw leven inblies.16
1987
Instorting van de LISP Machine Markt
Symbolics, LMI, e.a.
De markt voor gespecialiseerde AI-hardware stortte in, wat het begin van de tweede AI-winter markeerde.17
1997
Deep Blue Verslaat Kasparov
IBM
Een schaakspelende supercomputer versloeg de regerende wereldkampioen, een mijlpaal voor de "brute-force" symbolische benadering.18
2009
ImageNet Dataset Vrijgegeven
Fei-Fei Li, e.a. (Stanford/Princeton)
Een enorme, hoogwaardige gelabelde afbeeldingsdataset die de katalysator werd voor de deep learning-revolutie in computer vision.19
2012
AlexNet Wint ImageNet Challenge
Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton
Een diep convolutioneel neuraal netwerk dat dramatisch beter presteerde dan concurrenten, wat de moderne deep learning-boom inluidde.21
2016
AlphaGo Verslaat Lee Sedol
DeepMind (Google)
Een AI-programma versloeg een wereldkampioen Go-speler, wat de kracht van deep reinforcement learning in een complex strategisch spel demonstreerde.23
2017
"Attention Is All You Need" Paper
Google Brain
Introduceerde de Transformer-architectuur, die herhaling verving door zelf-aandacht, waardoor het schalen van grote taalmodellen mogelijk werd.24
2020
GPT-3 Vrijgegeven
OpenAI
Een taalmodel met 175 miljard parameters dat verbazingwekkende mogelijkheden in tekstgeneratie demonstreerde, wat de opkomst van foundation-modellen markeerde.
2022
Diffusie Modellen Verschijnen
OpenAI, Stability AI, Midjourney
Vrijgave van DALL-E 2, Stable Diffusion en Midjourney, wat een revolutie teweegbracht in hoogwaardige AI-beeldgeneratie.26
2024
Natuurlijk Multimodale Modellen
OpenAI, Google
Vrijgave van modellen zoals GPT-4o, die tekst-, audio-, beeld- en video-invoer en -uitvoer binnen een enkel neuraal netwerk verwerken, wat real-time, mensachtige interactie mogelijk maakt.28
2024-2025
Geavanceerde Redeneringsmodellen
OpenAI
Vrijgave van de O3-serie, die significante sprongen in prestaties op complexe redenering, wiskunde en wetenschap benchmarks toonde.30

II. De Genesis van een Veld (1940s–1950s): Logica, Automaten en de Dageraad van Denkende Machines

De intellectuele oorsprongen van Kunstmatige Intelligentie gaan vooraf aan de uitvinding van de digitale computer, met wortels in filosofie, wiskunde en logica. De jaren 1940 en 1950 zagen echter deze abstracte ideeën samenvallen met de nieuwe wetenschap van berekeningen, wat de formele basis legde voor een veld dat zich richtte op het mechaniseren van intelligentie. Het fundamentele DNA van deze nieuwe discipline was niet leren van gegevens, maar formele logica. De vroegste mijlpalen waren allemaal geworteld in de overtuiging dat de processen van denken konden worden vastgelegd door de nauwkeurige, regelgebaseerde manipulatie van symbolen, een benadering die het traject van het veld voor de volgende drie decennia zou bepalen.

De Logische Calculus van de Geest

Een cruciaal moment vond plaats in 1943 met de publicatie van "A Logical Calculus of the Ideas Immanent in Nervous Activity" door neurofysioloog Warren McCulloch en logicus Walter Pitts.6 Dit paper introduceerde een vereenvoudigd wiskundig model van een biologisch neuron, dat bekend kwam te staan als het McCulloch-Pitts neuron. Hun model was geen leersysteem; het was eerder een binaire apparaat dat "vuurde" als de som van zijn gewogen ingangen een bepaalde drempel overschreed. Door deze eenvoudige eenheden in netwerken te verbinden, toonden McCulloch en Pitts aan dat ze in principe elke logische functie konden implementeren, zoals EN, OF en NIET.6 
De betekenis van dit werk was diepgaand. Het was de eerste die voorstelde dat de cognitieve processen van de hersenen in computationele termen konden worden begrepen, wat een conceptuele brug tussen neurobiologie en berekeningen vestigde. Diep beïnvloed door de symbolische logica van denkers zoals Rudolf Carnap, Alfred North Whitehead en Bertrand Russell, behandelde het paper de hersenen niet als een biologisch substraat voor leren, maar als een logisch apparaat voor redeneren.6 Dit vestigde de fundamentele premisse dat denken kon worden gemecaniseerd, wat de basis legde voor de opkomst van AI als discipline.

"Kunnen Machines Denken?"

In 1950 publiceerde de Britse wiskundige en codekraker Alan Turing zijn baanbrekende paper, "Computing Machinery and Intelligence," in het filosofietijdschrift Mind.7 Hierin confronteerde hij de diep ambiguë vraag: "Kunnen machines denken?" Turing erkende de nutteloosheid van het definiëren van termen als "machine" en "denken" en stelde voor om de vraag te vervangen door een concrete, operationele test die hij het "Imitation Game" noemde.7 
Het oorspronkelijke spel omvatte drie menselijke spelers: een man (A), een vrouw (B) en een ondervrager (C) van elk geslacht. De ondervrager, geïsoleerd van de andere twee, probeert te bepalen wie de man is en wie de vrouw door schriftelijke vragen te stellen. Het doel van de man is om de ondervrager te misleiden, terwijl het doel van de vrouw is om te helpen. Turing vroeg zich vervolgens af: "Wat zal er gebeuren wanneer een machine de rol van A in dit spel overneemt?".7 In deze gewijzigde versie, nu beroemd bekend als de Turing Test, converseert een menselijke rechter met zowel een mens als een computer via een tekstgebaseerde interface. Als de rechter de machine niet betrouwbaar van de mens kan onderscheiden, wordt gezegd dat de machine de test heeft doorstaan.7 
Turings paper was een werk van opmerkelijke vooruitziendheid. Hij was niet bezorgd over de beperkte digitale computers van zijn tijd, maar over "voorstelbare computers" van de toekomst met voldoende geheugen en snelheid.7 Door het Imitation Game voor te stellen, verschuift hij het filosofische debat over bewustzijn en "denken" naar een pragmatischer discussie over prestaties en mogelijkheden. Hoewel de test door de jaren heen veel kritiek heeft gekregen, bood het het veld zijn eerste en meest blijvende filosofische maatstaf voor machine-intelligentie.

De Doop van een Discipline

Het veld van Kunstmatige Intelligentie werd formeel geboren en benoemd tijdens de zomer van 1956 op een workshop gehouden aan Dartmouth College. Georganiseerd door een jonge wiskundige genaamd John McCarthy, bracht het "Dartmouth Summer Research Project on Artificial Intelligence" de oprichters van het veld samen, waaronder Marvin Minsky, Nathaniel Rochester van IBM en de informatie-theoreticus Claude Shannon.8 
Het oprichtingsvoorstel van de workshop was gebaseerd op een gedurfde conjectuur: "dat elk aspect van leren of een andere eigenschap van intelligentie in principe zo nauwkeurig kan worden beschreven dat een machine kan worden gemaakt om het te simuleren".8 McCarthy bedacht opzettelijk de term "Kunstmatige Intelligentie" voor het voorstel, en koos het vanwege de neutraliteit om het nieuwe veld te onderscheiden van de meer gevestigde domeinen van cybernetica, die sterk geassocieerd waren met analoge feedbacksystemen, en automaten-theorie.8 De Dartmouth-workshop vestigde AI als een distinct academische discipline, en stelde de initiële onderzoeksagenda vast, die onderwerpen omvatte die tot op de dag van vandaag relevant zijn, zoals natuurlijke taalverwerking, neurale netwerken en creativiteit.8 

Vroege Implementaties

De Dartmouth-workshop was niet slechts een theoretische oefening. Allen Newell, Herbert A. Simon en programmeur Cliff Shaw arriveerden met een werkende demonstratie van wat ze de Logic Theorist noemden, een programma dat ze hadden ontwikkeld bij de RAND Corporation.9 Algemeen beschouwd als het eerste echte AI-programma, was de Logic Theorist ontworpen om geautomatiseerd redeneren uit te voeren door stellingen uit het hoofdstuk over propositionele calculus in Whitehead en Russell's monumentale werk van symbolische logica, Principia Mathematica, te bewijzen.9 
Het programma was een verbluffend succes. Het slaagde erin 38 van de eerste 52 stellingen te bewijzen, en in één geval, voor stelling 2.85, vond het een bewijs dat eleganter en directer was dan dat van Russell en Whitehead zelf.9 Simon was zelfs in staat om het nieuwe bewijs aan Bertrand Russell te tonen, die naar verluidt "met vreugde reageerde".9 De Logic Theorist was het bewijs dat een machine taken kon uitvoeren die redeneren en creativiteit vereisten, domeinen die voorheen als uitsluitend menselijk werden beschouwd. Het belichaamde de kernprincipes van de vroege symbolische benadering: intelligentie als een proces van heuristische zoektocht door een boom van logische mogelijkheden.9 
Na deze vroege successen uitvond John McCarthy de LISP (List Processing) programmeertaal in 1958.10 LISP werd al snel de lingua franca van AI-onderzoek voor tientallen jaren. Het ontwerp was revolutionair voor zijn tijd, met de introductie van concepten zoals recursie en conditionals.10 Cruciaal was LISP's vermogen om code als gegevens te behandelen—een eigenschap die bekend staat als homoiconiciteit—wat het uitzonderlijk goed geschikt maakte voor de soort symbolische manipulatie die programma's zoals de Logic Theorist vereisten. Het bood de praktische toolkit die het mogelijk maakte om de ambities van het opkomende veld te implementeren en te verkennen.10 

III. Het Symbolische Tijdperk (1960s–1970s): De Kracht en Grenzen van Kennisrepresentatie

De twee decennia na de Dartmouth-workshop worden vaak aangeduid als de "gouden eeuw" van AI-onderzoek. Deze periode werd overweldigend gedomineerd door het symbolische paradigma, later door filosoof John Haugeland "Good Old-Fashioned AI" (GOFAI) genoemd. Het centrale geloof van GOFAI was dat intelligentie kon worden bereikt door systemen te creëren die expliciete, door mensen leesbare representaties van kennis—feiten, regels en concepten—bevatten, en een formele redeneerengine om die symbolen te manipuleren.2 Het onderzoek richtte zich op gebieden die zich leenden tot deze benadering, zoals spelletjes spelen, stelling bewijzen en probleemoplossing in beperkte "microwerelden".

Mijlpalen in Symbolische AI

Simuleren van Gesprekken: ELIZA en het "Eliza Effect"

Een van de beroemdste vroege programma's was ELIZA, ontwikkeld tussen 1964 en 1967 aan MIT door Joseph Weizenbaum.11 ELIZA was een vroeg natuurlijk taalverwerkingsprogramma dat was ontworpen om menselijke-machinecommunicatie te verkennen. Het meest beroemde script, DOCTOR, simuleerde een Rogerian psychotherapeut door een eenvoudige maar opmerkelijk effectieve techniek te gebruiken: het zou sleutelwoorden in de getypte invoer van een gebruiker herkennen en deze terugreflecteren in de vorm van een niet-directionele vraag.11 Bijvoorbeeld, als een gebruiker typt "Ik voel me verdrietig," zou ELIZA kunnen antwoorden: "Hoe lang voel je je al verdrietig?" 
Weizenbaum was zich er volledig van bewust dat zijn programma geen echte begrip had; hij beschreef het later als een "elektronisch oplichtersspel" dat opereerde door middel van slimme patroonherkenning en substitutieregels.11 Wat hij niet had voorzien, was de krachtige psychologische reactie die het zou oproepen. Tot zijn verbazing begonnen gebruikers, waaronder zijn eigen secretaresse, intense emotionele banden met het programma te vormen, vertrouwend op het en het menselijke gevoelens van empathie en begrip toeschrijvend.11 Dit fenomeen, waarbij mensen de neiging hebben om computerprogramma's te anthropomorfiseren en een groter niveau van intelligentie aan te nemen dan daadwerkelijk aanwezig is, werd bekend als het "Eliza-effect".11 Weizenbaums ervaring was een profetische waarschuwing over de eenvoud waarmee de façade van intelligentie kon worden gecreëerd en de diepgaande ethische en psychologische implicaties van het doen. Deze vroege demonstratie van hoe gemakkelijk een eenvoudig script mensen kon misleiden, droeg bij aan de hype en overoptimisme van het tijdperk, wat een onrealistische verwachting creëerde van wat complexere systemen binnenkort zouden kunnen bereiken.

Integreren van Perceptie en Actie: Shakey de Robot

Terwijl ELIZA opereerde in het puur symbolische domein van tekst, had een ander baanbrekend project aan het Stanford Research Institute (SRI International) als doel om symbolisch redeneren te verbinden met de fysieke wereld. Shakey de Robot, ontwikkeld tussen 1966 en 1972, was de eerste mobiele robot die zijn omgeving waar nam, over zijn acties redeneerde en een plan uitvoerde.12 
Shakey leefde in een speciaal geconstrueerde omgeving van kamers, deuropeningen en grote houten blokken. Het was uitgerust met een televisiecamera, een laserafstandsmeter en "stootdetectoren" om zijn omgeving waar te nemen.12 Wanneer het een hoog niveau commando van een menselijke operator kreeg, zoals "duw het blok van het platform," zou Shakey zijn software gebruiken om het probleem op te splitsen in een reeks kleinere stappen. Het zou zijn camerafeed analyseren om een intern logisch model van zijn wereld op te bouwen, en vervolgens een geautomatiseerde planner genaamd STRIPS (Stanford Research Institute Problem Solver) gebruiken om een plan te bedenken om het doel te bereiken. Dit zou kunnen inhouden dat het naar een helling navigeert, de helling tegen het platform duwt, de helling oprijdt en uiteindelijk het blok duwt.12 Shakey was een baanbrekende integratie van de kernsubvelden van AI: computer vision, natuurlijke taalverwerking en logische redenering. Het project produceerde ook verschillende blijvende technische innovaties, waaronder het A* zoekalgoritme voor efficiënte padvinding en de Hough-transformatie voor functie-extractie in afbeeldingen, die beide nog steeds veel worden gebruikt.12 

De Grote Debatten: Paradigma's en Filosofieën van de Geest

De dominantie van de symbolische benadering bleef niet onbetwist. De jaren 1970 en het begin van de jaren 1980 zagen de articulatie van zowel een concurrerend technisch paradigma als een krachtige filosofische kritiek, wat de basis legde voor een debat over de aard van intelligentie dat het veld blijft vormen.

Symbolische versus Connectionistische AI

Het centrale intellectuele conflict in de geschiedenis van AI is de kloof tussen de symbolische en connectionistische scholen van gedachte. Dit is niet louter een technische onenigheid over de beste manier om intelligente systemen te bouwen, maar een fundamentele, quasi-filosofische geschil over de aard van kennis en cognitie zelf. 
Tabel 2: Symbolische AI versus Connectionistische AI: Een Vergelijkende Analyse

Kenmerk
Symbolische AI (GOFAI)
Connectionistische AI
Kernfilosofie
In lijn met het Rationalisme en de "taal van het denken." Intelligentie is een formeel proces van redeneren over expliciete kennis.1
In lijn met het Empirisme en biologische inspiratie. Intelligentie is een emergente eigenschap die wordt geleerd uit statistische patronen in gegevens.1
Kennisrepresentatie
Expliciet en Gelokaliseerd. Kennis is gecodeerd in door mensen leesbare symbolen, regels (als-dan), logica en gestructureerde netwerken (bijv. semantische netten).2
Implicit en Gedistribueerd. Kennis is opgeslagen in de numerieke gewichten van de verbindingen tussen eenvoudige verwerkingsunits (neuronen).2
Leermethode
Geprogrammeerd. Kennis wordt moeizaam geëxtraheerd van menselijke experts en handmatig gecodeerd in een kennisbasis door ingenieurs.2
Getraind. Het systeem leert automatisch door de gewichten van zijn verbindingen aan te passen op basis van blootstelling aan grote hoeveelheden voorbeeldgegevens.33
Verwerkingsstijl
Serieel en Logisch. Werkt via een stap-voor-stap inferentie-engine die regels op symbolen toepast, vergelijkbaar met een logisch bewijs.33
Parallel en Gedistribueerd. Informatie wordt gelijktijdig verwerkt over duizenden of miljoenen onderling verbonden eenheden, vergelijkbaar met de hersenen.35
Belangrijkste Sterkte
Uitlegbaarheid en Precisie. Het redeneringsproces is transparant en kan stap voor stap worden gevolgd. Uitstekend in taken met duidelijke, formele regels.2
Patroonherkenning en Aanpassingsvermogen. Uitstekend in perceptuele taken (visie, spraak) en leren van complexe, ruisachtige, real-world gegevens. Kan generaliseren naar nieuwe voorbeelden.2
Belangrijkste Zwakte
Brittleness en Schaalbaarheid. Faalt catastrofaal wanneer geconfronteerd met invoer buiten zijn vooraf geprogrammeerde kennis. Moeilijk en duur om grote kennisbasissen te creëren en te onderhouden.33
Ondoorzichtigheid ("Black Box") en Gegevenshonger. Het redeneringsproces is ondoorzichtig en moeilijk te interpreteren. Vereist enorme hoeveelheden trainingsgegevens en rekenkracht.33
Historische Voorbeelden
Logic Theorist, ELIZA, Shakey, Expert Systemen (DENDRAL, MYCIN).
Perceptron, Backpropagation, Convolutional Neural Networks (CNNs), Transformers (Deep Learning).

De Chinese Kamer

In 1980 publiceerde filosoof John Searle een gedachte-experiment dat een van de beroemdste kritieken op de claims van symbolische AI blijft: het Chinese Kamer Argument.15 Searle vroeg ons ons een persoon voor te stellen die geen Chinees begrijpt, opgesloten in een kamer. In de kamer bevindt zich een groot regelboek geschreven in het Engels en dozen gevuld met Chinese symbolen. Mensen buiten de kamer schuiven slips papier met vragen in het Chinees onder de deur. De persoon binnen, die de instructies in het Engelse regelboek volgt, vindt de bijbehorende symbolen in de dozen en geeft slips papier met de juiste antwoorden terug, ook in het Chinees.15 
Voor de waarnemers buiten lijkt het erop dat de kamer een vloeiende Chinese spreker bevat. De persoon binnen de kamer begrijpt echter geen enkel woord Chinees. Ze manipuleren simpelweg formele symbolen volgens een set regels, net zoals een computer doet.15 
Searle's conclusie was dat dit systeem het cruciale verschil aantoont tussen syntaxis (de manipulatie van symbolen) en semantiek (echte begrip van betekenis). De persoon in de kamer heeft syntaxis, maar geen semantiek. Daarom betoogde Searle dat zelfs als er een computerprogramma zou kunnen worden geschreven dat zo geavanceerd was dat het de Turing Test zou kunnen doorstaan, het niet echt taal zou begrijpen of denken op de manier waarop een mens dat doet. Het argument was een directe aanval op het concept van "Sterke AI"—de opvatting dat een geschikt geprogrammeerde computer een geest en bewustzijn kan hebben. Het verwoordde krachtig de intuïtie dat het simuleren van een intelligent proces niet hetzelfde is als het instantiëren ervan.15

IV. De AI Winters: Een Noodzakelijke Afrekening

Het initiële optimisme en de snelle vooruitgang van het symbolische tijdperk botsten uiteindelijk met de immense moeilijkheid van het creëren van ware intelligentie. Het traject van het veld werd gekenmerkt door twee grote periodes van ineenstorting in financiering en belangstelling, bekend als de "AI-winter". Dit waren niet louter mislukkingen, maar kritische correcties, geboren uit een groeiende kloof tussen ambitieuze beloften en de praktische beperkingen van zowel de onderliggende technologie als het dominante symbolische paradigma. De winters onthulden een fundamentele tekortkoming in de GOFAI-benadering: de veronderstelling dat de rommelige, genuanceerde en impliciete kennis die nodig is voor echte wereldintelligentie volledig kon worden vastgelegd in een set expliciete, handmatig gemaakte regels.

De Eerste AI Winter (midden jaren 1970 tot begin jaren 1980)

Tegen het begin van de jaren 1970 was de initiële opwinding rond AI begonnen te vervagen. Programma's die indrukwekkend werkten in sterk beperkte "microwerelden" of "speelgoeddomeinen" faalden om op te schalen naar complexere, real-world problemen. Deze mislukking was geworteld in twee fundamentele kwesties.

Het Lighthill Rapport en Combinatorische Explosie

In 1973 gaf de Britse regering, een belangrijke financier van AI-onderzoek, de vooraanstaande toegepaste wiskundige Sir James Lighthill de opdracht om een rapport op te stellen over de staat van het veld. Het resulterende "Artificial Intelligence: A General Survey," algemeen bekend als het Lighthill Rapport, was een verwoestende kritiek.13 
Lighthill's centrale argument was dat AI er niet in was geslaagd het probleem van "combinatorische explosie" aan te pakken. Dit verwijst naar het feit dat naarmate een probleem complexer wordt, het aantal mogelijke toestanden of paden naar een oplossing exponentieel groeit, waardoor elk systeem dat afhankelijk is van brute-force zoekmethoden snel overweldigd raakt.13 Terwijl AI-technieken werkten voor eenvoudige problemen met een klein aantal variabelen, werden ze computationeel onhandelbaar wanneer ze werden toegepast op de ambiguïteit en complexiteit van de realiteit. Het rapport was zeer kritisch over fundamenteel onderzoek in gebieden zoals robotica en taalverwerking, en concludeerde dat "in geen enkel deel van het veld de ontdekkingen tot nu toe de grote impact hebben geproduceerd die toen werd beloofd".13 De pessimistische conclusies van het rapport leidden direct tot zware bezuinigingen op de financiering van AI-onderzoek in de meeste Britse universiteiten, wat effectief de eerste AI-winter in het VK inluidde.13 

Technologische Bottlenecks

De kritiek op de combinatorische explosie was onlosmakelijk verbonden met de technologische realiteit van die tijd. De computers van de jaren 1970 waren simpelweg niet krachtig genoeg om de immense computationele eisen van AI-programma's aan te kunnen.38 Beperkte verwerkingssnelheden, dure geheugen en onvoldoende gegevensopslag creëerden een harde limiet op wat mogelijk was.39 Vroege AI-programma's konden alleen triviale versies van de problemen aan die ze bedoeld waren op te lossen, omdat de hardware de complexe berekeningen of het opslaan van de enorme hoeveelheden informatie die nodig waren voor meer geavanceerde benaderingen niet kon ondersteunen.5 Deze technologische beperking betekende dat de ambitieuze visies van de pioniers van het veld fundamenteel niet in overeenstemming waren met de beschikbare tools.

De Boom van Expert Systemen: Een Valse Lente

De AI-winter begon te ontdooien in het begin van de jaren 1980 met het commerciële succes van een nieuw type symbolisch AI-programma: het expert systeem. Deze systemen vertegenwoordigden een pragmatischer benadering. In plaats van te proberen het grote probleem van algemene intelligentie op te lossen, richtten ze zich op het vastleggen van de kennis van een menselijke expert in een zeer smal en specifiek domein en deze te coderen in een kennisbasis van "als-dan" regels.2 
Twee vroege expert systemen die aan de Stanford Universiteit werden ontwikkeld, toonden de belofte van deze benadering aan: 
DENDRAL, dat in 1965 begon te worden ontwikkeld maar in de jaren 1970 volwassen werd, was ontworpen om organische chemici te helpen. Het nam ruwe gegevens van een massaspectrometer en, met behulp van een kennisbasis van chemische regels, inferreerde de waarschijnlijke moleculaire structuur van de verbinding die werd geanalyseerd. Het was een van de eerste AI-systemen die een real-world wetenschappelijk probleem op een niveau van menselijke expertise oploste.41 
MYCIN, ontwikkeld in de jaren 1970, was een medisch diagnostisch systeem. Het voerde een dialoog met een arts, vroeg om patiëntgegevens en gebruikte vervolgens zijn kennisbasis van ongeveer 600 regels om de bacteriën die ernstige infecties veroorzaakten te identificeren en een behandelingskuur met antibiotica aan te bevelen. Om de inherente onzekerheid van medische diagnose aan te pakken, introduceerde MYCIN het concept van "zekerheidfactoren," een numerieke methode voor het weergeven van het vertrouwen van het systeem in zijn conclusies.14 
Het succes van systemen zoals DENDRAL en MYCIN, samen met commerciële toepassingen zoals XCON bij Digital Equipment Corporation, leidde tot een bloei. Corporaties over de hele wereld investeerden miljarden in het ontwikkelen van hun eigen interne expert systemen, en er ontstond een nieuwe industrie van gespecialiseerde hardware (LISP-machines) en software shells om hen te ondersteunen.17 

De Tweede AI Winter (eind jaren 1980 tot begin jaren 1990)

De boom van expert systemen bleek een "valse lente" te zijn. Tegen het einde van de jaren 1980 stortte de markt in en het veld ging de tweede en diepere AI-winter binnen. De mislukking was niet die van een enkel project, maar van het hele expert systemen-paradigma, dat bleek fundamenteel gebrekkig en onhoudbaar te zijn.

De Brittleness van Expertise

De kern technische mislukking van expert systemen was hun "brittleness".36 Omdat hun kennis volledig vooraf was geprogrammeerd en gebaseerd op een eindige set regels, hadden ze geen gezond verstand of het vermogen om buiten hun smalle domein te redeneren. Wanneer ze werden geconfronteerd met een ongebruikelijke of onverwachte invoer die niet perfect overeenkwam met een van hun regels, degradeerden ze niet op een elegante manier; ze faalden vaak catastrofaal, wat absurde of onzinnige resultaten opleverde.36 
Deze brittleness was een directe consequentie van de beperkingen van hun kennisrepresentatie en inferentie-engines: 
De Kennisverwervings Bottleneck: Het proces van het creëren van de kennisbasis was ongelooflijk moeilijk, tijdrovend en duur. Het vereiste "kennisingenieurs" om menselijke experts te interviewen en te proberen hun vaak impliciete, intuïtieve kennis te destilleren in een set expliciete, formele regels—een proces dat een belangrijke bottleneck bleek te zijn.44 
Onderhoud en Schaalbaarheid: De systemen waren bijna onmogelijk te onderhouden of bij te werken. De kennis was gecodeerd in programmeercode, en het toevoegen van een enkele nieuwe regel kon onvoorziene en cascaderende interacties met honderden bestaande regels hebben, waardoor het systeem instabiel werd. Ze konden niet leren of zich aanpassen aan nieuwe informatie op hun eigen; elke wijziging vereiste handmatige herprogrammering.36 
Het Kwalificatieprobleem: Ze vielen ten prooi aan fundamentele problemen in de logica, zoals het kwalificatieprobleem, dat de onmogelijkheid benadrukt om alle voorwaarden en uitzonderingen die vereist zijn voor een regel om waar te zijn in de echte wereld expliciet op te sommen.36 
Uiteindelijk was de mislukking van expert systemen een epistemologische. Het was het empirische bewijs dat de kernveronderstelling van GOFAI—dat alle kennis die nodig is voor intelligent gedrag expliciet kon worden gearticuleerd en gecodeerd—onjuist was. De echte wereld was te complex, rommelig en vol uitzonderingen om te worden vastgelegd in een eindige, handmatig gemaakte regels.

De Ineenstorting van een Markt

De technische mislukkingen werden verergerd door een economische. De industrie van expert systemen was gebouwd rond dure, gespecialiseerde computers genaamd LISP-machines, die waren geoptimaliseerd voor het draaien van de LISP-programmeertaal.17 In 1987 stortte de markt voor deze machines bijna van de ene op de andere dag in. De reden was eenvoudig: krachtige en veel goedkopere algemene werkstations van bedrijven zoals Sun Microsystems waren in staat om LISP-omgevingen effectief uit te voeren, waardoor de gespecialiseerde hardware obsoleet werd. Een hele industrie ter waarde van een half miljard dollar werd in een enkel jaar weggevaagd, wat het definitieve begin van de tweede AI-winter markeerde.17 

V. De Connectionistische Renaissance en de Opkomst van Machine Learning (1990s–2000s)

Terwijl het symbolische paradigma wankelde, werd de intellectuele vacuüm dat het achterliet geleidelijk opgevuld door de heropkomst van zijn oude rivaal: connectionisme. De tweede AI-winter was geen einde, maar een overgang. Terwijl de publieke en commerciële belangstelling voor AI afnam, vond er een stillere revolutie plaats in onderzoeksinstellingen. Deze periode zag de ontwikkeling en verfijning van de wiskundige en algoritmische fundamenten die de moderne AI-tijdperk zouden ondersteunen. Het was een "herbouw" fase waarin de tools voor de deep learning-revolutie werden gesmeed, wachtend op de juiste omstandigheden—enorme gegevens en krachtige rekencapaciteit—om te worden ontketend.

De Terugkeer van het Neuraal Netwerk

De sleutel die het potentieel van neurale netwerken ontsloot, was een efficiënte methode om ze te trainen. Vroege enkelvoudige netwerken, bekend als perceptrons, konden worden getraind maar waren fundamenteel beperkt in de soorten problemen die ze konden oplossen. Meerlaagse netwerken waren theoretisch krachtiger, maar er was geen effectieve manier om hun interne "verborgen" lagen te trainen. 
De oplossing was het backpropagation-algoritme. Hoewel de wiskundige wortels teruggaan tot de jaren 1960 en 1970, was het een baanbrekende paper uit 1986 van David Rumelhart, Geoffrey Hinton en Ronald Williams die de techniek populair maakte en de kracht ervan demonstreerde.16 Backpropagation werkt door de fout in de output van een netwerk te berekenen en vervolgens dit foutsignaal achterwaarts door de lagen van het netwerk te propagateren, waarbij de gewichten van de verbindingen in elke laag worden aangepast om de fout te verminderen. Dit bood een wiskundig principiële en computationeel efficiënte manier om diepe, meerlaagse neurale netwerken te trainen, waarmee een kritische obstakel werd overwonnen dat het connectionistische onderzoek jarenlang had stilgelegd.16 

De "Godfathers van Deep Learning"

De popularisering van backpropagation stelde een nieuwe generatie onderzoekers in staat om de mogelijkheden van meerlaagse neurale netwerken te verkennen. Drie figuren, die later gezamenlijk de Turing Award 2018 zouden ontvangen voor hun fundamentele bijdragen, waren centraal in deze renaissance.45 
Geoffrey Hinton, een Brits-Canadese cognitieve psycholoog en computerwetenschapper, was een sleutelfiguur in de backpropagation-paper van 1986 en ging verder met pionierswerk op het gebied van diepe geloofsnetwerken en andere fundamentele concepten van deep learning.45 
Yann LeCun, een Frans-Amerikaanse computerwetenschapper, ontwikkelde Convolutional Neural Networks (CNNs) in de late jaren 1980 en vroege jaren 1990. Geïnspireerd door de structuur van de visuele cortex, gebruiken CNNs gespecialiseerde lagen om functies zoals randen, texturen en vormen te detecteren, waardoor ze uitzonderlijk krachtig zijn voor beeld- en videoprocessing. Zijn werk legde de basis voor vrijwel alle moderne computer vision-systemen.45 
Yoshua Bengio, een Canadese computerwetenschapper, deed cruciale bijdragen aan probabilistische modellen van sequenties, representatieleren en het toepassen van neurale netwerken op natuurlijke taal, en hielp de technieken vast te stellen die later grote taalmodellen zouden aandrijven.45 

Een Symbolische Nederlaag

Terwijl de connectionistische revival stilletjes aan momentum won, behaalde het symbolische paradigma zijn meest beroemde publieke overwinning. In mei 1997 versloeg IBM's schaakspelende supercomputer, Deep Blue, de regerende wereldkampioen schaken Garry Kasparov in een zes-game match met een eindscore van 3½–2½.18 
Het evenement was een wereldwijde media-sensatie en een mijlpaal in de geschiedenis van AI. Deep Blue "dacht" of "leerde" echter niet schaken op een menselijke manier. In plaats daarvan was het een massaal parallel systeem met gespecialiseerde hardware dat in staat was om 200 miljoen schaakposities per seconde te evalueren.18 Zijn overwinning was een triomf van brute-force berekeningen en geavanceerde zoekalgoritmen, wat aantoonde dat voor een goed gedefinieerd, logisch domein zoals schaken, overweldigende computationele kracht menselijke strategische genialiteit kon verslaan. De overwinning sloot symbolisch een hoofdstuk af over een van de klassieke grote uitdagingen van AI, maar de toekomst van het veld was al op weg in een andere, datagestuurde richting.18 

De Statistische Wending

De connectionistische renaissance maakte deel uit van een bredere verschuiving in de AI-gemeenschap weg van handmatig gemaakte regels en naar statistisch leren van gegevens. De jaren 1990 en 2000 zagen de ontwikkeling van een reeks krachtige en wiskundig rigoureuze machine learning-algoritmen die de hoekstenen van het veld werden. Technieken zoals Support Vector Machines (SVMs), die een optimale grens vinden om gegevenspunten in verschillende klassen te scheiden, en boosting-algoritmen, die veel eenvoudige "zwakke" leraren combineren tot een enkele krachtige "sterke" leraar, bleken zeer effectief te zijn voor een breed scala aan classificatie- en regressietaken. Deze "statistische wending" bevestigde het idee dat leren van gegevens een robuustere en schaalbare weg was naar het bouwen van intelligente systemen dan proberen intelligentie vanuit eerste principes te programmeren.

VI. De Deep Learning Revolutie (2010s): De Convergentie van Gegevens, Rekencapaciteit en Algoritmen

De jaren 2010 getuigen van een explosieve reeks doorbraken die AI transformeerden van een niche academisch veld naar een dominante technologische kracht. Deze "deep learning revolutie" was niet het resultaat van een enkele nieuwe uitvinding. Het was eerder een verhaal van convergentie, waarbij drie onafhankelijke en krachtige stromen—enorme datasets, parallelle hardware en verfijnde algoritmen—uiteindelijk samenkwamen, waardoor het potentieel werd ontsloten dat connectionistische modellen decennialang hadden vastgehouden.

De Drie Pilaren van de Revolutie

De moderne AI-boom rust op de samenvloeiing van drie essentiële pijlers. Zonder een van hen zou de revolutie niet hebben plaatsgevonden.

Big Data: De ImageNet Katalysator

De eerste pijler was de beschikbaarheid van enorme, hoogwaardige, gelabelde gegevens. Neurale netwerken zijn "gegevenshongerig," en jarenlang was hun potentieel beperkt door de kleine, schone datasets die beschikbaar waren voor training.49 Dit veranderde dramatisch met de creatie van de ImageNet dataset, een project geleid door computerwetenschapper Fei-Fei Li aan de Stanford en Princeton Universiteiten en voor het eerst gepresenteerd in 2009.19 
ImageNet was een ongekende inspanning om een grootschalige database van afbeeldingen te creëren die was georganiseerd volgens de WordNet-hiërarchie van zelfstandige naamwoorden. Het project verzamelde uiteindelijk meer dan 14 miljoen afbeeldingen, handmatig geannoteerd door crowdsourced werknemers via Amazon Mechanical Turk, en gecategoriseerd in meer dan 20.000 verschillende klassen (bijv. "Siberische husky," "motor scooter," "kers").20 De jaarlijkse ImageNet Large Scale Visual Recognition Challenge (ILSVRC), gelanceerd in 2010, gebruikte een subset van deze gegevens om een gestandaardiseerde benchmark voor computer vision-algoritmen te creëren. Deze competitie bood een duidelijke, objectieve maatstaf voor vooruitgang en werd de smeltkroes waarin de deep learning-revolutie werd gesmeed.50 

Parallelle Rekencapaciteit: De GPU

De tweede pijler was de hardware om deze gegevens te verwerken. Het trainen van diepe neurale netwerken omvat het uitvoeren van miljarden eenvoudige wiskundige bewerkingen—voornamelijk matrixvermenigvuldigingen en vectoroptellingen—herhaaldelijk. Terwijl traditionele Central Processing Units (CPU's) zijn ontworpen voor complexe, sequentiële taken, is dit soort massaal parallelle berekeningen precies waar Graphics Processing Units (GPU's) voor zijn gebouwd.21 Oorspronkelijk ontwikkeld om complexe graphics voor videospellen te renderen, realiseerden onderzoekers in het midden van de jaren 2000 dat GPU's konden worden hergebruikt voor algemene wetenschappelijke berekeningen. Dit bood de immense computationele kracht die nodig was om diepe netwerken op datasets ter grootte van ImageNet in een redelijke tijd en tegen een toegankelijke kostprijs te trainen, een taak die op CPU's alleen prohibitief traag en duur zou zijn geweest.21 

Algoritmische Doorbraken

Met de gegevens en de hardware op hun plaats, was het podium klaar voor algoritmische innovatie om de revolutie te ontbranden. 
2012: AlexNet: Het "big bang" moment voor deep learning vond plaats tijdens de ILSVRC van 2012. Een diep convolutioneel neuraal netwerk genaamd AlexNet, gecreëerd door Alex Krizhevsky, Ilya Sutskever en hun promotor Geoffrey Hinton aan de Universiteit van Toronto, bereikte een top-5 foutpercentage van 15,3%.21 Dit resultaat was verbluffend, aangezien de volgende beste concurrent, die meer traditionele computer vision-technieken gebruikte, slechts 26,2% wist te behalen.21 De architectuur van AlexNet, hoewel gebaseerd op de eerdere CNN's van LeCun, omvatte verschillende belangrijke innovaties: het was veel dieper (8 lagen), gebruikte de computationeel efficiënte ReLU (Rectified Linear Unit) activatiefunctie in plaats van de traditionele sigmoid, en werd parallel getraind op twee NVIDIA GPU's.21 Zijn beslissende overwinning was onmiskenbaar bewijs van de superioriteit van deep learning en overtuigde een sceptische onderzoeksgemeenschap om oudere methoden bijna van de ene op de andere dag te verlaten, wat de moderne AI-boom inluidde.22 
2016: AlphaGo: Als AlexNet de kracht van deep learning in perceptie demonstreerde, toonde AlphaGo de kracht ervan in strategisch redeneren. Ontwikkeld door Google's DeepMind, nam AlphaGo het op tegen Lee Sedol, een van de grootste spelers ter wereld van het oude spel Go, in een vijf-game match in Seoul, Zuid-Korea.23 Go wordt als veel complexer dan schaken beschouwd voor een AI. Het bord is groter, en het aantal mogelijke spelposities is groter dan het aantal atomen in het bekende universum, waardoor een brute-force zoekbenadering zoals die van Deep Blue onmogelijk is.23 Het succes van AlphaGo kwam voort uit een nieuwe combinatie van deep learning en reinforcement learning. Het gebruikte twee diepe neurale netwerken: een "policy network" om de meest veelbelovende volgende zetten te voorspellen, en een "value network" om de winnaar van een gegeven bordpositie te schatten. Het werd aanvankelijk getraind op een database van menselijke expert-spellen en verbeterde vervolgens door miljoenen spellen tegen zichzelf te spelen, waarbij het volledig nieuwe strategieën leerde en ontdekte.23 AlphaGo versloeg Lee Sedol met een score van 4 tegen 1.23 Zijn overwinning was een monumentale prestatie, die een AI toonde die in staat was tot een soort creativiteit en intuïtie die zelfs zijn makers en de Go-meesters die zijn spellen bestudeerden verraste. 
2017: De Transformer: De laatste pijler van het moderne AI-tijdperk was een architectonische innovatie die natuurlijke taalverwerking (NLP) revolutioneerde. Jarenlang was het verwerken van sequentiële gegevens zoals tekst het domein van Recurrent Neural Networks (RNN's) en hun meer geavanceerde variant, LSTMs, die woorden één voor één in volgorde verwerken.25 In 2017 publiceerde een team van Google Brain een paper met een bedrieglijk eenvoudige titel: "Attention Is All You Need".24 Het introduceerde de Transformer-architectuur, die volledig afzag van herhaling. De belangrijkste innovatie was het zelf-aandachtmechanisme, waarmee het model het belang van alle andere woorden in de invoersequentie gelijktijdig kon wegen bij het verwerken van een bepaald woord.24 Omdat het niet sequentieel hoefde te werken, kon de Transformer tot een enorme mate worden parallelized. Deze architectonische doorbraak was de sleutel die de mogelijkheid ontsloot om modellen op web-grote tekstdatasets te trainen, wat de creatie van de enorme Large Language Models (LLMs) mogelijk maakte die het huidige tijdperk definiëren.24 

VII. Het Hedendaagse Tijdperk (2020s): Generatieve AI, Wereldwijde Impact en de Zoektocht naar AGI

De jaren 2020 markeren de rijping van AI van een gespecialiseerd onderzoeksveld naar een transformerende, algemene technologie met diepgaande wereldwijde impact. Dit hedendaagse tijdperk wordt gekenmerkt door de explosieve groei van generatieve modellen, de opkomst van een handvol krachtige "foundation models" die dienen als basis voor talloze toepassingen, en de versnelde integratie van AI in de economische, maatschappelijke en geopolitieke structuur van de wereld. De snelle vooruitgang heeft het debat over Kunstmatige Algemene Intelligentie (AGI) nieuw leven ingeblazen en heeft vragen over veiligheid, ethiek en governance naar de voorgrond van het internationale beleid gebracht. Deze periode wordt gedreven door een krachtige feedbackloop: steeds capabelere modellen creëren commercieel waardevolle producten, die op hun beurt de enorme inkomsten en investeringen genereren die nodig zijn om de volgende, nog krachtigere generatie modellen te bouwen, wat een zelfversterkende cyclus van versnelling creëert.

Het Tijdperk van Grote Taalmodellen (LLMs)

De Transformer-architectuur maakte een paradigma verschuiving mogelijk naar het bouwen van enorme foundation-modellen die zijn getraind op enorme hoeveelheden publieke internetgegevens. Deze modellen zijn niet ontworpen voor een enkele taak, maar verwerven een breed scala aan kennis en mogelijkheden die kunnen worden aangepast, of "fijn afgestemd," voor talrijke downstream-toepassingen. 
De GPT-serie en de Opkomst van Foundation-modellen: OpenAI's Generative Pre-trained Transformer (GPT) serie staat aan de voorhoede van deze trend. Terwijl eerdere versies bestonden, was de release van GPT-3 in 2020, met zijn 175 miljard parameters, een keerpunt. Het vermogen om coherente, contextueel relevante en vaak creatieve tekst te genereren was een dramatische sprong in capaciteit. Dit culmineerde in de release van ChatGPT eind 2022, een gebruiksvriendelijke chatbot-interface gebouwd op het GPT-3.5-model dat de kracht van LLMs naar het publiek bracht en een wereldwijde virale sensatie werd.27 De daaropvolgende release van GPT-4 en, in mei 2024, de natuurlijk multimodale GPT-4o, zette deze koers voort. GPT-4o ("o" voor "omni") vertegenwoordigt een significante architectonische verschuiving, die tekst-, audio-, beeld- en video-invoer en -uitvoer binnen een enkel, end-to-end getraind neuraal netwerk verwerkt. Dit maakt real-time, naadloze conversatie-interactie over modaliteiten mogelijk, met responstijden zo laag als 232 milliseconden, vergelijkbaar met menselijke conversatie.28 
De Redeneringssprong: Een belangrijke focus in 2024 en 2025 is het verbeteren van de complexe redeneringscapaciteiten van LLMs. OpenAI's O3-modelserie, vrijgegeven eind 2024, toonde een significante vooruitgang op dit gebied aan. O3 bereikte bijna 90% nauwkeurigheid op de ARC-AGI benchmark en een score van ongeveer 25% op de zeer uitdagende Frontier Math Benchmark, een dramatische verbetering ten opzichte van de vorige beste score van 2%.30 Deze modellen maken gebruik van technieken zoals een "private chain of thought," waarbij het model intern zijn redenering verfijnt voordat het een antwoord produceert, wat leidt tot grotere nauwkeurigheid en verminderde "hallucinaties".30 
Het Open-Source Ecosysteem: Parallel aan de ontwikkeling van deze grote, propriëtaire "frontier" modellen is er een levendig open-source ecosysteem ontstaan, dat de toegang tot krachtige AI democratiseert. Meta is een belangrijke speler geweest, die zijn Llama-serie modellen heeft vrijgegeven, waarbij Llama 3.1 en het multimodale Llama 4 (vrijgegeven in 2025) prestaties bieden die concurreren met propriëtaire alternatieven.53 Andere significante bijdragers zijn onder andere het Franse Mistral AI, wiens modellen bekend staan om hun efficiëntie en prestaties, en Alibaba met zijn Qwen-serie.53 Deze open-source beweging heeft snelle innovatie gestimuleerd en kleinere bedrijven en onderzoekers in staat gesteld om voort te bouwen op state-of-the-art fundamenten.

Voorbij Tekst: De Multimodale en Generatieve Explosie

Terwijl LLMs de wereld van tekst transformeerden, vond er een parallelle revolutie plaats in de generatie van andere media, aangedreven door nieuwe architecturen en de bredere trend naar multimodaliteit. 
Diffusie Modellen: Beginnend in 2022 leidde een nieuwe klasse van generatieve modellen, bekend als diffusie modellen, tot een doorbraak in beeldgeneratie. Deze modellen werken door te beginnen met willekeurige ruis en deze stap voor stap te verfijnen tot een coherente afbeelding die overeenkomt met een tekstprompt. Systemen zoals OpenAI's DALL-E 2, het onafhankelijke onderzoeksinstituut Midjourney, en de open-source Stable Diffusion (eerder vrijgegeven door Stability AI in augustus 2022) produceerden afbeeldingen van verbluffende kwaliteit, realisme en artistieke stijl, wat creatieve industrieën transformeerde.26 
Natuurlijk Multimodale Systemen: De grens van AI in 2024-2025 is de verschuiving naar enkele, verenigde modellen die van nature verschillende datatypes kunnen verwerken en redeneren. In tegenstelling tot eerdere systemen die aparte modellen voor visie, audio en tekst aan elkaar zouden bevestigen, worden nieuwe architecturen zoals GPT-4o end-to-end getraind op een mix van modaliteiten.29 Dit maakt veel rijkere en vloeiendere interacties mogelijk, zoals het voeren van een gesproken gesprek met een AI over een live videofeed van de camera van een telefoon. Deze verschuiving van gespecialiseerde modellen naar verenigde, multimodale intelligentie is een belangrijke stap naar meer algemene en capabele AI-systemen.56 
Tabel 3: Belangrijke Generatieve AI Modellen en Architecturen (2022-2025)

Model/Reeks
Ontwikkelaar
Jaar van Vrijgave
Type
Belangrijke Capaciteiten/Innovaies
GPT-3.5 / ChatGPT
OpenAI
2022
LLM
Maakte hoogwaardige conversatie-AI breed toegankelijk, wat een wereldwijde boom in de adoptie van generatieve AI veroorzaakte.27
Diffusie Modellen
OpenAI, Stability AI, Midjourney
2022
Tekst-naar-Beeld
DALL-E 2, Stable Diffusion en Midjourney maakten hoogwaardige, controleerbare beeldgeneratie mogelijk vanuit tekstprompts.26
GPT-4
OpenAI
2023
Multimodale LLM
Groot verbeterde redenering en prestaties; accepteerde zowel tekst- als beeldinvoer.27
Claude 3
Anthropic
2024
Multimodale LLM
Een familie van modellen (Haiku, Sonnet, Opus) met sterke prestaties, vooral in het omgaan met lange contexten en bedrijfsgebruik. 
Llama 3 / 3.1
Meta
2024
Open-Source LLM
Hoogpresterende open-source modellen die rivaliseerden met propriëtaire systemen, wat wijdverspreide innovatie stimuleerde.53
GPT-4o
OpenAI
2024
Natuurlijk Multimodaal
Een enkel end-to-end model voor tekst, audio en visie, waardoor real-time, mensachtige conversatie-interactie mogelijk wordt.28
O3 Serie
OpenAI
2024-2025
LLM (Redenering)
Een nieuwe serie modellen die een significante sprong in prestaties op complexe redenering, wiskunde en wetenschap benchmarks demonstreert.30
Llama 4
Meta
2025
Open-Source Multimodaal
Introduceerde natuurlijke multimodaliteit en een Mixture-of-Experts (MoE) architectuur aan de open-source Llama-familie.54 

Het AGI Debat (2024-2025)

Het adembenemende tempo van verbetering in LLMs heeft het ooit marginale onderwerp van Kunstmatige Algemene Intelligentie (AGI)—een hypothetische AI met het vermogen om elke intellectuele taak te begrijpen of te leren die een mens kan—in de mainstream discussie gebracht. 
Tijdslijnen Verkorten: Terwijl enkele jaren geleden de mediane voorspelling van AI-onderzoekers voor de komst van AGI rond 2060 lag, heeft de recente vooruitgang deze tijdlijnen dramatisch verkort. Huidige deskundigenenquêtes plaatsen de mediane voorspelling nu dichter bij 2040, terwijl veel leiders in de industrie en ondernemers veel optimistischer zijn, met sommigen die AGI-niveau systemen voorspellen tegen 2026-2030.59 
De Schaalhypothese: De kern van het debat is of AGI kan worden bereikt door simpelweg de huidige architecturen op te schalen—dat wil zeggen, door steeds grotere Transformer-modellen op meer gegevens met meer rekencapaciteit te trainen—of dat fundamentele nieuwe wetenschappelijke doorbraken vereist zijn. Voorstanders van de "schaalhypothese" wijzen op de voorspelbare prestatieverbeteringen die te zien zijn bij verhoogde investeringen en de snelle verzadiging van moeilijke benchmarks als bewijs dat we op een directe weg naar AGI zijn.60 
De Rol van LLMs: Onderzoekers zijn verdeeld over de vraag of huidige LLMs "vonken" van AGI vertegenwoordigen. Sommigen beweren dat hun prestaties op een breed scala aan taken die voorheen als vereisend voor algemene intelligentie werden beschouwd, bewijs zijn van opkomende generaliteit.62 Anderen zijn van mening dat deze modellen nog steeds fundamenteel beperkt zijn, geen echt begrip, belichaming of het vermogen voor lange termijn autonome planning hebben, en dat nieuwe architecturen nodig zullen zijn om deze kloof te overbruggen.59 

AI als een Geopolitieke en Maatschappelijke Kracht

Naarmate de capaciteiten van AI zijn gegroeid, is ook de impact ervan toegenomen, waardoor het een kwestie van nationale strategie en internationale governance is geworden. 
Economische en Maatschappelijke Impact: AI wordt nu algemeen erkend als een algemene technologie met het potentieel om de wereldwijde productiviteit en economische groei aanzienlijk te verhogen.63 Studies hebben grote productiviteitswinsten aangetoond voor werknemers in verschillende beroepen die AI-tools gebruiken. Er zijn echter aanzienlijke zorgen dat AI ook ongelijkheid zou kunnen verergeren door sommige banen te vervangen en hoogopgeleide werknemers meer te complementeren dan anderen.64 Bovendien heeft de explosieve groei van AI een significante en vaak over het hoofd geziene ecologische voetafdruk. De zoektocht naar digitale intelligentie is een diepgaand fysiek proces, ondersteund door een wereldwijd netwerk van energie-hongerende datacenters die enorme hoeveelheden elektriciteit en water verbruiken. Terwijl de hoge energiekosten van het trainen van modellen goed bekend zijn, tonen recente gegevens aan dat de inferentiefase—het dagelijks gebruik van modellen—de dominante factor is, goed voor 60-70% van het totale energieverbruik. Dit energieverbruik heeft een zeer variabele koolstofvoetafdruk, afhankelijk van het lokale elektriciteitsnet, en een volledige afrekening moet ook de "verweven koolstof" van de productie van de gespecialiseerde hardware omvatten. Voorbij energie heeft AI een significante watervoetafdruk, waarbij datacenters miljarden liters verbruiken voor koeling, wat een grote zorg is in de droogtegevoelige gebieden waar ze vaak zijn gevestigd. (Voor een gedetailleerde analyse, zie Bijlage D). 
Het Wereldwijde Regelgevende Landschap: In erkenning van de transformerende kracht van AI zijn grote wereldmachten begonnen met het implementeren van verschillende regelgevende kaders. 
De Europese Unie: De EU heeft een rechten-georiënteerde, uitgebreide benadering aangenomen met haar AI-wet, die in 2024 in werking trad. Het maakt gebruik van een risicogebaseerd kader, waarbij toepassingen die als "onaanvaardbaar risico" worden beschouwd (zoals sociale scoring) worden verboden en strikte transparantie-, gegevensbeheer- en toezichtseisen worden opgelegd aan "hoog-risico" systemen die worden gebruikt in gebieden zoals werkgelegenheid, wetshandhaving en kritieke infrastructuur.66 
De Verenigde Staten: De Amerikaanse benadering, gearticuleerd in het "AI Actieplan" van 2025, prioriteert innovatie, commercialisering en het behouden van een wereldwijde concurrentievoordeel. Het plan is gericht op het versnellen van de ontwikkeling van AI door milieuregels voor de bouw van datacenters te verminderen, de export van Amerikaanse AI-technologie te bevorderen en de ontwikkeling van open-source modellen aan te moedigen, terwijl ook wordt gestreefd naar een uniforme federale standaard voor regulering om een lappendeken van staatswetten te vermijden.68 
China: China's strategie is staatsgeleide, gericht op wereldwijde AI-leiderschap tegen 2030. De regelgevende aanpak in 2024-2025 richt zich op het balanceren van snelle ontwikkeling met staatscontrole. Regels benadrukken nationale veiligheid, ideologische afstemming en sociale stabiliteit, met vereisten voor algoritmeregistratie, veiligheidsbeoordelingen en, vanaf september 2025, verplichte labeling van alle AI-gegenereerde inhoud.71 
AI Veiligheid en Afstemming: Naarmate modellen krachtiger en autonomer zijn geworden, is AI-veiligheid een kritisch onderzoeksgebied geworden. Het gaat verder dan traditionele zorgen zoals bias en eerlijkheid om de nieuwe risico's aan te pakken die door zeer capabele toekomstige AI-systemen worden gepresenteerd. Het centrale doel van AI-afstemming is ervoor te zorgen dat de doelstellingen van een AI-systeem robuust zijn afgestemd op menselijke waarden en intenties.75 Onderzoek richt zich op uitdagingen zoals het voorkomen van opkomende, onbedoelde doelen (zoals machtszoekend gedrag), ervoor zorgen dat AI-systemen eerlijk en interpreteerbaar zijn, en het ontwikkelen van methoden voor schaalbaar toezicht om systemen veilig te beheren die op een dag mogelijk de menselijke capaciteiten zullen overtreffen.75 

Conclusie: Een Nieuw Traject

De geschiedenis van Kunstmatige Intelligentie is een verhaal van oscilleren paradigma's, van ambitieuze dromen getemperd door de harde realiteiten van computationele limieten. Decennialang werd het veld gedefinieerd door een fundamenteel debat tussen logica-gebaseerde symbolische systemen en hersen-geïnspireerde connectionistische netwerken. Het symbolische tijdperk produceerde fundamentele concepten in redeneren en kennisrepresentatie, maar faalde uiteindelijk op de "brittleness" van zijn handmatig gemaakte regels, wat leidde tot periodes van stagnatie die bekend staan als de AI-winter. 
De huidige deep learning-revolutie vertegenwoordigt de beslissende triomf van de connectionistische benadering, een overwinning die alleen mogelijk werd door de recente, ongekende convergentie van drie kritische krachten: web-grote gegevens, massaal parallelle berekeningen via GPU's, en transformerende neurale architecturen zoals de Transformer. Deze convergentie heeft niet alleen de vooruitgang versneld; het heeft het traject van het veld veranderd. De focus is verschoven van het creëren van gespecialiseerde AI voor smalle taken naar het bouwen van algemene foundation-modellen met een breed scala aan opkomende mogelijkheden. 
We bevinden ons nu in het generatieve en multimodale tijdperk, waarin AI nieuwe inhoud kan creëren over tekst, afbeeldingen en audio, en begint de wereld te begrijpen via meerdere zintuigen tegelijkertijd. Deze snelle vooruitgang heeft AI van het laboratorium naar de kern van de wereldeconomie en het hart van geopolitieke concurrentie gedreven, waardoor samenlevingen gedwongen worden om zich te verhouden tot een nieuwe en krachtige klasse technologie. De centrale vragen voor AI gaan niet langer alleen over logica of leeralgoritmen. Ze gaan over governance, veiligheid en de definitie van intelligentie zelf, terwijl de lang-hypothetische zoektocht naar AGI een tastbaar en urgent onderwerp van onderzoek en debat wordt. Het volgende hoofdstuk in de geschiedenis van AI zal niet alleen in onderzoeksdocumenten worden geschreven, maar ook in de bestuurskamers van bedrijven, internationale beleidsforums en het dagelijks leven van mensen over de hele wereld. 
Werken geciteerd
Looking back, looking ahead: Symbolic versus connectionist AI, geraadpleegd op 25 juli 2025, <https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/download/15111/18883>
Symbolic AI vs. Connectionist AI: Know the Difference - SmythOS, geraadpleegd op 25 juli 2025, <https://smythos.com/developers/agent-development/symbolic-ai-vs-connectionist-ai/>
AI for Beginners - The Difference Between Symbolic & Connectionist AI - RE•WORK Blog, geraadpleegd op 25 juli 2025, <https://blog.re-work.co/the-difference-between-symbolic-ai-and-connectionist-ai/>
Hybrid AI: Merging Symbolic and Connectionist AI - Number Analytics, geraadpleegd op 25 juli 2025, <https://www.numberanalytics.com/blog/hybrid-ai-merging-symbolic-connectionist-ai>
AI Winter: The Highs and Lows of Artificial Intelligence - History of ..., geraadpleegd op 25 juli 2025, <https://www.historyofdatascience.com/ai-winter-the-highs-and-lows-of-artificial-intelligence/>
A Logical Calculus of the Ideas Immanent in Nervous Activity ..., geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/A_Logical_Calculus_of_the_Ideas_Immanent_in_Nervous_Activity>
Computing Machinery and Intelligence - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence>
Dartmouth workshop - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Dartmouth_workshop>
Logic Theorist - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Logic_Theorist>
Lisp (programming language) - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Lisp_(programming_language)>
ELIZA - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/ELIZA>
Shakey the robot - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Shakey_the_robot>
Lighthill report - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Lighthill_report>
Mycin - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Mycin>
The Chinese Room Argument (Stanford Encyclopedia of Philosophy), geraadpleegd op 25 juli 2025, <https://plato.stanford.edu/entries/chinese-room/>
Backpropagation – Algorithm Hall of Fame, geraadpleegd op 25 juli 2025, <https://www.algorithmhalloffame.org/algorithms/neural-networks/backpropagation/>
AI winter - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/AI_winter>
Deep Blue versus Garry Kasparov - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov>
ImageNet: A large-scale hierarchical image database, geraadpleegd op 25 juli 2025, <https://www.computer.org/csdl/proceedings-article/cvpr/2009/05206848/12OmNxWcH55>
ImageNet Definition | DeepAI, geraadpleegd op 25 juli 2025, <https://deepai.org/machine-learning-glossary-and-terms/imagenet>
AlexNet: ImageNet Classification with Deep Convolutional Neural ..., geraadpleegd op 25 juli 2025, <https://dataturbo.medium.com/alexnet-imagenet-classification-with-deep-convolutional-neural-networks-4cbafdf76ae1>
AlexNet and ImageNet: The Birth of Deep Learning - Pinecone, geraadpleegd op 25 juli 2025, <https://www.pinecone.io/learn/series/image-search/imagenet/>
AlphaGo versus Lee Sedol - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol>
Attention Is All You Need - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Attention_Is_All_You_Need>
The Transformer Revolution: How “Attention Is All You Need” Changed AI Forever - Medium, geraadpleegd op 25 juli 2025, <https://medium.com/@sebuzdugan/the-transformer-revolution-how-attention-is-all-you-need-changed-ai-forever-c43b620b5671>
Stable Diffusion - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Stable_Diffusion>
AI Timeline, geraadpleegd op 25 juli 2025, <https://nhlocal.github.io/AiTimeline/>
ChatGPT-4.0: Key Features, Benefits & Uses Explained | The Flock, geraadpleegd op 25 juli 2025, <https://www.theflock.com/content/blog-and-ebook/chatgpt-4o-features-benefits-and-uses>
Hello GPT-4o - OpenAI, geraadpleegd op 25 juli 2025, <https://openai.com/index/hello-gpt-4o/>
An In-Depth Analysis of OpenAI's O3 Model and Its Comparative ..., geraadpleegd op 25 juli 2025, <https://medium.com/@thomas_78526/an-in-depth-analysis-of-openais-o3-model-and-its-comparative-performance-813a7c57a83e>
Professor John McCarthy | Stanford Computer Science, geraadpleegd op 25 juli 2025, <https://legacy.cs.stanford.edu/memoriam/professor-john-mccarthy>
David Kalat | Nervous System: The ELIZA Effect | Insights | BRG - Berkeley Research Group, geraadpleegd op 25 juli 2025, <https://www.thinkbrg.com/insights/publications/nervous-system-eliza-effect/>
Connectionist approaches to cognition | Intro to Cognitive Science Class Notes - Fiveable, geraadpleegd op 25 juli 2025, <https://library.fiveable.me/introduction-cognitive-science/unit-7/connectionist-approaches-cognition/study-guide/KhLCI3SrjO7UABrX>
Symbolism vs. Connectionism: A Closing Gap in Artificial Intelligence | Jieshu's Blog, geraadpleegd op 25 juli 2025, <http://wangjieshu.com/2017/12/23/symbol-vs-connectionism-a-closing-gap-in-artificial-intelligence/>
Connectionist vs Symbolic Models - (Intro to Cognitive Science) - Fiveable, geraadpleegd op 25 juli 2025, <https://library.fiveable.me/key-terms/introduction-cognitive-science/connectionist-vs-symbolic-models>
history - Why did expert systems fall? - Retrocomputing Stack ..., geraadpleegd op 25 juli 2025, <https://retrocomputing.stackexchange.com/questions/6456/why-did-expert-systems-fall>
Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents - arXiv, geraadpleegd op 25 juli 2025, <https://arxiv.org/html/2407.08516v1>
Complete History of AI | LeanIX, geraadpleegd op 25 juli 2025, <https://www.leanix.net/en/wiki/ai-governance/history-of-ai>
A Brief History of AI — Making Things Think - Holloway, geraadpleegd op 25 juli 2025, <https://www.holloway.com/g/making-things-think/sections/a-brief-history-of-ai>
Two winters and a spring of artificial intelligence - QED Software, geraadpleegd op 25 juli 2025, <https://qedsoftware.com/blog/two-winters-and-a-spring-of-artificial-intelligence/>
Computers, Artificial Intelligence, and Expert Systems in Biomedical Research | Joshua Lederberg - Profiles in Science, geraadpleegd op 25 juli 2025, <https://profiles.nlm.nih.gov/spotlight/bb/feature/ai>
Dendral - Wikipedia, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Dendral>
What is the brittleness problem in AI reasoning? - Zilliz Vector ..., geraadpleegd op 25 juli 2025, <https://zilliz.com/ai-faq/what-is-the-brittleness-problem-in-ai-reasoning>
What is an expert system? - Autoblocks AI, geraadpleegd op 25 juli 2025, <https://www.autoblocks.ai/glossary/expert-system>
Here are 3 godfathers of AI reshaping the world of artifical ..., geraadpleegd op 25 juli 2025, <https://yourstory.com/2025/06/3-godfathers-ai-hinton-bengio-lecun>
en.wikipedia.org, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Geoffrey_Hinton>
en.wikipedia.org, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Yann_LeCun>
en.wikipedia.org, geraadpleegd op 25 juli 2025, <https://en.wikipedia.org/wiki/Yoshua_Bengio>
History of AI: Unraveling the Epic Saga of Minds and Machines - OpenCV, geraadpleegd op 25 juli 2025, <https://opencv.org/blog/history-of-ai/>
ImageNet Explained: The Backbone of Deep Learning in Vision, geraadpleegd op 25 juli 2025, <https://eureka.patsnap.com/article/imagenet-explained-the-backbone-of-deep-learning-in-vision>
Exploring Deep Learning Models: ImageNet dataset with VGGNet, ResNet, Inception, and Xception using Keras for Image Classification | by Sanjay Dutta, PhD | Medium, geraadpleegd op 25 juli 2025, <https://medium.com/@sanjay_dutta/exploring-deep-learning-models-imagenet-dataset-with-vggnet-resnet-inception-and-xception-using-5f0f30b83ef9>
GPT-4o: What's cool, what's hype, and what happens next - Section, geraadpleegd op 25 juli 2025, <https://www.sectionai.com/blog/what-is-gpt4o>
Best Open Source LLMs of 2025 — Klu, geraadpleegd op 25 juli 2025, <https://klu.ai/blog/open-source-llm-models>
How to Choose the Best Open Source LLM (2025 Guide) - Imaginary Cloud, geraadpleegd op 25 juli 2025, <https://www.imaginarycloud.com/blog/best-open-source-llm>
Top 8 Open‑Source LLMs to Watch in 2025 - JetRuby Agency, geraadpleegd op 25 juli 2025, <https://jetruby.com/blog/top-8-open-source-llms-to-watch-in-2025/>
Multimodal Models and Agentic AI: Generative AI in 2025 - Spitch.ai, geraadpleegd op 25 juli 2025, <https://spitch.ai/news/multimodal-models-and-agentic-ai-generative-ai-in-2025/>
What is multimodal AI: Complete overview 2025 | SuperAnnotate, geraadpleegd op 25 juli 2025, <https://www.superannotate.com/blog/multimodal-ai>
Multimodal AI in 2025: The Business Intelligence Revolution That ..., geraadpleegd op 25 juli 2025, <https://jenstirrup.com/2025/07/11/multimodal-ai-in-2025-the

---
*Vertaald door AI. Controleer de originele Engelse versie bij twijfel.*