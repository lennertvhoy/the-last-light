# Bijlage D: De Milieu-impact van Kunstmatige Intelligentie

Inleiding: De Ongeziene Kosten van een Digitale Revolutie

De snelle opkomst van kunstmatige intelligentie vertegenwoordigt een cruciaal moment in de technologische geschiedenis, met de belofte om industrieën te herdefiniëren, wetenschappelijke ontdekkingen te versnellen en menselijke interactie te hervormen. Toch is deze digitale revolutie geen etherisch fenomeen dat zich in een abstracte "cloud" afspeelt. Het is een diep fysiek proces, ondersteund door een wereldwijd netwerk van enorme, energieverslindende datacenters en een ingewikkelde toeleveringsketen die enorme hoeveelheden elektriciteit, water en grondstoffen verbruikt.1 De immense rekenkracht die nodig is om geavanceerde AI-modellen te trainen en in te zetten, leidt tot een aanzienlijke en vaak over het hoofd geziene milieu-impact, wat een centraal paradox voor het moderne tijdperk creëert: de zoektocht naar digitale intelligentie is onlosmakelijk verbonden met een groeiende, tastbare vraag naar de eindige hulpbronnen van de aarde.
De omvang van deze uitdaging is verbluffend en blijft exponentieel groeien. In 2022 waren datacenters en de datatransmissienetwerken die hen verbinden al verantwoordelijk voor een geschatte 2-3% van het wereldwijde elektriciteitsverbruik en ongeveer 1% van de energiegerelateerde broeikasgasemissies (GHG).2 De proliferatie van generatieve AI is de belangrijkste katalysator voor toekomstige groei, met sommige prognoses die aangeven dat de elektriciteitsvraag van datacenters in de Verenigde Staten alleen al zou kunnen stijgen van 4% van het nationale totaal in 2023 tot maar liefst 12% tegen 2030.3 Deze explosieve vraag legt al druk op de elektriciteitsnetten, draagt bij aan de vertraagde afschakeling van fossiele brandstofcentrales en verergert de concurrentie om water in droge gebieden.1
Om de milieu-implicaties van AI volledig te begrijpen, is een uitgebreide, veelzijdige analyse vereist. Deze bijlage biedt een gedetailleerd onderzoek naar de milieu-impact van AI, beginnend bij de fundamentele maatstaven van berekeningen tot de werkelijke gevolgen voor energie, koolstofemissies, watervoorraden en hardwarelevenscycli. Het begint met het ontleden van het concept van rekenvraag, waarbij de maatstaven worden uitgelegd die worden gebruikt om de immense schaal van moderne AI-modellen te kwantificeren. Vervolgens verkent het de fysieke infrastructuur—de datacenters—die deze berekeningen huisvest, en analyseert hun energieverbruik en de maatstaven die worden gebruikt om hun efficiëntie te meten. Van daaruit breidt de analyse zich uit om de volledige koolstofvoetafdruk te omvatten, inclusief het cruciale onderscheid tussen training en inferentie, de beslissende rol van het lokale elektriciteitsnet en de verborgen "verweven" koolstof binnen de hardware zelf. Het onderzoek onthult ook de vaak onzichtbare maar kritieke dorst van AI naar water, een hulpbron die essentieel is voor het koelen van de motoren van het digitale tijdperk. Ten slotte schetst deze bijlage een pad vooruit, met een reeks mitigatiestrategieën en het opkomende paradigma van "Groene AI" dat erop gericht is de drang naar innovatie te verzoenen met de noodzaak van milieuduurzaamheid.

1. De Motor van AI: Kwantificeren van de Rekenbehoefte

In het hart van kunstmatige intelligentie ligt berekening, of "compute"—een abstracte term voor de ruwe verwerkingskracht die nodig is om de miljarden wiskundige bewerkingen uit te voeren die een model in staat stellen om van gegevens te leren en reacties te genereren. Om de milieu-impact van AI te begrijpen, moet men eerst begrijpen hoe deze rekenbehoefte wordt gemeten en de astronomische schaal die het heeft bereikt in moderne systemen. Deze sectie ontrafelt de taal van berekeningen en biedt een fundamenteel kader voor de daaropvolgende analyse.

1.1. De Taal van Berekeningen: Begrijpen van FLOPS

De fundamentele eenheid voor het kwantificeren van de rekenbelasting is de floating-point operatie, of FLOP.7 Een floating-point operatie is een enkele wiskundige berekening die betrekking heeft op reële getallen met decimalen, zoals optelling, aftrekking, vermenigvuldiging of deling.8 De snelheid waarmee een processor deze berekeningen kan uitvoeren, wordt gemeten in FLOPS, of floating-point operaties per seconde. Deze maatstaf dient als een primaire benchmark voor het vergelijken van de ruwe verwerkingskracht van verschillende computersystemen, van persoonlijke laptops tot de krachtigste supercomputers ter wereld.8
In de context van AI wordt de term "FLOPs" vaak op twee verschillende manieren gebruikt, wat verwarring kan veroorzaken. Terwijl het kan verwijzen naar de snelheid van operaties per seconde, kwantificeert het vaker het totale aantal floating-point operaties dat nodig is om een specifieke taak uit te voeren, zoals een enkele "forward pass" van een neuraal netwerk om één voorspelling te genereren.11 Dit gebruik biedt een hardware-onafhankelijke maat voor de rekencomplexiteit van een model. Voor de duidelijkheid zal deze tekst "FLOPs" gebruiken om te verwijzen naar de snelheid (operaties per seconde) en expliciet "totaal FLOPs" of "FLOP" vermelden wanneer naar het aantal operaties voor een bepaalde taak wordt verwezen, een conventie die in sommige technische literatuur is aangenomen om ambiguïteit te vermijden.12 De betrokken getallen zijn vaak zo immens dat ze worden uitgedrukt met standaardvoorvoegsels: GigaFLOPs (GFLOPs, of miljarden operaties), TeraFLOPs (TFLOPs, of triljoenen), en PetaFLOPs (PFLOPs, of quadriljoenen).7
Echter, niet alle FLOPs zijn gelijk. Een genuanceerd begrip vereist het onderscheiden van verschillende types:
Precisie: De precisie van een floating-point getal verwijst naar het aantal bits dat wordt gebruikt om het op te slaan. Dubbele precisie (FP64) gebruikt 64 bits en is de standaard voor hoogprecisie wetenschappelijk rekenen, zoals in de TOP500 supercomputer rankings.10
Enkele precisie (FP32) gebruikt 32 bits. Om de snelheid te verbeteren en het energieverbruik te verminderen, gebruiken AI-werkbelastingen vaak gemengde precisieformaten, die 32-bits operaties combineren met lagere precisieformaten zoals 16-bits (FP16 of bfloat16) of zelfs 8-bits gehele getallen (INT8) voor bepaalde berekeningen zonder een significante verlies in modelnauwkeurigheid.10
Theoretisch vs. Gemeten: Het is cruciaal om te onderscheiden tussen Peak FLOPS en Sustained FLOPS.8 Peak FLOPS vertegenwoordigt het theoretische maximum aantal operaties dat een processor per seconde kan uitvoeren onder ideale omstandigheden, zoals gespecificeerd door de fabrikant. Sustained FLOPS, daarentegen, is de werkelijke prestatie die wordt bereikt tijdens een toepassing in de echte wereld. Deze gemeten waarde is bijna altijd lager dan de theoretische piek vanwege praktische beperkingen zoals geheugensnelheden, gegevensafhankelijkheden en algoritmische inefficiënties.7
De significante kloof tussen het theoretische potentieel van een hardware systeem en de werkelijke prestatie in de echte wereld is een belangrijke, vaak over het hoofd geziene bron van inefficiëntie. Deze kloof wordt gekwantificeerd door een maat die bekend staat als Model FLOPs Utilization (MFU), die wordt berekend als de verhouding van de behaalde (volgehouden) FLOPs tot de theoretische piek FLOPS van de hardware 7:
MFU=Peak FLOPs/secAchieved FLOPs/sec​
In de praktijk ligt MFU voor grootschalige AI-training vaak in de range van slechts 30% tot 60%.7 Dit betekent dat tot 70% van de potentiële rekenkracht van een processor—en bij uitbreiding, zijn potentiële energie-efficiëntie—kan verloren gaan door systeemgebonden knelpunten voordat andere inefficiënties, zoals datacenterkoeling, zelfs maar in overweging worden genomen. Factoren zoals geheugentoegangssnelheden, waarbij de processor wacht op gegevens om aan te komen, en de specifieke architectuur van het AI-model kunnen voorkomen dat het systeem zijn volledige rekenkundige verzadiging bereikt.14 Deze inherente inefficiëntie, ingebakken in de interactie tussen software en hardware, is een fundamenteel element van het totale energieverbruik van AI.

1.2. De Schaalimperatief: Rekenvereisten van Moderne Modellen

De bepalende trend in AI in het afgelopen decennium is er een van exponentiële schaalvergroting. Onderzoekers ontdekten dat, voor een klasse van modellen die bekend staat als Transformers, de prestaties voorspelbaar verbeteren met toenames in modelgrootte (het aantal parameters) en de hoeveelheid gegevens die voor training wordt gebruikt. Dit heeft geleid tot een wapenwedloop om steeds grotere modellen te bouwen, die astronomische hoeveelheden berekeningen vereisen.
De rekenkosten voor het trainen van een Transformer-gebaseerd model kunnen worden geschat met een algemeen aanvaarde formule 15:
Ctrain​≈6ND
Hier is Ctrain​ de totale berekening voor training in FLOP, N is het aantal parameters in het model, en D is het aantal tokens (stukjes woorden) in de trainingsdataset. De coëfficiënt 6 komt voort uit het feit dat voor elke parameter in het model een forward pass (om een voorspelling te doen) en een backward pass (om van de fout te leren) samen ongeveer 6 floating-point operaties per token vereisen.15
In tegenstelling tot de kosten van inferentie—het gebruik van het getrainde model om een enkele nieuwe token te genereren—zijn aanzienlijk lager, geschat op ongeveer 1 tot 2 FLOP per parameter.16 Deze eenvoudige vergelijking verduidelijkt onmiddellijk waarom een enkele query aan een AI-model rekenkundig veel goedkoper is dan het initiële trainingsproces. Echter, zoals later zal worden verkend, kunnen de cumulatieve kosten van miljarden inferentievragen snel oplopen tot rivaliserende of zelfs het eenmalige trainingskosten overschrijden.
Deze schaalimperatief heeft de rekenvereisten voor state-of-the-art modellen naar verbluffende niveaus gedreven. In het midden van 2025 wordt geschat dat meer dan 30 publiek aangekondigde AI-modellen zijn getraind met meer dan 10^25 totale FLOP—een drempel die voor het eerst werd overschreden door OpenAI's GPT-4 in 2023. Het trainen van een model op deze schaal kost tientallen miljoenen dollars met de huidige hardware.18
De onderstaande tabel, gebaseerd op gegevens van Epoch AI, biedt geschatte trainingsberekeningen voor verschillende prominente grote taalmodellen, waarmee de immense schaal van deze ondernemingen wordt geïllustreerd. Het "Vertrouwen" niveau weerspiegelt de mate van publieke onthulling door de ontwikkelaars, wat een groeiende trend van ondoorzichtigheid in de industrie benadrukt—een thema dat later in deze bijlage opnieuw zal worden behandeld.
Tabel 1: Geschatte Trainingsberekeningen (FLOPs) voor Belangrijke LLM's (2023-2025)

Model
Geschatte Trainingsberekening (FLOP)
Vertrouwen
Rechtvaardiging voor Schatting
Bron
GPT-4
2.1×10^25
Laag-precisie
Geschat met behulp van trainingshardware en duur.
18
Llama 3.1-405B
3.8×10^25
Hoog-precisie
Geschat met behulp van parameter telling en datasetgrootte.
18
Gemini 1.0 Ultra
5.0×10^25
Laag-precisie
Aggregatie van benchmark imputatie en hardware details.
18
Claude 3.5 Sonnet
3.6×10^25
Laag-precisie
Imputatie van benchmark scores en kostenramingen van ontwikkelaars.
18
Claude 3.7 Sonnet
3.4×10^25
Laag-precisie
Gebaseerd op kostenramingen van ontwikkelaars ("een paar tientallen miljoenen").
18
Inflection-2
1.0×10^25
Hoog-precisie
Direct gerapporteerd door de ontwikkelaar.
18
Llama 4 Behemoth
5.2×10^25
Hoog-precisie
Voorlopige schatting op basis van parameters en dataset.
18
GPT-4o
3.8×10^25
Laag-precisie
Imputatie van benchmark scores.
18
Grok-2
3.0×10^25
Hoog-precisie
Gebaseerd op trainingstijd, hardware en verklaringen van ontwikkelaars.
18
Claude Opus 4
1.5×10^26
Speculatief
Afgeleid van schaalvergroting voorbij eerdere modellen.
18

Bron: Aangepast van Epoch AI, juni 2025.18 Vertrouwensniveaus (Hoog, Laag, Speculatief) worden door Epoch AI toegewezen op basis van de kwaliteit en directheid van beschikbare informatie.

2. De Fysieke Infrastructuur: Energieverbruik in Datacenters

De abstracte wereld van berekeningen, gemeten in quadriljoenen FLOPs, wordt werkelijkheid binnen datacenters—uitgestrekte industriële faciliteiten die elektrische energie omzetten in digitale informatie. Het begrijpen van de energiedynamiek van deze gebouwen is de tweede kritische stap in het beoordelen van de milieu-impact van AI. Deze sectie overbrugt de kloof tussen rekenbehoefte en werkelijk elektriciteitsverbruik, en onderzoekt de stijgende energiebehoeften van de AI-industrie en de primaire maatstaf die wordt gebruikt om de efficiëntie van de infrastructuur die het ondersteunt te evalueren.

2.1. Van Berekeningen naar Kilowatt-Uren: De Stijgende Vraag naar Energie

De onophoudelijke groei in AI-werkbelastingen vertaalt zich direct in een enorme en versnellende vraag naar elektriciteit. Wereldwijd verbruikten datacenters naar schatting 240-340 Terawatt-uur (TWh) in 2022, ongeveer 1-1.3% van de wereldwijde finale elektriciteitsvraag. De datatransmissienetwerken die hen verbinden verbruikten een vergelijkbare hoeveelheid, tussen 260-360 TWh.4 Terwijl efficiëntieverbeteringen historisch gezien de groei in energieverbruik hebben getemperd, overweldigt de recente explosie in generatieve AI deze verbeteringen.
AI is nu de belangrijkste drijfveer van nieuwe energiebehoefte in de datacentersector. Het gecombineerde elektriciteitsverbruik van grote technologiebedrijven zoals Amazon, Microsoft, Google en Meta—de sleutelspelers in AI-ontwikkeling—is tussen 2017 en 2021 meer dan verdubbeld, en bereikte ongeveer 72 TWh.4 Prognoses voor de komende jaren zijn nog dramatischer. In de Verenigde Staten wordt voorspeld dat de elektriciteitsvraag van datacenters zal groeien met een samengesteld jaarlijks percentage van 23%, mogelijk oplopend tot 1.050 TWh, of 12% van de totale elektriciteitsvraag van het land, tegen 2030.3 Deze stijging legt ongekende druk op de elektriciteitsnetten, waardoor nutsbedrijven hun capaciteitsplanning snel moeten heroverwegen en in sommige gevallen leidt tot het uitstellen van geplande sluitingen van fossiele brandstofcentrales, zoals kolencentrales in Kansas City en West Virginia, om aan de verwachte vraag te voldoen.1

2.2. Het Meten van Faciliteitsefficiëntie: Power Usage Effectiveness (PUE)

Als reactie op groeiende zorgen over het energieverbruik van datacenters, ontwikkelde en onderschreef de industrie, geleid door het consortium The Green Grid, in 2007 een standaardmaatstaf genaamd Power Usage Effectiveness (PUE).19 PUE is ontworpen om de energie-efficiëntie van de datacenterfaciliteit te meten—dat wil zeggen, hoeveel van de totale energie die door het gebouw wordt verbruikt, direct naar het voeden van de IT-apparatuur gaat versus verloren gaat aan "overhead" functies zoals koeling, verlichting en energieconversie.
De berekening is een eenvoudige verhouding 19:
PUE=IT Equipment EnergyTotal Facility Energy​
Total Facility Energy is de totale energie die door het datacenter wordt verbruikt, meestal gemeten bij de nutsmeter. Het omvat alles: servers, netwerkinfrastructuur, koelsystemen, verlichting, beveiligingssystemen en energieverliezen in ononderbroken stroomvoorzieningen (UPS) en energieverdeelunits (PDU's).19
IT Equipment Energy is het gedeelte van de energie dat uitsluitend door de computerhardware zelf wordt verbruikt—de servers, opslagarrays en netwerkswitches die het werk van het verwerken en opslaan van gegevens uitvoeren.20
Een PUE van 1.0 vertegenwoordigt een theoretisch ideaal van perfecte efficiëntie, waarbij 100% van de energie die het gebouw binnenkomt, door de IT-apparatuur wordt gebruikt met nul overhead.21 In werkelijkheid is dit onmogelijk te bereiken. Toen PUE werd geïntroduceerd, was het gemiddelde in de industrie een zeer inefficiënte 2.5, wat betekent dat voor elke 2.5 watt die uit het net werd getrokken, slechts 1 watt de IT-apparatuur bereikte.19 De introductie van PUE stimuleerde een succesvolle inspanning in de hele industrie om de efficiëntie van de infrastructuur te verbeteren. Vanaf 2021-2023 was het wereldwijde gemiddelde PUE gedaald tot ongeveer 1.57, een aanzienlijke verbetering.19
Leidende hyperscale datacenteroperators, die zwaar hebben geïnvesteerd in geoptimaliseerde ontwerpen en geavanceerde koeling, rapporteren zelfs nog indrukwekkendere cijfers. In het eerste kwartaal van 2025 meldde Google een gemiddelde PUE van 1.09 over de afgelopen twaalf maanden (TTM), wat betekent dat slechts 9% van zijn energieverbruik overhead van de faciliteit is.22
De onderstaande tabel presenteert recente PUE-gegevens van een selectie van Google's wereldwijde datacenters. Het illustreert niet alleen de hoge efficiëntie die door state-of-the-art faciliteiten is bereikt, maar ook de variabiliteit van de maatstaf op basis van het lokale klimaat en seizoen.
Tabel 2: PUE-waarden voor Representatieve Hyperscale Datacenters (2024-2025)
Datacenter Locatie
Kwartaal
Kwartaal PUE
Trailing 12-Month (TTM) PUE
Gemiddelde Google Vloot
Q1 2025
1.08
1.09
Centraal Ohio, VS (Lancaster)
Q1 2025
1.04
1.04
Hamina, Finland
Q4 2024
1.09
1.10
Dublin, Ierland
Q4 2024
1.08
1.08
Douglas County, Georgia, VS
Q3 2024
1.12
1.09
Storey County, Nevada, VS
Q3 2024
1.22
1.17
Singapore (2e faciliteit)
Q1 2025
1.14
1.15

Bron: Aangepast van Google Data Center Efficiency rapporten, juli 2025.22 TTM PUE is een voortschrijdend gemiddelde van de voorgaande vier kwartalen.

2.3. Een Kritische Onderzoek naar de Beperkingen van PUE

Ondanks de brede acceptatie en het succes in het verlagen van infrastructuurafval, is PUE een onvolledige en soms misleidende maatstaf voor de algehele duurzaamheid. De beperkingen ervan zijn cruciaal om het volledige milieubeeld te begrijpen.
Ten eerste biedt PUE een "laagje van efficiëntie" door zich uitsluitend te concentreren op de prestaties van de infrastructuur van de faciliteit, terwijl de primaire bron van energieverbruik—de IT-apparatuur zelf—wordt genegeerd. Een datacenter kan een wereldklasse PUE hebben terwijl de servers inefficiënte algoritmen uitvoeren of grotendeels inactief zijn, waardoor enorme hoeveelheden energie worden verbruikt om weinig of geen nuttig werk te verrichten.23 Het succes van de maatstaf in het richten van de industrie op het optimaliseren van de PUE-verhouding heeft de aandacht afgeleid van de explosieve groei in absoluut energieverbruik. Een faciliteit kan trots een uitstekende PUE van 1.1 rapporteren terwijl het totale energieverbruik—en dus de milieu-impact—van jaar tot jaar verdubbelt om te voldoen aan de onverzadigbare eisen van AI.
Ten tweede is PUE blind voor de bron van energie. Het maakt geen onderscheid tussen elektriciteit die wordt opgewekt uit het verbranden van kolen en elektriciteit uit zonne- of windenergie. Een datacenter in een kolenrijke regio met een PUE van 1.5 heeft een fundamenteel andere koolstofvoetafdruk dan een faciliteit met dezelfde PUE in een regio met een schoon net, maar de maatstaf behandelt ze als gelijkwaardig.25
Ten derde was PUE oorspronkelijk bedoeld voor interne benchmarking—het volgen van de efficiëntie van een enkele faciliteit in de loop van de tijd—niet voor directe, competitieve vergelijkingen tussen verschillende datacenters.27 Dergelijke vergelijkingen kunnen zeer misleidend zijn omdat PUE sterk wordt beïnvloed door externe factoren zoals lokaal klimaat, hoogte en datacenterontwerp, die buiten de directe controle van een operator liggen.24 Een faciliteit in een koel klimaat zoals Finland kan gemakkelijker een lage PUE bereiken door "gratis koeling" dan een in een heet, vochtig klimaat zoals Singapore, dat afhankelijk moet zijn van energie-intensievere koelmachines.28
Ten slotte kan PUE tegenintuïtieve resultaten opleveren. Naarmate IT-apparatuur energie-efficiënter wordt, kan de PUE paradoxaal genoeg toenemen (d.w.z. slechter worden). Dit gebeurt omdat de IT-energie, die de noemer is in de PUE-formule (Totaal/IT), afneemt. Als de overheadenergie constant blijft, neemt de verhouding toe, waardoor de faciliteit minder efficiënt lijkt, zelfs als het absolute energieverbruik is gedaald.24 Deze tekortkoming toont aan hoe een obsessieve focus op de PUE-maatstaf echte vooruitgang in de algehele energievermindering kan verdoezelen en zelfs bestraffen.

3. Van Energie naar Emissies: Een Veelzijdige Koolstofvoetafdruk

Het omzetten van ruwe elektriciteitsconsumptie in de uiteindelijke klimaatimpact—de koolstofvoetafdruk—vereist een complexere analyse. De totale broeikasgasemissies die verband houden met een AI-model hangen niet alleen af van hoeveel energie het verbruikt, maar ook van waar die energie vandaan komt en de verborgen milieu kosten die zijn ingebed in de hardware waarop het draait. Deze sectie onthult dat de koolstofvoetafdruk van AI een driedimensionaal probleem is, bepaald door de aard van de rekenbelasting, de koolstofintensiteit van het lokale elektriciteitsnet en de levenscyclusemissies van de fysieke infrastructuur.

3.1. Het Grote Debat: Training vs. Inferentie

Jarenlang concentreerde de publieke en academische discussie over de milieu-impact van AI zich op de enorme, eenmalige energiekosten van het trainen van een groot model. Dit intensieve proces, dat weken of maanden kan duren op duizenden gespecialiseerde processors, genereert een aanzienlijke koolstofvoetafdruk. Vroege, invloedrijke studies produceerden alarmerende vergelijkingen, waarbij de emissies van het trainen van een enkel groot AI-model werden gelijkgesteld aan de levenslange emissies van meerdere auto's of honderden transatlantische vluchten.1
Echter, er is een kritische paradigma verschuiving gaande, aangedreven door recent onderzoek en onthullingen van grote technologiebedrijven. Terwijl de trainingsfase onmiskenbaar energie-intensief is, breidt de focus zich nu uit om de inferentiefase in te sluiten—het continue, operationele gebruik van een model om vragen te beantwoorden en taken uit te voeren. Hoewel een enkele inferentie veel minder energie verbruikt dan de gehele trainingsronde, kan het sheer volume aan vragen dat door een populair model zoals ChatGPT wordt bediend leiden tot een cumulatieve energieconsumptie die rivaliseert met, en vaak de initiële trainingskosten overschrijdt.14
Deze verschuiving wordt ondersteund door overtuigend kwantitatief bewijs van de toonaangevende AI-ontwikkelaars in de industrie:
Google-analisten hebben geschat dat inferentie verantwoordelijk is voor 60% van de totale energie die door hun generatieve AI-systemen wordt verbruikt, met training die de resterende 40% uitmaakt.33
Meta heeft gerapporteerd dat inferentieprocessen verantwoordelijk kunnen zijn voor tot 70% van het totale energieverbruik dat met AI samenhangt.34
Amazon Web Services (AWS) heeft aangegeven dat inferentiegerelateerde werkbelastingen tot 80-90% van de vraag naar AI-rekenbronnen kunnen vertegenwoordigen.34
Onafhankelijke academische studies bevestigen deze trend, met schattingen dat inferentie verantwoordelijk is voor 30-65% van de totale levenscyclusemissies van een AI-model, een aandeel dat waarschijnlijk zal groeien naarmate AI breder wordt aangenomen.36
Bovendien is de energiekost van inferentie niet uniform; deze is sterk afhankelijk van de specifieke taak die wordt uitgevoerd. Generatieve taken, zoals het creëren van tekst of afbeeldingen, zijn aanzienlijk duurder in termen van berekeningen en veroorzaken dus veel meer emissies dan classificatietaken, zoals het identificeren van objecten in een foto of het categoriseren van tekst.36 Binnen generatieve AI hebben beeldgerelateerde taken een aanzienlijk hogere koolstofvoetafdruk dan niet-beeldtaken. Een studie ontdekte bijvoorbeeld dat beeldgeneratie aanzienlijk hogere emissies produceerde dan enige andere onderzochte taak, wat een aanzienlijke milieuzorg toevoegt aan de voortdurende debatten rond AI-gegeneerde kunst.36

3.2. De Beslissende Rol van het Net: De Impact van Koolstofintensiteit

De hoeveelheid koolstofdioxide die per eenheid energie die wordt verbruikt, wordt uitgestoten, is geen globale constante. Het is een directe functie van de "koolstofintensiteit" van het lokale elektriciteitsnet, die de mix van energiebronnen die voor opwekking wordt gebruikt, weerspiegelt. Een berekening die wordt aangedreven door een net dat sterk afhankelijk is van kolen of aardgas, zal een veel hogere koolstofvoetafdruk hebben dan exact dezelfde berekening die wordt uitgevoerd in een regio met overvloedige waterkracht, nucleaire of hernieuwbare energiebronnen.4 Deze koolstofintensiteit wordt doorgaans gemeten in grammen koolstofdioxide-equivalent per kilowattuur (gCO2eq/kWh).
Deze factor is van het grootste belang omdat datacenters niet uniform over de wereld zijn verspreid. Ze zijn geconcentreerd in specifieke geografische hubs, gekozen op basis van factoren zoals connectiviteit, beschikbaarheid van land en gunstige belastingregimes. Belangrijke hubs zijn onder andere Noord-Virginia in de Verenigde Staten (vaak de "datacenterhoofdstad van de wereld" genoemd), Dublin in Ierland en de stadstaat Singapore.2 De immense concentratie van datacenters in deze locaties legt enorme druk op hun lokale elektriciteitsnetten en bedreigt hun vermogen om klimaatdoelen te bereiken. Als reactie hebben sommige jurisdicties, waaronder Dublin en Singapore, gedwongen tijdelijke moratoria op de bouw van nieuwe datacenters moeten implementeren om de druk op hun energie-infrastructuur te beheersen.2
De onderstaande tabel illustreert de dramatische variatie in koolstofintensiteit van het net over deze belangrijke hubs, wat aantoont waarom de locatie van een AI-werkbelasting een cruciale determinant is van de milieu-impact ervan.
Tabel 3: Vergelijkende Koolstofintensiteit van het Net (gCO2eq/kWh) voor Sleutel Datacenter Hubs

Datacenter Hub
Koolstofintensiteit (gCO2/kWh)
Jaar
Primaire Energiebronnen
Bron
Virginia, VS
~269
2023
Aardgas (35%), Nucleair (41%), Kolen (20%)
38
Ierland (Dublin)
234 (vraag) / 291 (opwekking)
2023
Aardgas, Wind, Invoeren
40
Singapore
412
2023
Aardgas (94.5%)
41
Wereldgemiddelde
~445
2024
Mix van Fossiele Brandstoffen, Nucleair, Hernieuwbare Energie
42

Bronnen: U.S. Energy Information Administration (EIA) 38, Ierlands Climate Change Advisory Council 40, Singapore's Energy Market Authority 41, International Energy Agency (IEA).42 Opmerking: Het cijfer voor Virginia is omgezet van 594 lbs/MWh. Het cijfer voor Ierland toont zowel de intensiteit aan de vraagzijde (die invoeren als nul-koolstof telt) als de hogere intensiteit aan de opwekkingzijde.
Deze gegevens maken duidelijk dat een kilowattuur energie niet milieuvriendelijk gelijk is over de wereld. Een AI-werkbelasting die in Singapore wordt uitgevoerd, genereert bijvoorbeeld meer dan 50% meer koolstofemissies dan dezelfde werkbelasting die in Virginia wordt uitgevoerd, simpelweg vanwege het verschil in hun elektriciteitsnetten.

3.3. Verweven Koolstof: De Verborgen Voetafdruk van Hardware

De analyse van de koolstofvoetafdruk is onvolledig als alleen de operationele emissies van elektriciteitsverbruik worden overwogen. Een uitgebreide verantwoording moet ook de verweven koolstof omvatten—de broeikasgasemissies die verband houden met de gehele levenscyclus van de fysieke hardware, van de winning van grondstoffen en de productie van componenten tot transport, en uiteindelijk, verwijdering en recycling.43 Deze holistische benadering staat bekend als een Life Cycle Assessment (LCA).44
Tot voor kort was de verweven koolstof van gespecialiseerde AI-hardware zoals GPU's en TPU's een belangrijke blinde vlek in milieuanalyses. Dit veranderde met een baanbrekende studie van Google in februari 2025, die de eerste uitgebreide LCA van zijn op maat ontworpen Tensor Processing Units (TPUs) publiceerde.12 Dit onderzoek bood ongekende inzichten in de volledige "cradle-to-grave" emissies van AI-versnellers.
De belangrijkste bevindingen van de Google TPU-studie waren:
Operationele Emissies Domineren (Voor Nu): Voor huidige generaties van TPU's is het operationele elektriciteitsverbruik de grootste bijdrage aan de levenslange emissies, goed voor meer dan 70% van de totale koolstofvoetafdruk.45 Dit onderstreept het onmiddellijke belang van het verbeteren van de energie-efficiëntie van chips en het decarboniseren van de elektriciteitsnetten die hen van stroom voorzien.
Productie Telt: Hoewel operationele emissies momenteel dominant zijn, is de verweven koolstof van de productie nog steeds een significante factor. Cruciaal is dat, naarmate elektriciteitsnetten schoner worden en operationele emissies afnemen, het relatieve aandeel van verweven koolstof in de totale levenscyclusvoetafdruk zal toenemen, waardoor duurzame productie een steeds kritischer aandachtspunt wordt.45
Een Nieuwe Maatstaf: Compute Carbon Intensity (CCI): Om gestandaardiseerde, vergelijkbare vergelijkingen van de duurzaamheid van hardware over verschillende generaties en types chips mogelijk te maken, introduceerde de studie een nieuwe maatstaf: Compute Carbon Intensity (CCI). Het wordt gedefinieerd als de totale levenscyclus koolstofemissies per eenheid van berekening 12:
CCI=ExaFLOPgram CO2​e​
Een lagere CCI-score duidt op een koolstofefficiënte versneller, wat betekent dat deze minder emissies genereert voor dezelfde hoeveelheid rekenwerk.
Generatieverbeteringen: De studie onthulde dramatische verbeteringen in koolstofefficiëntie met elke nieuwe generatie hardware. Google's 6e generatie TPU, Trillium (TPU v6e), biedt een verbetering van 3x in CCI (d.w.z. het is drie keer koolstofefficiënter) vergeleken met de TPU v4i, die slechts een paar jaar eerder werd uitgebracht.43 Dit benadrukt het snelle tempo van innovatie in hardware-efficiëntie, wat een krachtige hefboom is voor het mitigeren van de milieu-impact van AI.
De onderlinge verbondenheid van deze drie dimensies—werkbelasting, locatie en hardware—is multiplicatief. Een inefficiënte, generatieve inferentietaak (werkbelasting) die wordt uitgevoerd in een datacenter op een hoog-koolstofnet (locatie) met hardware met een grote verweven koolstofvoetafdruk (hardware) vertegenwoordigt een slecht geval van milieueffecten. Omgekeerd vereist echte duurzaamheid optimalisatie over alle drie de dimensies tegelijk: het ontwikkelen van efficiënte algoritmen, deze uitvoeren op gespecialiseerde hardware en die hardware van schone energie voorzien in locaties met een lage koolstofintensiteit.

4. De Over het Hoofd Gezien Hulpbron: AI's Onverzadigbare Dorst naar Water

Naast energie en koolstof is er een andere kritische, vaak onzichtbare hulpbron die de AI-industrie ondersteunt: water. Datacenters, het fysieke hart van AI, hebben een immense dorst en verbruiken miljarden liters zoetwater, voornamelijk voor het koelen van de krachtige processors die enorme hoeveelheden warmte genereren.6 Deze sectie belicht de watervoetafdruk van AI, een groeiende milieuzorg die de huidige industriemaatstaven onvoldoende vastleggen.

4.1. Koeling van de Cloud: De Rol van Water en de WUE-Maatstaf

De dichte concentratie van servers in een modern datacenter genereert een enorme thermische belasting. Om oververhitting te voorkomen en een betrouwbare werking te waarborgen, moet deze warmte constant worden afgevoerd. Terwijl sommige datacenters luchtgebaseerde koeling gebruiken, vertrouwen veel van de grootste faciliteiten op verdampingskoelsystemen, zoals koeltorens, die zeer effectief zijn maar grote hoeveelheden water verbruiken.29 In deze systemen wordt water verdampt om warmte uit de faciliteit te verwijderen, een proces dat vergelijkbaar is met hoe transpiratie het menselijk lichaam koelt.
Om de water efficiëntie van deze operaties te meten, introduceerde The Green Grid de Water Usage Effectiveness (WUE) maatstaf. Deze wordt gedefinieerd als de verhouding van het jaarlijkse waterverbruik van een datacenter tot de energie die door de IT-apparatuur wordt verbruikt 47:
WUE=IT Equipment Energy (kWh)Jaarlijks Waterverbruik (Liters)​
Een lagere WUE-waarde betekent een hogere water efficiëntie. Het gemiddelde datacenter heeft naar schatting een WUE van 1.8 L/kWh, wat betekent dat het 1.8 liter water gebruikt voor elke kilowattuur energie die door zijn servers en opslag wordt verbruikt.47

4.2. Kwantificeren van de Dorst: Water voor Training en Inferentie

Recente onderzoeken hebben begonnen de specifieke watervoetafdruk van AI te kwantificeren, en de cijfers zijn alarmerend. Een baanbrekende studie uit 2023, "Making AI Less 'Thirsty'," bood de eerste gedetailleerde publieke schattingen van het waterverbruik door grote taalmodellen.50
Belangrijke bevindingen uit deze en gerelateerde onderzoeken zijn onder andere:
Trainingsverbruik: Het trainen van OpenAI's GPT-3-model in de Amerikaanse datacenters van Microsoft wordt geschat op een direct verbruik van 700.000 liters (ongeveer 185.000 gallons) schoon, ter plaatse beschikbaar zoetwater voor koeling.48 Wanneer off-site water dat voor elektriciteitsopwekking wordt gebruikt, wordt meegerekend, stijgt de totale operationele watervoetafdruk voor het trainen van GPT-3 naar een geschatte 5.4 miljoen liters.50
Inferentieverbruik: De watervoetafdruk strekt zich uit tot elke gebruikersinteractie. Dezelfde studie schat dat een eenvoudig gesprek met een model zoals ChatGPT, bestaande uit 10 tot 50 vragen en antwoorden, een fles van 500 ml water kan "drinken".6
Geprojecteerde Wereldwijde Vraag: De cumulatieve impact is enorm. De wereldwijde AI-vraag wordt geschat op 4.2 tot 6.6 miljard kubieke meters (1.1 tot 1.7 biljoen gallons) wateronttrekking in 2027 alleen—een hoeveelheid groter dan de totale jaarlijkse wateronttrekking van landen zoals Denemarken of de helft van die van het Verenigd Koninkrijk.48
Stijgende Waterconsumptie van Bedrijven: Deze trend wordt weerspiegeld in de duurzaamheidsrapporten van grote technologiebedrijven. In één jaar (2021-2022) steeg het waterverbruik van Microsoft met 34%, terwijl dat van Google met 20% steeg, waarbij beide bedrijven de stijging grotendeels toeschrijven aan de eisen van hun groeiende AI-operaties.1
De watervoetafdruk van een AI-model is geen vaste waarde; deze varieert dramatisch afhankelijk van de geografische locatie van het datacenter. Deze ruimtelijke variabiliteit wordt gedreven door twee belangrijke factoren: het lokale klimaat, dat de koelbehoeften dicteert, en de waterintensiteit van het lokale elektriciteitsnet. De onderstaande tabel, gebaseerd op gegevens van de studie "Making AI Less 'Thirsty'," biedt gedetailleerde schattingen van de watervoetafdruk voor het trainen en gebruiken van GPT-3 op verschillende wereldwijde locaties, waarmee deze variabiliteit krachtig wordt geïllustreerd.
Tabel 4: Geschat Waterverbruik voor LLM Training en Inferentie per Geografische Locatie
Datacenter Locatie
Totaal Water voor Training (miljoen L)
Water per Inferentie (mL)

# Inferences om 500ml Water te Verbruiken

Gemiddelde VS
5.44
16.9
29.6
Arizona, VS
9.63
29.9
16.7
Virginia, VS
3.68
11.4
43.7
Iowa, VS
4.81
15.0
33.4
Ierland
2.29
7.1
70.4
Nederland
5.13
15.9
31.4
Finland
6.56
20.4
24.5
India
6.34
19.7
25.4

Bron: Aangepast van Peng et al., "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models," 2023.50 "Totaal Water voor Training" en "Water per Inferentie" omvatten zowel ter plaatse (Scope 1) als off-site (Scope 2) operationeel waterverbruik.
Deze gegevens onthullen dat het trainen van hetzelfde model in een watergestrest, heet gebied zoals Arizona meer dan vier keer zoveel water verbruikt als het trainen ervan in een koeler, water-efficiënter gebied zoals Ierland. Evenzo kan een gebruiker in Ierland meer dan vier keer zoveel vragen stellen als een gebruiker in Arizona voordat hun interactie dezelfde 500 ml water verbruikt. Dit onderstreept dat "waar" AI wordt uitgevoerd een cruciale determinant is van de watervoetafdruk.

4.3. Kritiek op WUE: Een Misleidend Eenvoudige Maatstaf

Net zoals PUE een onvolledig beeld van energie duurzaamheid biedt, heeft de WUE-maatstaf aanzienlijke beperkingen die de volledige reikwijdte van AI's impact op watervoorraden verdoezelen.
De meest significante tekortkoming is het probleem van "verweven water." WUE houdt alleen rekening met het directe, ter plaatse verbruikte water dat wordt gebruikt voor de koeling van het datacenter. Het negeert volledig de enorme hoeveelheden "indirect" of "off-site" water die worden verbruikt om de elektriciteit te genereren die de faciliteit van stroom voorziet.29 Energieopwekking, vooral uit thermische bronnen zoals kolen, aardgas en nucleair, evenals sommige vormen van waterkracht, is een zeer waterintensief proces. Door deze off-site watervoetafdruk niet mee te tellen, rapporteert WUE systematisch de ware waterimpact van de operaties van een datacenter te laag.
Een andere belangrijke kritiek is dat WUE de waterbron negeert. De maatstaf beschouwt elke liter water als gelijkwaardig, zonder onderscheid te maken tussen het onttrekken van schaarse, sterk behandelde drinkwater uit een gemeentelijke voorziening en het gebruiken van meer duurzame bronnen zoals niet-drinkbaar rivierwater of gerecycled "grijs water" uit andere industriële processen.29 De ecologische en maatschappelijke kosten van het consumeren van één liter drinkwater in een droogtegebied zijn veel groter dan die van het gebruik van één liter onbehandeld rivierwater in een waterrijke omgeving, maar WUE is blind voor deze kritische context.
Ten slotte is er vaak een inherente afweging tussen water efficiëntie en energie efficiëntie. Datacenteroperators moeten kiezen tussen verschillende koeltechnologieën. Luchtgebaseerde koelsystemen gebruiken meer energie (wat resulteert in een slechtere PUE) maar verbruiken weinig tot geen water (een perfecte WUE van 0). Omgekeerd zijn watergebaseerde verdampingskoelsystemen energie-efficiënter (een betere PUE) maar gebruiken aanzienlijke hoeveelheden water (een slechtere WUE).29 Dit creëert een moeilijke dilemma voor operators, waardoor ze twee concurrerende milieuprioriteiten moeten balanceren. Optimaliseren voor één maatstaf kan onbedoelde negatieve gevolgen voor de andere hebben, waardoor holistische duurzaamheid een complexe, multi-variabele uitdaging wordt die een enkele maatstaf zoals WUE niet kan oplossen.

5. Een Duurzaam Pad Uitzetten: Mitigatiestrategieën en Groene AI

Het analyseren van de schaal van de milieu-impact van AI is een noodzakelijke eerste stap, maar het uiteindelijke doel is om deze te mitigeren. Een groeiend aantal onderzoeken en industriële praktijken richt zich op het ontwikkelen van oplossingen die de hulpbronnenintensiteit van AI kunnen verminderen zonder innovatie te verstikken. Deze strategieën beslaan de gehele technologie-stack, van de abstracte wiskunde van het algoritme en het ontwerp van de software, tot de fysieke silicium van de processors en de locatie van de datacenters. Deze "full-stack" benadering is essentieel, aangezien optimalisaties op verschillende lagen niet alleen optellen, maar vaak multiplicatief zijn in hun voordelen.

5.1. Algoritmische en Software-innovaties ("Software-First" Oplossingen)

De meest fundamentele manier om de milieu-impact van AI te verminderen, is door de vraag naar berekeningen aan de bron te verlagen: het model zelf. Een reeks "software-first" technieken heeft als doel AI-modellen efficiënter te maken, waardoor hun reken- en geheugeneisen worden verminderd zonder significante prestatieverlies.
Model Pruning en Quantization: Dit zijn twee van de meest effectieve modelcompressietechnieken. Pruning houdt in dat systematisch overbodige of onbelangrijke verbindingen (parameters) binnen een neuraal netwerk worden geïdentificeerd en verwijderd, vergelijkbaar met het snoeien van dode takken van een boom. Dit vermindert de grootte van het model en het aantal berekeningen dat voor elke operatie nodig is. Quantization houdt in dat de numerieke precisie van de gewichten van het model wordt verminderd, bijvoorbeeld door 32-bits floating-point getallen om te zetten naar 8-bits gehele getallen (INT8). Dit verkleint de geheugenspoor van het model aanzienlijk en maakt snellere, energie-efficiënte berekeningen op compatibele hardware mogelijk.13 Modeldistillatie heeft aangetoond dat het modelgroottes met tot 90% kan verminderen, wat leidt tot een vermindering van 50-60% in het energieverbruik tijdens inferentie.53
Kennisdistillatie: Deze techniek houdt in dat een groot, krachtig "leraar" model wordt gebruikt om een veel kleiner, efficiënter "student" model te trainen. Het studentmodel leert de outputs en interne representaties van de leraar na te volgen, waardoor de capaciteiten van het grotere model effectief worden overgedragen naar een compacter en minder computationeel duur architectuur.13
Efficiënte Architecturen en Parameter-Efficiënte Fijn-Tuning (PEFT): Onderzoekers richten zich steeds meer op het ontwerpen van neurale netwerkarchitecturen die van nature efficiënter zijn, zoals MobileNet en EfficientNet, die specifiek zijn gemaakt voor apparaten met een laag vermogen.13 Naast dit zijn PEFT-technieken cruciaal geworden voor het aanpassen van grote voorgetrainde modellen aan nieuwe taken. In plaats van het volledige multi-miljard parameter model opnieuw te trainen (wat prohibitief duur is), "bevriezen" methoden zoals Low-Rank Adaptation (LoRA) het oorspronkelijke model en trainen ze alleen een klein percentage van nieuwe, toegevoegde parameters. Deze aanpak kan prestaties bereiken die vergelijkbaar zijn met volledige fijn-tuning terwijl de rekenkosten met een paar ordes van grootte worden verminderd.13
Adaptieve Inferentie: Niet alle queries vereisen dezelfde hoeveelheid rekeninspanningen. Adaptieve inferentietechnieken, zoals "vroegtijdig verlaten," stellen een model in staat om zijn berekeningen dynamisch aan te passen op basis van de complexiteit van de invoer. Voor een eenvoudige, rechttoe rechtaan vraag kan het model een snelkoppeling gebruiken en een antwoord produceren nadat het slechts door enkele van zijn lagen is verwerkt. Voor een complexere vraag zou het de volledige diepte van het netwerk inschakelen. Dit voorkomt het verspillen van energie aan onnodige berekeningen voor eenvoudige taken en heeft aangetoond dat het het energieverbruik tijdens inferentie met maar liefst 20% kan verminderen.13

5.2. Hardware- en Infrastructuuroplossingen

Aanvullend op software-optimalisaties zijn er innovaties in de fysieke hardware en infrastructuur die AI aandrijven. Deze strategieën richten zich op het efficiënter leveren van de benodigde berekeningen en het minimaliseren van de milieu-impact van datacenteroperaties.
Energie-Efficiënte Hardware: De ontwikkeling van gespecialiseerde processors is een hoeksteen van duurzame AI. Chips zoals Google's Tensor Processing Units (TPUs), Apple's Neural Processing Units (NPUs) en de nieuwste generaties van GPU's van bedrijven zoals NVIDIA zijn geen algemene processors. Ze zijn specifiek ontworpen en geoptimaliseerd voor de soorten matrixvermenigvuldiging en tensoroperaties die de basis vormen van neurale netwerken. Deze specialisatie stelt hen in staat om AI-gerelateerde berekeningen veel efficiënter en met een lager energieverbruik uit te voeren dan traditionele CPU's.13
Koolstofbewuste Planning: Dit is een krachtige en steeds populairdere strategie die de geografische en temporele variabiliteit van koolstofintensiteit van het net benut. In plaats van een computationeel intensieve werkbelasting (vooral een niet-tijdgevoelige zoals modeltraining) op elk moment of op elke plaats uit te voeren, verschuiven koolstofbewuste planningssystemen de taak intelligent naar een datacenterlocatie of een tijdstip waarop het lokale elektriciteitsnet de hoogste proportie hernieuwbare energie beschikbaar heeft. Dit minimaliseert direct de koolstofemissies die verband houden met de verbruikte energie, zonder het model of de hardware te veranderen.13
Geavanceerd Datacenterontwerp: Naast de standaard PUE-verbeteringen incorporeren datacenters meer geavanceerde duurzaamheidskenmerken. Deze omvatten gratis koelingstechnieken die gebruik maken van de buitenlucht of natuurlijk koude waterbronnen om de faciliteit te koelen, waardoor de noodzaak voor energie-intensievere koelmachines wordt geëlimineerd.28 Een ander veelbelovend gebied is het hergebruik van restwarmte, waarbij de aanzienlijke hoeveelheid laagwaardige warmte die door een datacenter wordt afgevoerd, wordt opgevangen en voor andere doeleinden wordt gebruikt, zoals het verwarmen van nabijgelegen woningen of commerciële gebouwen, waardoor een afvalproduct in een waardevolle hulpbron wordt omgezet.26
Edge Computing: Voor veel inferentietaken hoeft de berekening niet plaats te vinden in een enorm, gecentraliseerd cloud datacenter. Edge computing houdt in dat de werkbelasting naar de "rand" van het netwerk wordt verschoven, naar de lokale apparaten waar de gegevens worden gegenereerd of gebruikt, zoals smartphones, slimme luidsprekers of sensoren in een fabriek. Het uitvoeren van AI-inferentie op deze kleinere, sterk geoptimaliseerde, energiezuinige apparaten kan leiden tot opmerkelijke energiebesparingen, met sommige schattingen die suggereren dat het energieverbruik per operatie met 100 tot 1000 keer kan worden verminderd in vergelijking met het uitvoeren van dezelfde taak in de cloud.13

5.3. Het Groene AI Paradigma

De verzameling strategieën die gericht zijn op het verminderen van de milieu kosten van AI wordt vaak aangeduid als Groene AI. De Green Software Foundation en andere onderzoekers definiëren Groene AI als werk dat zich richt op het verbeteren van de efficiëntie en duurzaamheid van AI-systemen gedurende hun levenscyclus. Dit omvat alles, van het ontwerpen van efficiëntere algoritmen en hardware tot het aandrijven van AI-werkbelastingen met laag-koolstofenergiebronnen.54
Het is van cruciaal belang om Groene AI te onderscheiden van het gerelateerde maar verschillende concept van "AI voor Groen" (ook bekend als AI voor Duurzaamheid). "AI voor Groen" verwijst naar de toepassing van AI als een hulpmiddel om milieuproblemen op te lossen—bijvoorbeeld het gebruik van AI om de efficiëntie van een elektriciteitsnet te optimaliseren, ontbossing te monitoren vanuit satellietbeelden, of nieuwe materialen voor batterijen te ontdekken.58 Hoewel deze toepassingen enorme beloftes inhouden, kan de focus op hen het risico van "greenwashing" met zich meebrengen, waarbij de potentiële toekomstige milieuv voordelen van AI worden gebruikt om af te leiden van of te rechtvaardigen de onmiddellijke en directe milieukosten van het ontwikkelen en uitvoeren van de technologie zelf. Groene AI daarentegen gaat over AI die zijn eigen milieuhuis schoonmaakt, en ervoor zorgt dat de technologie zelf zo duurzaam mogelijk wordt ontwikkeld en ingezet.57

6. Conclusie: De Imperatief van Transparantie en Informatie Keuzes

De reis van de abstracte FLOP naar de tastbare impact op wereldwijde elektriciteitsnetten en waterbassins onthult een complexe en snel evoluerende uitdaging. De milieu-impact van kunstmatige intelligentie is geen eenvoudig, enkel probleem, maar een multidimensionaal probleem dat een holistisch begrip en een geconcentreerde, multidimensionale respons vereist. Naarmate AI dieper geïntegreerd raakt in de weefsels van de samenleving, van een niche-technologie naar een alomtegenwoordige nut, wordt de behoefte aan verantwoordelijkheid, transparantie en duurzame praktijken van het grootste belang.

6.1. Het Aanpakken van Desinformatie door Weglating

Een belangrijke barrière voor het aanpakken van de milieu-impact van AI is een wijdverspreide gebrek aan transparantie van de belangrijkste spelers in de industrie. Een analyse uit juni 2025 met de titel "Desinformatie door Weglating" benadrukt een diep zorgwekkende trend: naarmate AI-modellen krachtiger en commercieel waardevoller zijn geworden, is de industrie paradoxaal genoeg minder transparant geworden over zijn milieu kosten.59 Een analyse van opmerkelijke AI-modellen die zijn uitgebracht tussen 2010 en begin 2025 vond dat hoewel de transparantie rond 2022 kortstondig verbeterde, de periode sinds de lancering van ChatGPT een dramatische omkering heeft gezien. Tegen het eerste kwartaal van 2025 viel de meerderheid van nieuwe, opmerkelijke AI-modellen in de categorie "geen openbaarmaking", waarbij geen publieke gegevens over hun energieverbruik of koolstofvoetafdruk werden verstrekt.59
Deze ondoorzichtigheid heeft een vacuüm gecreëerd dat is gevuld met wat het rapport "desinformatie door weglating" noemt, waar enkele uit de context gerukte of slecht onderbouwde statistieken wijdverspreid worden geciteerd als proxies voor een veel complexere realiteit. Twee prominente voorbeelden zijn:
De "Vijf Auto's" Claim: De vaak herhaalde uitspraak dat "het trainen van een AI-model evenveel CO2 uitstoot als vijf auto's in hun levensduur" komt voort uit een studie uit 2019. Deze cijfers waren echter afkomstig van een specifieke, niet-representatieve casestudy van een bijzonder inefficiënt en computationeel intensief onderzoeksproces (neuraal architectuuronderzoek). Dezelfde studie rapporteerde veel lagere emissies voor standaard modeltraining. Terwijl deze specifieke claim uit de context is gehaald, is het opmerkelijk dat de training van enkele van de grootste modellen van vandaag, zoals de Llama 3-familie van Meta, deze "vijf auto's" schatting daadwerkelijk kan overschrijden, wat de noodzaak onderstreept voor specifieke, model-voor-model gegevens in plaats van gegeneraliseerde tropen.59
De "10x een Google Zoekopdracht" Claim: De populaire bewering dat een ChatGPT-query tien keer meer energie verbruikt dan een traditionele Google-zoekopdracht kan worden herleid tot een ondoordachte opmerking uit 2023, geen rigoureuze wetenschappelijke studie. De schatting voor de Google-zoekopdracht zelf is gebaseerd op verouderde gegevens uit 2009. Toch is dit onbevestigde cijfer herhaald in tientallen nieuwsartikelen, vaak zonder bron of kwalificatie, en heeft het de publieke perceptie gevormd met onbetrouwbare informatie.59
Dit gebrek aan openbaarmaking is geen passieve nalatigheid; het is een actieve barrière voor verantwoordelijkheid. Het voorkomt dat onderzoekers onafhankelijke analyses kunnen uitvoeren, beleidsmakers effectieve regelgeving kunnen opstellen en klanten milieuvriendelijk geïnformeerde keuzes kunnen maken over de AI-diensten die ze gebruiken.

6.2. Een Kader voor Holistische Verantwoording

Het aanpakken van deze uitdaging vereist het voorbijgaan aan elke enkele maatstaf. Zoals deze bijlage heeft aangetoond, zijn maatstaven zoals PUE en WUE, hoewel nuttig, diep gebrekkig en bieden ze een onvolledig beeld. Ware verantwoordelijkheid vereist een holistisch kader dat meerdere maatstaven over de volledige levenscyclus van een AI-systeem omvat. Een uitgebreide duurzaamheidsrapportage voor een AI-model of -dienst zou moeten omvatten:
Energie-efficiëntie (PUE): Om de efficiëntie van de datacenterinfrastructuur te meten.
Water efficiëntie (WUE): Om het ter plaatse waterverbruik te meten, maar moet worden vergezeld van gegevens over de waterbron (bijv. drinkbaar vs. gerecycled) en een erkenning van off-site verweven water.
Koolstofgebruikseffectiviteit (CUE): Een maatstaf die de koolstofemissies meet die verband houden met het energieverbruik van datacenters, direct de koolstofintensiteit van het lokale net incorporerend.26
Compute Carbon Intensity (CCI): De nieuwe maatstaf uit de Google TPU-studie, die de levenscyclus koolstofefficiëntie van de hardware zelf meet, inclusief verweven koolstof.43
Alleen door te rapporteren over deze volledige reeks van maatstaven kan een compleet en eerlijk beeld van de milieu-impact van een AI-systeem ontstaan. Dit moet gepaard gaan met een toewijding aan een "cradle-to-grave" Life Cycle Assessment (LCA) benadering, die de milieukosten in de wereldwijde toeleveringsketens van hardwareproductie en -verwijdering in rekening brengt, niet alleen de gemakkelijker te meten operationele impact.44

6.3. Aanbevelingen voor een Duurzame Toekomst

Het pad naar een duurzamer AI-ecosysteem vereist gezamenlijke actie van alle belanghebbenden.
Voor AI-ontwikkelaars en organisaties:
Zet je in voor Radical Transparantie: Meet en publiceer de energieconsumptie, koolstofemissies (opgebroken naar scope en inclusief zowel training als inferentie) en watervoetafdruk (inclusief bron en volume) voor alle belangrijke modellen en diensten. Deze gegevens moeten worden opgenomen in modelkaarten, onderzoeksdocumenten en API-documentatie.51
Omarm Groene AI-principes: Integreer milieuefficiëntie als een kernontwerpeis vanaf het allereerste begin van de levenscyclus van modelontwikkeling. Geef prioriteit aan het gebruik van efficiënte algoritmen, modelcompressietechnieken en gespecialiseerde hardware.13
Benut Inkoopkracht: Bij het gebruik van derde partij cloudservices of API's, integreer milieutransparantie en prestatie-eisen in contracten en service level agreements. Vraag gegevens van leveranciers en kies partners die duurzaamheid prioriteren.59
Voor Beleidsmakers:
Stel Standaard Rapportage Verplicht: Ontwikkel en implementeer duidelijke, verplichte rapportagekaders voor de milieu-impact van AI. Deze kunnen voortbouwen op bestaande regelgeving, zoals de Corporate Sustainability Reporting Directive (CSRD) van Europa, maar met specifieke vereisten die zijn afgestemd op de AI-waardeketen.59
Stimuleer Duurzaamheid: Creëer beleidsincentives, zoals belastingvoordelen of koolstofkredieten, die de ontwikkeling en adoptie van verifieerbare "Groene AI"-technologieën, energie-efficiënte hardware en koolstofbewuste computervaardigheden belonen.13
Financier Publiek Onderzoek: Ondersteun onafhankelijk, publiek onderzoek naar efficiëntere, transparantere en duurzamere AI-systemen om een tegenwicht te bieden aan commercieel gedreven, propriëtaire ontwikkeling.
Voor Eindgebruikers:
Cultiveer Bewustzijn: Erken dat digitale interacties een fysieke kost hebben. Wees bewust van de milieu-impact die gepaard gaat met frequent of computationeel intensief gebruik van generatieve AI-diensten.48
Pleidooi voor Verandering: Als consumenten en burgers, eis meer transparantie van de technologiebedrijven die AI-diensten aanbieden. Steun bedrijven en platforms die voorop lopen in milieureportage en prestaties.
Kunstmatige intelligentie is een transformerende technologie met het potentieel om immens goed te brengen. Echter, de huidige koers van exponentiële groei in hulpbronnenverbruik is niet duurzaam. Door de milieu kosten ervan te demystificeren, transparantie te eisen en een holistische benadering van mitigatie te omarmen, kunnen we ervoor zorgen dat de zoektocht naar kunstmatige intelligentie niet ten koste gaat van de natuurlijke wereld.
Werken geciteerd
Milieu-impact van kunstmatige intelligentie - Wikipedia, geraadpleegd op 23 juli 2025, <https://en.wikipedia.org/wiki/Environmental_impact_of_artificial_intelligence>
Groeiende datavolumes drijven de behoefte aan ICT-energie-innovatie - Het Wereld Economisch Forum, geraadpleegd op 23 juli 2025, <https://www.weforum.org/stories/2024/05/data-growth-drives-ict-energy-innovation/>
Hoe datacenters en de energiesector de honger van AI naar energie kunnen stillen - McKinsey, geraadpleegd op 23 juli 2025, <https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power>
Datacenters & netwerken - IEA, geraadpleegd op 23 juli 2025, <https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks>
Energiebehoeften van datacenters kunnen elektriciteitsnetten ontwrichten en het klimaat bedreigen | Artikel, geraadpleegd op 23 juli 2025, <https://www.eesi.org/articles/view/data-center-energy-needs-are-upending-power-grids-and-threatening-the-climate>
AI-waterverbruik: De onhoudbare dorst van generatieve AI - De Toekomst van Handel, geraadpleegd op 23 juli 2025, <https://www.the-future-of-commerce.com/2023/10/10/ai-water-consumption/>
FLOPs in LLM Training: De Ultieme Gids | door Pratish Dewangan | Jul, 2025 | Medium, geraadpleegd op 23 juli 2025, <https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48>
Het Belang van FLOPS en de Impact op de Snelheid van je PC en ..., geraadpleegd op 23 juli 2025, <https://www.lenovo.com/us/en/glossary/flops/>
Flops - Lark, geraadpleegd op 23 juli 2025, <https://www.larksuite.com/en_us/topics/ai-glossary/flops>
FLOPS (Floating Point Operations Per Second) — Klu, geraadpleegd op 23 juli 2025, <https://klu.ai/glossary/flops>
FLOPs: Computationele Complexiteit van Machine Learning Modellen - Ultralytics, geraadpleegd op 23 juli 2025, <https://www.ultralytics.com/glossary/flops>
Levenscyclusemissies van AI-hardware: Een Cradle-To-Grave ... - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/pdf/2502.01671v1.pdf?ref=aquietlittlerebellion.com>
Hoe kunnen we AI- en ML-algoritmen optimaliseren om energieverbruik te verminderen en duurzaamheid in computing te verbeteren? | ResearchGate, geraadpleegd op 23 juli 2025, <https://www.researchgate.net/post/How_can_we_optimize_AI_and_ML_algorithms_to_reduce_energy_consumption_and_improve_sustainability_in_computing>
Het Kwantificeren van het Energieverbruik en de Koolstofemissies van LLM Inferentie via Simulaties - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/html/2507.11417v1>
De FLOPs Calculus van Training van Taalmodellen | door Dzmitry Bahdanau - Medium, geraadpleegd op 23 juli 2025, <https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4>
Groot taalmodel - Wikipedia, geraadpleegd op 23 juli 2025, <https://en.wikipedia.org/wiki/Large_language_model>
Optimaliseer ook voor inferentie, niet alleen voor trainings-FLOPs - MatX, geraadpleegd op 23 juli 2025, <https://matx.com/research/lifetime_llm_cost>
Meer dan 30 AI-modellen zijn getraind op de schaal van GPT-4 | Epoch AI, geraadpleegd op 23 juli 2025, <https://epoch.ai/data-insights/models-over-1e25-flop>
Wat is Power Usage Effectiveness (PUE)? - Digital Realty, geraadpleegd op 23 juli 2025, <https://www.digitalrealty.com/resources/articles/what-is-power-usage-effectiveness>
Wat is Power Usage Effectiveness (PUE) in het Datacenter ..., geraadpleegd op 23 juli 2025, <https://blog.purestorage.com/purely-educational/what-is-power-usage-effectiveness-pue-in-the-data-center/>
Begrijpen van Power Usage Effectiveness (PUE) in Datacenters - Komprise, geraadpleegd op 23 juli 2025, <https://www.komprise.com/glossary_terms/power-usage-effectiveness-pue/>
Power usage effectiveness – Google Data Centers, geraadpleegd op 23 juli 2025, <https://datacenters.google/efficiency>
Power usage effectiveness in datacenters: Overloaded and underachieving - ResearchGate, geraadpleegd op 23 juli 2025, <https://www.researchgate.net/publication/303847575_Power_usage_effectiveness_in_data_centers_Overloaded_and_underachieving>
PUE in 2024: Wat het is en wat het niet is, geraadpleegd op 23 juli 2025, <https://blog.stulz-usa.com/pue-2024>
Het Belang van Power Usage Effectiveness in een Datacenter, geraadpleegd op 23 juli 2025, <https://www.trgdatacenters.com/resource/the-importance-of-power-usage-effectiveness-in-a-datacenter/>
Hoe Werkt Power Usage Effectiveness? - Energie → Duurzaamheidsdirectory, geraadpleegd op 23 juli 2025, <https://energy.sustainability-directory.com/question/how-does-power-usage-effectiveness-work/>
REHVA Journal Analyse van prestatiemaatstaven voor de efficiëntie van datacenters – moet de Power Utilization Effectiveness PUE nog steeds worden gebruikt als de belangrijkste indicator? (Deel 1), geraadpleegd op 23 juli 2025, <https://www.rehva.eu/rehva-journal/chapter/analysis-of-performance-metrics-for-data-center-efficiency-should-the-power-utilization-effectiveness-pue-still-be-used-as-the-main-indicator-part-1>
PUE: Power usage effectiveness | Flexential, geraadpleegd op 23 juli 2025, <https://www.flexential.com/resources/brochure/power-usage-effectiveness>
Wat is Water Usage Effectiveness (WUE) in Datacenters ..., geraadpleegd op 23 juli 2025, <https://blog.equinix.com/blog/2024/11/13/what-is-water-usage-effectiveness-wue-in-data-centers/>
Energie-Efficiënt Training en Inferentie in Grote Taalmodellen: Optimaliseren van Computationele en Energie Kosten - ResearchGate, geraadpleegd op 23 juli 2025, <https://www.researchgate.net/publication/392908333_Energy-Efficient_Training_and_Inference_in_Large_Language_Models_Optimizing_Computational_and_Energy_Costs>
Naar Duurzame NLP: Inzichten uit Benchmarking Inferentie-energie in Grote Taalmodellen - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/html/2502.05610v1>
Het Meten en Verbeteren van de Energie-efficiëntie van Inferentie in Grote Taalmodellen, geraadpleegd op 23 juli 2025, <https://www.researchgate.net/publication/381199506_Measuring_and_Improving_the_Energy_Efficiency_of_Large_Language_Models_Inference>
AI's Milieu-impact: Een Informatie Keuze Maken - Marmelab, geraadpleegd op 23 juli 2025, <https://marmelab.com/blog/2025/03/19/ai-carbon-footprint.html>
De Energie Kost van Redeneren: Analyseren van Energieverbruik in LLMs ..., geraadpleegd op 23 juli 2025, <https://arxiv.org/pdf/2505.14733>
De Energie Kost van Redeneren: Analyseren van Energieverbruik in LLMs met Testtijd Compute, geraadpleegd op 23 juli 2025, <https://arxiv.org/html/2505.14733v1>
Emissies van Kunstmatige Intelligentie (AI) gebruik — Register Dynamics ..., geraadpleegd op 23 juli 2025, <https://www.register-dynamics.co.uk/blog/emissions-from-artificial-intelligence-ai-use>
AI's Milieu-impact: Berekend en Uitleg - Arbor.eco, geraadpleegd op 23 juli 2025, <https://www.arbor.eco/blog/ai-environmental-impact>
Virginia Elektriciteitsprofiel 2023 - U.S. Energy Information ..., geraadpleegd op 23 juli 2025, <https://www.eia.gov/electricity/state/virginia/>
State of Virginia Energy Sector Risk Profile, geraadpleegd op 23 juli 2025, <https://www.energy.gov/sites/prod/files/2016/09/f33/VA_Energy%20Sector%20Risk%20Profile.pdf>
Jaarlijkse Review 2024: Elektriciteit - Climate Change Advisory Council, geraadpleegd op 23 juli 2025, <https://www.climatecouncil.ie/councilpublications/annualreviewandreport/AR2024-Electricity-final.pdf>
EMA | SES Hoofdstuk 2: Energie Transformatie, geraadpleegd op 23 juli 2025, <https://www.ema.gov.sg/resources/singapore-energy-statistics/chapter2>
Emissies – Elektriciteit 2025 – Analyse - IEA, geraadpleegd op 23 juli 2025, <https://www.iea.org/reports/electricity-2025/emissions>
arxiv.org, geraadpleegd op 23 juli 2025, <https://arxiv.org/html/2502.01671v1>
Google Cloud meet zijn klimaatimpact via LCA, geraadpleegd op 23 juli 2025, <https://cloud.google.com/blog/topics/sustainability/google-cloud-measures-its-climate-impact-through-life-cycle-assessment>
TPU's verbeterden de koolstofefficiëntie van AI-werkbelastingen met 3x | Google Cloud Blog, geraadpleegd op 23 juli 2025, <https://cloud.google.com/blog/topics/sustainability/tpus-improved-carbon-efficiency-of-ai-workloads-by-3x>
Levenscyclusemissies van AI-hardware: Een Cradle-To-Grave Benadering en Generatie Trends | AI Research Paper Details - AIModels.fyi, geraadpleegd op 23 juli 2025, <https://www.aimodels.fyi/papers/arxiv/life-cycle-emissions-ai-hardware-cradle-to>
Wat is Water Usage Effectiveness (WUE)? - Sunbird DCIM, geraadpleegd op 23 juli 2025, <https://www.sunbirddcim.com/glossary/water-usage-effectiveness-wue>
De Vaak Over het Hoofd Gezien Watervoetafdruk van AI-modellen | door Julia Barnett, geraadpleegd op 23 juli 2025, <https://generative-ai-newsroom.com/the-often-overlooked-water-footprint-of-ai-models-46991e3094b6>
De Betekenis van de Water Usage Effectiveness Maatregel in de Duurzaamheid van Datacenters, geraadpleegd op 23 juli 2025, <https://www.nlyte.com/blog/the-significance-of-the-water-usage-effectiveness-measure-in-data-center-sustainability/>
Making AI Less 'Thirsty': Uncovering and Addressing the ... - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/pdf/2304.03271>
Water Is the New CO2 - sustAIn, geraadpleegd op 23 juli 2025, <https://sustain.algorithmwatch.org/en/water-is-the-new-co2/>
19 Praktische Manieren om de Milieu-impact van AI te Verminderen - Forbes, geraadpleegd op 23 juli 2025, <https://www.forbes.com/councils/forbestechcouncil/2025/05/21/19-practical-ways-to-reduce-ais-environmental-impact/>
Onderzoek naar Energie-efficiëntie en Prestatieafwegingen in LLM Inferentie Over Taken en DVFS-instellingen - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/pdf/2501.08219>? 
Top 3 Groene AI-methoden die AI-gestuurde Duurzaamheidsgebruikscases bevorderen | door René Bostic, geraadpleegd op 23 juli 2025, <https://medium.com/@renebostic/top-3-green-ai-methods-advancing-ai-enabled-sustainability-use-cases-f2b0d806cc6d>
Koolstofemissies in de Uitlaat van Generatieve AI - Harvard Data Science Review, geraadpleegd op 23 juli 2025, <https://hdsr.mitpress.mit.edu/pub/fscsqwx4>
AI's Klimaatcrisis: Verbranden we de Aarde om Onze Digitale Breinen te Voeden? – INDEED, geraadpleegd op 23 juli 2025, <https://www.indeed-innovation.com/the-mensch/ais-climate-crisis-are-we-burning-the-planet-to-feed-our-digital-brains/>
Groene AI Position Paper | GSF - Green Software Foundation, geraadpleegd op 23 juli 2025, <https://greensoftware.foundation/articles/green-ai-position-paper/>
AI en klimaatverandering – Energie en AI – Analyse - IEA, geraadpleegd op 23 juli 2025, <https://www.iea.org/reports/energy-and-ai/ai-and-climate-change>
Desinformatie door Weglaten: De Noodzaak voor Meer ... - arXiv, geraadpleegd op 23 juli 2025, <http://arxiv.org/pdf/2506.15572>
Desinformatie door Weglaten: De Noodzaak voor Meer Milieu Transparantie in AI - arXiv, geraadpleegd op 23 juli 2025, <https://arxiv.org/html/2506.15572v1>

---
*Vertaald door AI. Controleer de originele Engelse versie bij twijfel.*