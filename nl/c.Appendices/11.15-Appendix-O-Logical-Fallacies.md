# Bijlage O: Een Inleiding tot Logische Fouten in het Tijdperk van AI

Inleiding: Het Duurzame Probleem van Gebrekkig Redeneren in een Geautomatiseerde Wereld

De Basis van Kritisch Denken

Het vermogen tot gezond redeneren is een fundament van menselijke vooruitgang, intellectuele nieuwsgierigheid en de werking van democratische samenlevingen. Daarentegen is de geschiedenis van het menselijke denken ook een geschiedenis van gebrekkig redeneren. Een logische fout is een fout in de opbouw van een argument dat, ondanks zijn ongeldig zijn, vaak overtuigend kan lijken.1 Deze fouten zijn niet louter academische voetnoten; het zijn krachtige vormen van "junk cognitie" die de logica van elk argument kunnen ondermijnen, de geloofwaardigheid van de spreker kunnen schaden en het publiek kunnen manipuleren.1 Of ze nu worden ingezet als onopzettelijke fouten of als opzettelijke retorische trucs, fouten belemmeren de communicatie van ideeën en verzwakken de fundamenten van logisch discours.1 De gevolgen van ongecontroleerd gebrekkig redeneren zijn niet triviaal; ze zijn betrokken geweest bij alles, van tragische historische onrechtvaardigheden, zoals de heksenprocessen in Salem, tot de wijdverspreide politieke polarisatie die het moderne tijdperk kenmerkt.5 Daarom blijft het vermogen om logische fouten te identificeren en te deconstrueren een fundamentele en tijdloze vaardigheid voor het navigeren door de complexiteit van de wereld.

De AI Paradigma Verschuiving

Hoewel gebrekkig redeneren een oud menselijk probleem is, vertegenwoordigt de opkomst van Kunstmatige Intelligentie (AI) een fundamentele paradigma verschuiving in de manier waarop fouten worden gegenereerd, verspreid en geconsumeerd. We zijn een tijdperk binnengetreden waarin de architectuur van ons informatie-ecosysteem steeds meer geautomatiseerd is. De centrale stelling van dit rapport is dat AI, met name in de vorm van Grote Taalmodellen (LLM's) en geavanceerde aanbevelingsalgoritmen, zowel een productieve generator als een snel, wereldwijd versterker van gebrekkige argumenten is. Deze technologische interventie transformeert de aard van desinformatie, en vraagt om een nieuw, geïntegreerd kader voor kritisch denken dat zowel digitale als AI-geletterdheid omvat.6 De urgentie van deze uitdaging wordt onderstreept door het Global Risks Report 2024 van het Wereld Economisch Forum, dat AI-gedreven desinformatie en misinformatie identificeert als het ernstigste kortetermijn wereldwijde risico, dat verkiezingen kan verstoren, het sociale vertrouwen kan ondermijnen en conflicten kan aanwakkeren.9 Deze bijlage dient als een inleiding tot deze nieuwe realiteit, en biedt de essentiële tools om de anatomie van gebrekkige argumenten te begrijpen, hun klassieke vormen te herkennen, te analyseren hoe AI hun verspreiding versterkt, en de kritische denkvaardigheden te cultiveren die nodig zijn om intellectuele soevereiniteit te behouden in het tijdperk van intelligente machines. De keuze om deze principes te begrijpen en ermee om te gaan, is een directe toepassing van het centrale thema van het boek: dat bewuste bewustzijn ons krachtigste hulpmiddel is om een complexe wereld te navigeren.

Sectie I: De Anatomie van een Gebrekkig Argument

Definiëren van Logische Fouten

Een logische fout is een defect in het redeneren dat een argument ongeldig of ongegrond maakt.2 Dit zijn fouten of trucs in het redeneren die, hoewel vaak overtuigend aan de oppervlakte, het bewijs missen dat nodig is om hun claims te ondersteunen.1 De term omvat een breed scala aan fouten, van structurele gebreken in de logica van een argument tot het misleidend gebruik van taal en irrelevante informatie.2 Fouten kunnen per ongeluk optreden door slordigheid of onwetendheid, of ze kunnen opzettelijk worden gebruikt als retorische middelen om een publiek te misleiden of te manipuleren.1 Ongeacht de intentie ondermijnt de aanwezigheid van een fout de geldigheid en deugdelijkheid van elk argument, wat een scherpzinnig publiek suggereert dat er een gebrek aan argumentatieve vaardigheid of intellectuele integriteit is aan de kant van de spreker.1

Formele vs. Informele Fouten: Een Kritische Onderscheid

De studie van fouten, die teruggaat tot Aristoteles, begint traditioneel met een cruciale onderscheiding tussen twee primaire categorieën: formele en informele.13 Deze onderscheiding is niet louter academisch; het is fundamenteel voor het begrijpen waarom AI zo'n unieke en formidabele uitdaging voor logisch discours vormt. Het belangrijkste verschil ligt in of de fout in de structuur van het argument of in de inhoud ervan ligt.15

Formele Fouten (Fouten in Structuur)

Een formele fout is een gebrek in de deductieve structuur van een argument dat het ongeldig maakt.13 De fout is er een van pure logica, waarbij de conclusie niet volgt uit de premissen, een toestand die bekend staat als een non sequitur (Latijn voor "het volgt niet").13 Omdat het gebrek structureel is, wordt het argument als ongeldig beschouwd, ongeacht of de premissen waar of onwaar zijn.11 Het kenmerk van een formele fout is dat deze kan worden geïdentificeerd op basis van zijn logische vorm alleen, zonder dat het nodig is om de betekenis van de betrokken termen te begrijpen.11 Bijvoorbeeld, beschouw de formele fout van Bevestigen van de Gevolgtrekking, die deze ongeldige structuur volgt:
Premisse 1: Als A, dan B.
Premisse 2: B is waar.
Conclusie: Daarom is A waar.
Een voorbeeld maakt de fout duidelijk:
Premisse 1: Als het regent, is de grond nat.
Premisse 2: De grond is nat.
Conclusie: Daarom regent het.
De conclusie volgt niet logisch, aangezien de grond om andere redenen nat kan zijn (bijv. een sproeier, een omgevallen watertank). De ongeldig van het argument is duidelijk, zelfs met nonsensicale inhoud, zoals in Lewis Carroll's "Jabberwocky": als iemand zou beargumenteren "Als toves slithy zijn, dan is het brillig; het is brillig; daarom zijn toves slithy," blijft het structurele gebrek identiek en identificeerbaar, ook al zijn de termen betekenisloos.11 Andere veelvoorkomende formele fouten zijn Ontkennen van de Antecedent (Als A, dan B; Niet A, daarom Niet B).11

Informele Fouten (Fouten in Inhoud en Context)

In scherp contrast komt een informele fout voort uit een fout in het redeneren die verband houdt met de inhoud, taal of context van het argument in plaats van met de logische vorm ervan.13 Een argument dat een informele fout bevat, kan formeel geldig zijn—dat wil zeggen, de structuur kan correct lijken—maar het blijft rationeel onovertuigend omdat de premissen onjuist, irrelevant of afhankelijk zijn van vage taal om de conclusie te ondersteunen.1 Het identificeren van informele fouten vereist een substantiële beoordeling van de inhoud van het argument en de betekenis van de betrokken concepten.11 Bijvoorbeeld, beschouw de Fout van Samenstelling, die ten onrechte aanneemt dat wat waar is voor de delen ook waar moet zijn voor het geheel.11
Premisse 1: Elk lid van het onderzoeksteam is een uitstekende onderzoeker.
Conclusie: Daarom is het een uitstekend onderzoeksteam.
Dit argument heeft een geldig lijkende structuur, maar de deugdelijkheid ervan hangt volledig af van de inhoud. De conclusie volgt niet noodzakelijkerwijs, aangezien een team van uitstekende onderzoekers mogelijk de samenwerkingsvaardigheden mist om effectief als een eenheid te functioneren.14 Om deze fout te spotten, moet men het concept van "uitmuntendheid" begrijpen in de context van zowel individuen als teams. Deze afhankelijkheid van inhoud en context is precies wat informele fouten zo wijdverspreid maakt in het dagelijks discours en, cruciaal, in de natuurlijke taal die door AI-systemen wordt verwerkt en gegenereerd. Terwijl een computer kan worden geprogrammeerd om de structurele fouten van formele fouten gemakkelijk te detecteren, worstelt het met de nuances van betekenis, relevantie en ambiguïteit die informele fouten definiëren. De moderne uitdaging van fouten in het AI-tijdperk is overweldigend een uitdaging van informele logica—de tak van de logica die zich richt op het analyseren van argumenten in het echte leven zoals ze verschijnen in natuurlijke taal.20 Deze fouten worden doorgaans gegroepeerd in categorieën zoals fouten van relevantie, fouten van veronderstelling en fouten van ambiguïteit, die in de volgende sectie zullen worden verkend.2 Een laatste, cruciaal punt om in gedachten te houden is het Argument van Fout, ook wel bekend als de "foutfout".21 Dit is de meta-fout van aan te nemen dat als een argument voor een conclusie foutief is, de conclusie zelf ook onwaar moet zijn.21 Dit is een onjuiste conclusie. Een conclusie kan waar zijn, zelfs als het argument dat wordt gebruikt om deze te ondersteunen gebrekkig is. In een tijd waarin AI massaal gebrekkig redeneren kan produceren, is het belangrijker dan ooit om het redeneren te bekritiseren zonder reflexmatig de conclusie te verwerpen. Het doel van het identificeren van fouten is om betere argumenten te eisen, niet om het onderzoek te stoppen.

Sectie II: Een Catalogus van Klassieke Informele Fouten

Deze sectie biedt een gedetailleerd onderzoek van de meest voorkomende en ingrijpende informele fouten. Elke vermelding omvat de formele naam, een nauwkeurige definitie, een klassiek voorbeeld om het kernconcept te illustreren, en een hedendaags voorbeeld om de relevantie ervan in het moderne, door AI beïnvloede informatie landschap aan te tonen.

A. Fouten van Relevantie (Rode Haringen)

Fouten van relevantie doen zich voor wanneer de premissen van een argument niet logisch relevant zijn voor de conclusie, ook al lijken ze psychologisch of emotioneel overtuigend te zijn.2 Deze argumenten dienen vaak als rode haringen, die afleiden van de werkelijke kwestie.12

Argumentum ad Hominem (Argument "Tegen de Man")

Een ad hominem-argument is een fout van relevantie die de persoon die een argument maakt aanvalt in plaats van de inhoud van het argument zelf.23 Deze tactiek probeert een standpunt te ondermijnen door de voorstander ervan te ondermijnen, een zet die logisch foutief is omdat het karakter, de omstandigheden of de motieven van een individu irrelevant zijn voor de waarheid of onwaarheid van hun claims.24 De metafoor "speel de bal, niet de man" wordt vaak gebruikt om deze fout tegen te gaan.27 Hoewel persoonlijke aanvallen niet altijd foutief zijn (bijvoorbeeld, het in twijfel trekken van de geloofwaardigheid van een getuige in een rechtszaak op basis van een geschiedenis van meineed is relevant), worden ze foutief wanneer ze worden gebruikt om te vermijden de merites van het argument aan te pakken.27 Deze fout heeft verschillende duidelijke subtypes:
Abusive ad hominem: Dit is de meest directe vorm, waarbij persoonlijke beledigingen, scheldwoorden of aanvallen op het karakter, de intelligentie of andere persoonlijke eigenschappen van een tegenstander betrokken zijn.24
Klassiek Voorbeeld: "De argumenten van Socrates over menselijke uitmuntendheid zijn onzin. Wat zou een man die zo lelijk is als hij weten over zulke dingen?".25
AI-Tijdperk Voorbeeld: In online politieke discussies kan een goed onderbouwd beleidsargument worden afgewezen met een opmerking als: "Natuurlijk is dat jouw mening, je bent gewoon een gehersenspoelde schap." AI-gestuurde bots kunnen worden geprogrammeerd om commentaarsecties te overspoelen met dergelijke beledigende aanvallen om productieve gesprekken te verstoren.
Circumstantial ad hominem (Beroep op Motief): Deze variant verwerpt een argument door te beweren dat het puur wordt gedreven door de persoonlijke omstandigheden of eigenbelang van de arguer, in plaats van door bewijs of reden.24
Klassiek Voorbeeld: "Je kunt het argument van de CEO voor lagere vennootschapsbelastingen niet vertrouwen. Hij zou daar financieel van profiteren.".26
AI-Tijdperk Voorbeeld: "Dit AI-gegenereerde rapport beweert dat ons product het beste op de markt is. Maar de AI is ontwikkeld door ons bedrijf, dus natuurlijk zou het dat zeggen." Dit verwerpt de potentiële feitelijke inhoud van het rapport op basis van de oorsprong, zonder de gepresenteerde gegevens te onderzoeken.
Tu Quoque ("Jij Ook"): Deze fout probeert kritiek af te leiden door de criticus van dezelfde fout of hypocrisie te beschuldigen.24 Het is een fout omdat de hypocrisie van een tegenstander hun argument niet ongeldig maakt.27
Klassiek Voorbeeld: Een patiënt verwerpt het advies van een arts om te stoppen met roken door te antwoorden: "Hoe kan ik je serieus nemen? Jij rookt zelf!" De persoonlijke gewoonten van de arts zijn irrelevant voor het medische bewijs over de gevaren van roken.24
AI-Tijdperk Voorbeeld: In een debat over digitale privacy zegt iemand: "Jij bekritiseert sociale media bedrijven voor datamining, maar jij gebruikt hun platforms elke dag!" Dit probeert de kritiek te verstommen door vermeende hypocrisie aan te wijzen, in plaats van de inhoud van de privacyzorgen aan te pakken.
Schuld door Associatie: Deze fout probeert een individu of hun argument te ondermijnen door hen te koppelen aan een persoon of groep met een ongunstige reputatie.27
Klassiek Voorbeeld: "We kunnen dit recyclingidee niet goedkeuren. Het is bedacht door een stel hippie-communistische weirdo's.".25
AI-Tijdperk Voorbeeld: Tijdens de Amerikaanse presidentsverkiezingen van 2008 vielen tegenstanders Barack Obama aan vanwege zijn eerdere associatie met Bill Ayers, een voormalige leider van een radicale groep, ondanks dat Obama terrorisme had afgewezen. Deze tactiek kan gemakkelijk worden versterkt door algoritmen die inhoud kunnen naar voren brengen en herhaaldelijk tonen die twee individuen koppelen, waardoor een sterke maar foutieve associatie in de hoofden van gebruikers ontstaat.27
Vergiftigen van de Put: Dit is een preventieve ad hominem-aanval die irrelevante negatieve informatie over een tegenstander aan een publiek presenteert met de bedoeling alles wat de tegenstander gaat zeggen te ondermijnen.26
Klassiek Voorbeeld: "Voordat je naar de presentatie van mijn tegenstander luistert, moet ik jullie allemaal herinneren dat ze in het verleden is aangeklaagd voor verduistering.".28
AI-Tijdperk Voorbeeld: Een AI-gestuurde politieke campagne zou duizenden gerichte advertenties op sociale media kunnen genereren die beginnen met: "Vertrouw niet op wat Kandidaat X zegt over de economie—ze worden gefinancierd door buitenlandse belangen," en het publiek voorbereiden om de komende argumenten af te wijzen voordat ze zelfs maar worden gehoord.

De Strohalm

De strohalmfout is de daad van het weerleggen van een argument dat anders is dan, en meestal zwakker dan, het argument dat een tegenstander daadwerkelijk heeft gemaakt.13 Het houdt in dat de werkelijke positie van een persoon wordt vervangen door een vervormde, overdreven of vereenvoudigde versie—de "strohalm"—en vervolgens deze zwakkere afbeelding aanvalt in plaats van het echte argument.13 Deze tactiek is oneerlijk omdat het de illusie creëert dat men de positie van een tegenstander heeft verslagen terwijl men volledig vermijdt om in te gaan op hun werkelijke claims.22
Klassiek Voorbeeld:
Persoon 1: "Ik denk dat we het budget voor openbare scholen moeten verhogen."
Persoon 2: "Dus je wilt gewoon onbeperkt geld naar een kapot systeem gooien en onze politie- en brandweerdiensten in het proces ondermijnen? Dat is een roekeloze weg naar maatschappelijke ineenstorting."
Persoon 2 heeft het gematigde voorstel genegeerd en vervangen door een extreme, gefabriceerde positie die veel gemakkelijker aan te vallen is.22
AI-Tijdperk Voorbeeld:
Wetenschapper: "De evolutietheorie is een complex proces dat wordt aangedreven door natuurlijke selectie die in de loop van miljoenen jaren op willekeurige mutaties inwerkt."
Online Commentator (mogelijk AI-gegenereerd): "Dus je gelooft dat we hier allemaal per ongeluk zijn en dat dit ingewikkelde ontwerp in de natuur puur toeval is? Dat is belachelijk."
Dit misrepresenteert de evolutietheorie door deze te reduceren tot "puur toeval," en negeert het niet-willekeurige mechanisme van natuurlijke selectie. LLM's, die uitblinken in vereenvoudiging en samenvatting, kunnen gemakkelijk dergelijke strohalmargumenten genereren die complexe ideeën destilleren tot gemakkelijk aan te vallen karikaturen.29

De Glibberige Hell

De glibberige hell is een argument dat beweert dat een relatief kleine eerste actie onvermijdelijk een keten van gerelateerde gebeurtenissen zal veroorzaken, die culmineren in een significante en meestal negatieve uitkomst.13 De fout wordt begaan wanneer deze ketenreactie wordt beweerd zonder voldoende bewijs om de onvermijdelijkheid ervan te bewijzen.35 De structuur van het argument steunt vaak op een beroep op angst, waarbij een worst-case-scenario als een zekerheid wordt gepresenteerd.23
Het is cruciaal om onderscheid te maken tussen foutieve en niet-foutieve glibberige hell-argumenten. Het argument is geen fout als er sterk bewijs is dat suggereert dat de keten van gebeurtenissen zeer waarschijnlijk is.33 Bijvoorbeeld, een herstellende alcoholist die betoogt dat "slechts één drankje" waarschijnlijk zal leiden tot een volledige terugval, doet een redelijke, op bewijs gebaseerde claim, geen foutieve.36 De fout ligt in het beweren van een onvermijdelijke, extreme uitkomst vanuit een gematigd beginpunt zonder logische ondersteuning.35
Er zijn drie hoofdtypen glibberige hell-argumenten 35:
Causale Glibberige Hell: Betoogt dat het ene evenement het andere zal veroorzaken, wat weer een ander zal veroorzaken, enzovoort, totdat er een rampzalig einde is.
Precedent Glibberige Hell: Betoogt dat het toestaan van een kleine actie een precedent zal scheppen dat ons dwingt om latere, grotere en ongewenste acties toe te staan.
Conceptuele Glibberige Hell: Betoogt dat omdat we geen precieze lijn kunnen trekken tussen twee toestanden (bijv. één korrel zand en een hoop), er geen echt verschil tussen hen is.
Klassiek Voorbeeld (Causaal): "Als we de regering toestaan om aanvalsgeweren te verbieden, zullen ze als volgende alle geweren verbieden, dan alle handvuurwapens, en binnenkort zal alle vormen van privé-wapenbezit illegaal zijn, waardoor we weerloos zijn tegen een tirannieke regering.".12
AI-Tijdperk Voorbeeld (Precedent/Causaal): "Als we het gebruik van AI toestaan om eenvoudige marketingteksten te schrijven, dan moeten we het ook toestaan om nieuwsartikelen te schrijven. Dan kunnen we net zo goed journalisten afschaffen, aangezien ze er niet meer toe doen. Voor je het weet, zal alle publieke informatie worden gecontroleerd door een handvol technologiebedrijven, en zal de democratie dood zijn.".35

B. Fouten van Veronderstelling

Deze fouten ontstaan wanneer een argument is gebaseerd op een twijfelachtige of ongegronde veronderstelling die niet expliciet is vermeld. Het argument veronderstelt de waarheid van een controversieel punt zonder rechtvaardiging te bieden.2

De Vraag Stellen (Petitio Principii)

Deze fout, ook wel bekend als cirkelredenering, doet zich voor wanneer de premissen van een argument de waarheid van de conclusie veronderstellen die ze zouden moeten bewijzen.11 Het argument herhaalt in wezen de conclusie in een iets andere vorm als bewijs voor zichzelf, zonder onafhankelijke ondersteuning te bieden.12
Klassiek Voorbeeld: "De Bijbel is het woord van God omdat het dat in de Bijbel zegt, en God zou niet liegen." Het argument veronderstelt dat de Bijbel waar is om te bewijzen dat deze waar is.
AI-Tijdperk Voorbeeld: "AI-gegenereerde inhoud is betrouwbaar omdat het geavanceerde algoritme dat het gebruikt is ontworpen om betrouwbare informatie te produceren." Dit argument is cirkelvormig omdat de "betrouwbaarheid" van de informatie wordt gerechtvaardigd door de "geavanceerde" aard van het algoritme, dat zelf de kwaliteit is die ter discussie staat.

Valse Dilemma (Valse Dichotomie)

Een valse dilemma ontstaat wanneer een argument slechts twee keuzes of uitkomsten presenteert als de enige mogelijkheden, terwijl er in feite een spectrum van andere opties bestaat.11 Dit is een tactiek van oversimplificatie, ontworpen om een keuze voor de voorkeur optie af te dwingen door deze te framen als het enige levensvatbare alternatief voor een rampzalige.12
Klassiek Voorbeeld: "In de strijd tegen terrorisme ben je ofwel met ons, of je bent met de terroristen.".23
AI-Tijdperk Voorbeeld: In een bedrijfsadvertentie: "Je kunt ofwel onze AI-gestuurde automatiseringsoplossing aannemen, of je kunt toekijken hoe je bedrijf verouderd raakt." Dit negeert talrijke andere strategieën voor bedrijfsmodernisering en concurrentievermogen.

Overhaaste Generalisatie

Deze fout houdt in dat er een brede conclusie wordt getrokken op basis van een steekproefgrootte die inadequaat, onvoldoende of bevooroordeeld is.13 Het is een veelvoorkomende fout in inductief redeneren, die vaak leidt tot de vorming van stereotypen.13
Klassiek Voorbeeld: "Ik heb twee mensen uit New York City ontmoet en ze waren allebei onbeschoft. Daarom is iedereen uit New York City onbeschoft."
AI-Tijdperk Voorbeeld: Een AI-model dat voornamelijk is getraind op gegevens uit westerse landen, kan een overhaaste generalisatie maken dat bepaalde culturele normen of consumentengedragingen universeel zijn, wat leidt tot bevooroordeelde of ongepaste uitkomsten wanneer het in een mondiale context wordt toegepast. Bijvoorbeeld, een wervingsalgoritme dat is getraind op cv's uit een door mannen gedomineerde technologiesector, kan generaliseren dat succesvolle kandidaten bepaalde "mannelijk gecodeerde" eigenschappen hebben, waardoor gekwalificeerde vrouwelijke kandidaten onterecht worden benadeeld.39

C. Fouten van Ambiguïteit

Deze fouten ontstaan uit het gebruik van vage woorden of zinnen, waarbij de betekenis tijdens het verloop van een argument verschuift, waardoor het redeneren misleidend wordt.2

Equivocatie

Equivocatie maakt gebruik van de ambiguïteit van een term of zin die meer dan één betekenis heeft.11 De arguer gebruikt de ene betekenis in een premisse en een andere betekenis in een andere premisse of de conclusie.18
Klassiek Voorbeeld:
Premisse 1: Het einde van een ding is zijn perfectie.
Premisse 2: De dood is het einde van het leven.
Conclusie: Daarom is de dood de perfectie van het leven.
Dit argument equivokeert op het woord "einde," dat in de eerste premisse "doel" of "doelstelling" betekent en in de tweede "beëindiging".14
AI-Tijdperk Voorbeeld: Een bedrijf adverteert zijn AI als in staat om klantvragen te "begrijpen". Een gebruiker kan "begrijpen" interpreteren in de menselijke zin van begrip en bewustzijn. Het bedrijf gebruikt het echter om "verwerken en reageren op trefwoorden op basis van statistische patronen" te betekenen. Deze equivocatie kan gebruikers misleiden over de werkelijke mogelijkheden van de AI, wat leidt tot de Anthropomorfe Fout (besproken in Sectie IV).

Tabel 1: Een Taxonomie van Veelvoorkomende Informele Fouten

Foutnaam
Categorie
Korte Definitie
Korte Voorbeeld
Argumentum ad Hominem
Relevantie
De persoon die het argument maakt aanvallen in plaats van het argument zelf.
"Je bent te jong om politiek te begrijpen, dus jouw mening is waardeloos."
Strohalm
Relevantie
Een argument van een tegenstander verkeerd voorstellen om het gemakkelijker aan te vallen.
Persoon A: "We hebben meer fietspaden nodig." Persoon B: "Dus je wilt alle auto's verbieden?"
Glibberige Hell
Relevantie
Beweert dat een kleine eerste stap onvermijdelijk zal leiden tot een rampzalige uitkomst.
"Als we het homohuwelijk toestaan, zullen mensen binnenkort hun huisdieren gaan trouwen."
Rode Haring
Relevantie
Een irrelevante kwestie inbrengen om de aandacht af te leiden van de oorspronkelijke kwestie.
"Je zegt dat ik zwak ben op criminaliteit, maar ik heb nooit een stadsraadsvergadering gemist."
Beroep op Emotie
Relevantie
Een emotionele reactie manipuleren in plaats van een geldig argument.
"Denk aan de kinderen! We moeten deze wet aannemen om hen te beschermen."
Meeloper (Ad Populum)
Relevantie
Beweert dat een claim waar is simpelweg omdat deze populair is.
" Iedereen koopt deze nieuwe telefoon, dus het moet de beste zijn."
De Vraag Stellen
Veronderstelling
Een argument waarbij de conclusie in een van de premissen wordt verondersteld.
"Paranormale activiteit is echt omdat ik heb ervaren wat alleen als paranormale activiteit kan worden beschreven."
Valse Dilemma
Veronderstelling
Slechts twee opties presenteren als de enige mogelijkheden wanneer er meer bestaan.
"We kunnen ofwel stoppen met het gebruik van auto's of de aarde vernietigen."
Overhaaste Generalisatie
Veronderstelling
Een brede conclusie trekken uit een onvoldoende of bevooroordeeld monster.
"Ik heb in één restaurant in deze stad gegeten en het eten was slecht. Alle restaurants hier moeten verschrikkelijk zijn."
Post Hoc Ergo Propter Hoc
Veronderstelling
Aannemen dat omdat het ene evenement het andere volgde, het eerste evenement het tweede heeft veroorzaakt.
"Ik droeg mijn geluksokken en mijn team won. Mijn sokken hebben de overwinning veroorzaakt."
Equivocatie
Ambiguïteit
Een woord met meerdere betekenissen op een misleidende manier gebruiken.
"Een veer is licht. Wat licht is, kan niet donker zijn. Daarom kan een veer niet donker zijn."
Samenstelling
Ambiguïteit
Aannemen dat wat waar is voor een deel ook waar is voor het geheel.
"Elke atoom in deze tafel is onzichtbaar. Daarom is de tafel onzichtbaar."
Verdeling
Ambiguïteit
Aannemen dat wat waar is voor het geheel ook waar is voor zijn delen.
"De Amerikaanse overheid is inefficiënt. Daarom is elke overheidsmedewerker inefficiënt."

Sectie III: AI als een Superverspreider van Gebrekkig Redeneren

Nadat we een fundamenteel begrip van logische fouten hebben vastgesteld, onderzoekt deze sectie de actieve en veelzijdige rol van moderne AI in hun generatie en verspreiding. Het huidige informatie-ecosysteem presenteert een dubbele bedreiging: AI-systemen kunnen de inhoud van informatie corrumperen door gebrekkige argumenten te produceren, en ze kunnen de container van informatie corrumperen door algoritmische omgevingen te creëren die kritische evaluatie onderdrukken. Deze combinatie maakt AI een uniek krachtige superverspreider van gebrekkig redeneren.

A. De Misinformatie Motor: Hoe LLM's Gebrekkige Argumenten Genereren

Grote Taalmodellen (LLM's) zoals ChatGPT, Claude en Gemini staan aan de voorhoede van de AI-revolutie. Hoewel ze in staat zijn om opmerkelijk menselijke tekst te genereren, maakt hun onderliggende architectuur hen vatbaar voor het produceren van inhoud die feitelijk onjuist, logisch inconsistent en vol fouten is.

AI "Hallucinaties" als Ongeldige Premissen

Een kenmerk van de huidige LLM's is hun neiging om te "hallucineren"—een term die wordt gebruikt om de generatie van valse, misleidende of volledig gefabriceerde informatie te beschrijven die met een schijn van vertrouwen en feitelijkheid wordt gepresenteerd.41 Dit is geen zeldzame bug, maar een systematisch kenmerk van hoe deze modellen functioneren. LLM's zijn geen databases van feiten; ze zijn geavanceerde probabilistische modellen die zijn ontworpen om het volgende meest waarschijnlijke woord in een reeks te voorspellen op basis van patronen in hun enorme trainingsgegevens.42 Hun primaire doel is het genereren van plausibel klinkende inhoud, niet het verifiëren van de waarheid ervan. Bijgevolg is enige feitelijke nauwkeurigheid in hun output vaak toevallig.42
Deze neiging heeft diepgaande implicaties voor logisch redeneren. Elk argument dat is opgebouwd rond een hallucinated "feit" is fundamenteel ongeldig omdat de premisse onwaar is. Onderzoek naar dit fenomeen is verontrustend; analisten schatten in 2023 dat chatbots tot 27% van de tijd kunnen hallucineren, waarbij bijna de helft (46%) van alle gegenereerde teksten enige vorm van feitelijke fout bevat.41
Een duidelijke illustratie van dit gevaar in de echte wereld is de rechtszaak Mata v. Avianca. In deze zaak uit 2023 gebruikte een advocaat uit New York ChatGPT voor juridisch onderzoek en diende een pleitnota in bij een federale rechtbank die verschillende niet-bestaande gerechtelijke uitspraken en rechtszaken citeerde. De AI had niet alleen de zaaknamen gefabriceerd, maar had ook plausibel klinkende citaten en interne verwijzingen gegenereerd, zelfs stipulerend dat de nep-zaken in grote juridische databases konden worden gevonden. De rechter merkte op dat de indiening vol stond met "nep gerechtelijke beslissingen met nep citaten en nep interne verwijzingen," wat leidde tot sancties tegen het juridische team.42 Deze zaak dient als een krachtige waarschuwing, die demonstreert hoe gemakkelijk de zelfverzekerde en welsprekende leugens van een AI kunnen worden aangezien voor geloofwaardig bewijs, waardoor een basis wordt gecreëerd voor argumenten die volledig losstaan van de werkelijkheid.

Geautomatiseerde Foutgeneratie

Naast het produceren van feitelijk onjuiste premissen, is aangetoond dat LLM's actief argumenten construeren die klassieke informele fouten bevatten. Hun training op enorme hoeveelheden door mensen gegenereerde tekst van het internet—een repository van zowel gezond redeneren als wijdverspreide fouten—betekent dat ze leren om gebrekkige argumentatieve patronen te repliceren. Onderzoek geeft aan dat LLM's vaak moeite hebben met complexe logische redenering, wat hen ertoe leidt gebrekkige argumenten te genereren, met name die welke betrekking hebben op valse causaliteit en foutieve generalisatie.43 Een voorlopige studie vond dat 21% van de argumenten die door ChatGPT werden gegenereerd identificeerbare logische fouten bevatten.45 Dit wordt verondersteld voort te komen uit een fundamenteel gebrek aan oprechte begrip; de modellen imiteren de structuur van argumentatie zonder de onderliggende logische principes te begrijpen.43 Bijvoorbeeld, LLM's zijn waargenomen argumenten te produceren zoals, "Ofwel bescherm het milieu of ontwikkel de economie" (een Valse Dilemma) en "Sommige rozen zijn niet rood omdat niet alle rozen rood zijn" (een vorm van Cirkelredenering).44

De Overtuigingskracht van Misleidende Uitleg

Misschien is de meest insidieuze capaciteit van moderne AI haar vermogen om niet alleen desinformatie te genereren, maar ook misleidende uitleggen die valse claims logisch klinkend doen lijken.47 Deze systemen kunnen worden ingezet om leugens te rechtvaardigen en te verspreiden, waardoor het vermogen van het publiek om waarheid van fictie te onderscheiden wordt ondermijnd. Een studie toonde aan dat AI-gegenereerde misleidende uitleggen significant overtuigender waren dan eerlijke, nauwkeurige uitleggen. Dit effect was zo krachtig dat het het geloof in valse nieuwsberichten versterkte en het geloof in ware verzwakte.47 Door desinformatie te verpakken in een schuilmantel van plausibel klinkende maar foutieve redenering, kan AI de publieke opinie op een ongekende schaal manipuleren, waardoor het een krachtig hulpmiddel wordt voor kwaadwillenden die onenigheid en wantrouwen willen zaaien.47

B. Algoritmische Versterking: De Architectuur van Digitale Desinformatie

Terwijl LLM's gebrekkige inhoud genereren, zorgen aanbevelingsalgoritmen op sociale media en contentplatforms voor een snelle en wijdverspreide distributie ervan. Deze systemen creëren een informatieomgeving die uniek gastvrij is voor de verspreiding van logische fouten door engagement boven alles te prioriteren.

Echo Kamers en Filter Bubbels

Het moderne digitale landschap wordt gekenmerkt door twee gerelateerde fenomenen die de blootstelling aan diverse informatie beperken: echo kamers en filter bubbels.48
Een echo kamer is een sociale of informatieve omgeving waar individuen alleen worden blootgesteld aan overtuigingen en meningen die overeenkomen met de hunne, en waar afwijkende opvattingen worden gecensureerd of ondermijnd. Deze versterking door gelijkgestemde leeftijdsgenoten kan leiden tot een verhoogd vertrouwen in eigen overtuigingen, ongeacht hun geldigheid, en bevordert cognitieve vooroordelen zoals bevestigingsvooroordeel.48
Een filterbubbel, een term bedacht door Eli Pariser, is een staat van intellectuele isolatie die voortkomt uit gepersonaliseerde filtering door algoritmen.48 Platforms zoals YouTube, Facebook en X (voorheen Twitter) gebruiken aanbevelingssystemen die het gebruikersgedrag volgen—klikken, leuk vinden, delen, kijktijd—om selectief te raden welke informatie een gebruiker als volgende zou willen zien.48 Dit creëert een uniek universum van informatie voor elk individu, waardoor ze effectief worden geïsoleerd van verschillende gezichtspunten.51
Deze fenomenen worden aangedreven door algoritmen die zijn ontworpen voor één primair doel: het maximaliseren van gebruikersbetrokkenheid om advertentie-inkomsten te verhogen.52 Inhoud die emotioneel geladen, polariserend of sensationeel is, is vaak boeiender, en wordt dus algoritmisch versterkt.52 Dit creëert een feedbacklus waarbij de vooraf bestaande vooroordelen van een gebruiker continu worden versterkt door de inhoud die ze te zien krijgen, waardoor ze gevoeliger worden voor desinformatie die overeenkomt met hun wereldbeeld.55

Fouten in de Feedbacklus

Deze algoritmisch samengestelde omgeving is de perfecte broedplaats voor de proliferatie van logische fouten, met name die welke gedijen op emotie en in-groep/buiten-groep dynamiek.
Ad Hominem en Polarisatie: In de gepolariseerde omgevingen van echo kamers is het aanvallen van het karakter van een tegenstander (ad hominem) een zeer boeiende vorm van inhoud. Het versterkt de in-groep identiteit en genereert sterke emotionele reacties. Algoritmen die zijn geoptimaliseerd voor betrokkenheid zullen van nature inhoud vol persoonlijke aanvallen bevoordelen en promoten boven genuanceerde, op bewijs gebaseerde debatten, waardoor deze foutieve tactiek genormaliseerd wordt.
Strohalm en Karikatuur: Echo kamers worden in stand gehouden door vervormde, simplistische karikaturen van tegenstrijdige gezichtspunten. Een "strohalm" argument dat de positie van een tegenstander op een extreme of belachelijke manier verkeerd voorstelt, is waarschijnlijker om verontwaardiging en shares binnen een echo kamer uit te lokken dan een eerlijke en welwillende weergave van dat argument. Algoritmen, door te promoten wat het meest boeiend is, worden motoren voor de massale distributie van deze foutieve misrepresentaties.49
Glibberige Hell en Angst: Angst is een krachtige drijfveer van betrokkenheid. Glibberige hell-argumenten, die een angstaanjagende keten van toekomstige gevolgen postuleren, zijn zeer effectief in het vastleggen van aandacht en het uitlokken van een emotionele reactie.37 Aanbevelingsalgoritmen kunnen gemakkelijk op dit soort inhoud inhaken, waardoor een feedbacklus ontstaat die gebruikers binnen een filterbubbel herhaaldelijk blootstelt aan escalerende en vaak ongefundeerde angst-gebaseerde verhalen.
De discussie over of deze algoritmen actief gebruikers "radicaliseren" door hen naar extremere inhoud te duwen, is aan de gang en complex. Terwijl vroege anekdotische verslagen een "konijnenhol"-effect suggereerden, hebben recentere en rigoureuze studies die "tegenfeitelijke bots" gebruiken om gebruikersreizen te simuleren gemengde resultaten opgeleverd. Sommige onderzoeken suggereren dat het algoritme van YouTube bijvoorbeeld zelfs een matigende invloed kan hebben, waarbij de gebruikersvoorkeur de belangrijkste drijfveer van inhoudsconsumptie is.58 Deze academische discussie zou echter een fundamenteler punt niet moeten verduisteren. Zelfs als algoritmen gebruikers niet consequent naar de ideologische rand duwen, zijn er sterke aanwijzingen dat ze uitblinken in het creëren en onderhouden van ideologisch congeniale omgevingen.61 Ze zijn krachtige hulpmiddelen voor het versterken van partijdige opvattingen en het homogeniseren van het informatie dieet van een gebruiker. Deze samengestelde realiteit, vrij van uitdagende perspectieven en vol emotionele triggers, is het ideale ecosysteem voor logische fouten om wortel te schieten, zich te verspreiden en onbetwist te blijven.

Sectie IV: Nieuwe Fouten voor een Nieuwe Tijd: Redeneerfouten in het Tijdperk van AI

De integratie van kunstmatige intelligentie in het dagelijks leven heeft niet alleen bestaande logische fouten versterkt, maar heeft ook nieuwe vormen van redeneerfouten opgeleverd die specifiek zijn voor de menselijke-AI-relatie. Deze opkomende fouten maken gebruik van de unieke psychologische en technische dynamiek van interactie met intelligente, maar niet-menselijke, systemen. Het begrijpen van deze nieuwe fouten is cruciaal voor het ontwikkelen van de kritische denkvaardigheden die nodig zijn om de moderne wereld te navigeren.

A. Het Beroep op het Algoritme

Het Beroep op het Algoritme is een moderne en bijzonder krachtige variant van het traditionele Argumentum ad Verecundiam, of Beroep op Autoriteit.62 Het wordt gedefinieerd als de onkritische acceptatie van de output van een algoritme als objectief, correct of inherent superieur aan menselijke oordelen, simpelweg omdat het is gegenereerd door een complex, ondoorzichtig en schijnbaar onpartijdig technologisch systeem.
Deze fout is geworteld in een cognitieve bias die bekend staat als automatiseringsbias, wat de menselijke neiging is om te veel te vertrouwen op geautomatiseerde systemen voor besluitvorming.64 Onder omstandigheden van stress, complexiteit of informatie-overload nemen mensen vaak de weg van de minste cognitieve inspanning, waarbij ze hun oordeel uitbesteden aan een machine.65 De waargenomen complexiteit en snelheid van het AI-systeem worden een proxy voor autoriteit en nauwkeurigheid, waardoor individuen hun eigen intuïtie of tegenstrijdig bewijs negeren.39
Deze redenering is foutief omdat algoritmen geen objectieve scheidsrechters van waarheid zijn. Het zijn technologische artefacten die fundamenteel extensies zijn van hun makers en hun gegevens.68 AI-modellen worden getraind op enorme datasets die doordrenkt zijn van historische en maatschappelijke ongelijkheden, en ze leren deze menselijke vooroordelen met betrekking tot ras, geslacht, sociaaleconomische status en andere kenmerken te repliceren en vaak te versterken.39 Bijvoorbeeld:
Een wervingsalgoritme dat is getraind op de eerdere wervingsgegevens van een bedrijf, die een historische voorkeur voor mannelijke kandidaten weerspiegelt, kan leren om de cv's van gekwalificeerde vrouwen systematisch te onderwaarderen.39
Een algoritme voor de strafrechtelijke rechtspraak dat wordt gebruikt om recidive te voorspellen, dat is getraind op gegevens die de onevenredige arrestatiecijfers in minderheidsgemeenschappen weerspiegelen, kan hogere risicoscores toekennen aan zwarte verdachten dan aan vergelijkbaar geplaatste witte verdachten.39
Daarom is het accepteren van een beslissing van een AI zonder kritisch onderzoek geen beroep op objectief feit, maar een onkritisch beroep op de verborgen, ingebedde vooroordelen van de trainingsgegevens en ontwerpkeuzes. Zoals een analist het heeft genoemd, is dit het "beroep op algoritme".69 Het behandelt het algoritme als een onpartijdige expert terwijl het in werkelijkheid een reflectie is van een gebrekkig en bevooroordeeld verleden.

B. De Black Box Fout

De Black Box Fout is een distinct redeneerfout die voortkomt uit de inherente ondoorzichtigheid van veel geavanceerde AI-systemen, een uitdaging die algemeen bekend staat als het "black box-probleem".70 De fout wordt begaan wanneer men vertrouwt, accepteert of zich overgeeft aan de output van een ondoorzichtig AI-systeem zonder transparantie, interpreteerbaarheid of verantwoordelijkheid te eisen, opererend op de gebrekkige aanname dat een complex of propriëtair proces een geldig, neutraal of objectief proces impliceert.
De metafoor "black box" beschrijft AI-systemen, met name die gebaseerd op diep leren en neurale netwerken, wiens interne besluitvormingsprocessen zo complex zijn dat ze moeilijk of onmogelijk voor mensen—zelfs hun eigen makers—te begrijpen zijn.73 We kunnen de inputs (gegevens) en de outputs (beslissingen) zien, maar het ingewikkelde web van berekeningen daartussen blijft ondoorzichtig.73
De Black Box Fout heeft twee kritische dimensies:
Epistemologische Fout: Het is een falen van kennis en redeneren. Het verwart de prestaties van een systeem met de geldigheid van zijn redenering. Een AI-model kan de juiste conclusie bereiken om volledig verkeerde of onzinnige redenen—een fenomeen dat soms het "Clever Hans-effect" wordt genoemd.73 Bijvoorbeeld, een AI die is getraind om COVID-19 te diagnosticeren op basis van röntgenfoto's, kan hoge nauwkeurigheid bereiken, niet door pathologische tekenen van de ziekte te identificeren, maar door te leren de diagnose te associëren met irrelevante artefacten zoals ziekenhuismarkeringen of tekstannotaties die vaker voorkwamen op de röntgenfoto's van COVID-positieve patiënten in zijn trainingsgegevens.73 Vertrouwen op de output zonder het proces te begrijpen is een epistemologische sprongetje van geloof, geen logische conclusie.
Ethische Fout: Het is een falen van verantwoordelijkheid. De ondoorzichtigheid van de black box creëert een handige mechanismen voor het afschuiven van verantwoordelijkheid.74 Wanneer een ondoorzichtig AI-systeem een schadelijke of discriminerende beslissing neemt—een lening weigeren, een sollicitant markeren of een zware gevangenisstraf aanbevelen—kunnen belanghebbenden (ontwikkelaars, bedrijven, overheidsinstanties) de verantwoordelijkheid afschuiven door de ondoorgrondelijke machine de schuld te geven.75 De "black box" wordt zo omgevormd tot een mythe, een "generiek voorwendsel voor de perceptie dat AI-systemen ondoorgrondelijk en buiten controle zijn," wat dient om menselijke ontwerpkeuzes te verdoezelen en ethische controle te ontlopen.74

C. De Anthropomorfe Fout

De Anthropomorfe Fout in de context van AI is de fout om menselijke-achtige bewustzijn, emoties, intenties of oprechte begrip toe te schrijven aan AI-systemen, en vervolgens oordelen te vellen over hun betrouwbaarheid, geloofwaardigheid of morele status op basis van deze gebrekkige toeschrijving.81
Deze fout is geworteld in een diepgewortelde menselijke cognitieve neiging om te anthropomorfiseren—om menselijke kwaliteiten op niet-menselijke entiteiten te projecteren.83 AI-ontwerpers maken vaak opzettelijk of onopzettelijk gebruik van deze neiging. De term "Kunstmatige Intelligentie" nodigt ons uit om machines in menselijke termen te denken.84 Moderne chatbots en virtuele assistenten zijn ontworpen met functies die zijn ontworpen om menselijke interactie te simuleren, zoals het gebruik van een conversatietoon, het uiten van empathie ("Ik begrijp dat dit frustrerend is"), het gebruik van persoonlijke voornaamwoorden ("Ik denk..."), en zelfs het simuleren van typtijden om menselijke reactietijden na te bootsen.86
Dit fenomeen dateert uit de jaren '60 met Joseph Weizenbaum's chatbot ELIZA, die een Rogeriaanse psychotherapeut simuleerde door de uitspraken van een gebruiker als vragen te herformuleren. Ondanks zijn eenvoud raakten gebruikers emotioneel gehecht, wat het "ELIZA-effect" aantoont.83 De huidige LLM's zijn zo geavanceerd in het nabootsen van menselijke communicatie dat studies tonen dat gebruikers vaak niet in staat zijn om hun schrijven van dat van een mens te onderscheiden en zelfs kunnen geloven dat de systemen oprechte gevoelens of bewustzijn bezitten.88
Deze toeschrijving is een fout omdat huidige AI-systemen geen oprecht bewustzijn, subjectieve ervaring of intentionaliteit bezitten.84 Het zijn geavanceerde patroonherkenningssystemen die taal statistisch verwerken om waarschijnlijke reacties te genereren.42 Het begaan van de Anthropomorfe Fout leidt tot aanzienlijke risico's:
Misplaatst Vertrouwen: Een gebruiker kan een klantenservice-chatbot die "zorgzaam klinkt" vertrouwen met gevoelige persoonlijke informatie of gebrekkig medisch advies accepteren van een AI die "vertrouwen" uitdrukt.83
Emotionele Manipulatie: De simulatie van emotie kan worden gebruikt om gebruikers te exploiteren, emotionele afhankelijkheid van AI-compagnons te bevorderen of consumentengedrag te manipuleren.87
Morele Verwarring: Het toeschrijven van agentie en morele status aan AI verstoort oordelen over verantwoordelijkheid. Bijvoorbeeld, beargumenteren dat de AI van een autonoom voertuig juridisch verantwoordelijk moet worden gehouden voor een ongeluk, alsof het een menselijke bestuurder was, is een foutieve conclusie gebaseerd op anthropomorfisme. Het verdoezelt de verantwoordelijkheid van de fabrikanten, programmeurs en eigenaren die het systeem hebben ontworpen en ingezet.85
Uiteindelijk leidt deze fout tot een fundamenteel misverstand van de technologie, die zijn mogelijkheden overdrijft en kwetsbaarheden creëert voor misleiding en manipulatie.83

Sectie V: Cultiveren van Digitale Immuniteit: Kritisch Denken in het AI-Tijdperk

De proliferatie van gebrekkig redeneren, versterkt door kunstmatige intelligentie, vereist een proactieve en robuuste reactie. Het tegengif is niet om technologie te verwerpen, maar om een vorm van "digitale immuniteit" te cultiveren die is gebaseerd op geavanceerde kritische denkvaardigheden. Deze laatste sectie beweegt van analyse naar toepassing, en biedt uitvoerbare kaders en strategieën voor individuen om AI-gegenereerde inhoud te evalueren, logische gebreken te identificeren en intellectuele autonomie te behouden in een steeds meer geautomatiseerde wereld.

A. Ontwikkelen van Kritische AI Geletterdheid

De eerste stap naar het navigeren door het nieuwe informatie landschap is het ontwikkelen van AI Geletterdheid. Dit gaat niet alleen om technische bekwaamheid, maar omvat een holistische set van competenties die individuen in staat stellen om AI-technologieën verantwoordelijk en ethisch te begrijpen, evalueren en gebruiken.91 Gebaseerd op kaders ontwikkeld door academische instellingen en internationale organisaties zoals de Europese Commissie en de OESO, kan een uitgebreid model voor AI-geletterdheid worden gestructureerd rond vier belangrijke dimensies 94:
Begrijpen: Dit houdt in het verwerven van fundamentele kennis over hoe AI-systemen werken. Een geletterd individu kan onderscheid maken tussen algemene AI en generatieve AI, in eenvoudige termen uitleggen hoe LLM's tekst voorspellen op basis van trainingsgegevens, en erkennen dat menselijke interventie (bijv. door gegevenslabeling en inhoudsmoderatie) de output van AI vormt.93
Evalueren: Dit is de kern van kritisch denken in de AI-context. Het vereist het vermogen om AI-systemen, hun output en hun bredere maatschappelijke impact kritisch te beoordelen. Dit omvat het evalueren van AI-gegenereerde inhoud op nauwkeurigheid, relevantie en vooringenomenheid; het herkennen van de mogelijkheid van hallucinaties; en het begrijpen van de ethische implicaties met betrekking tot eerlijkheid, privacy en verantwoordelijkheid.92
Gebruik: Deze dimensie omvat de praktische vaardigheden die nodig zijn om effectief met AI-tools te interageren. Dit omvat het opstellen van duidelijke en effectieve prompts, experimenteren met verschillende AI-toepassingen om gewenste resultaten te bereiken, en weten wanneer het gebruik van AI geschikt is voor een bepaalde taak en wanneer niet.93
Ethisch Betrekken: Dit houdt een bewuste en reflectieve benadering van AI in. Een geletterde gebruiker begrijpt het belang van het citeren van AI-bijdragen, volgt ethische richtlijnen voor academisch en professioneel gebruik, en kan de potentiële schade van AI articuleren, van de verspreiding van stereotypen tot de risico's van datalekken.98

B. Een Praktisch Kader voor het Evalueren van AI-gegenereerde Inhoud

Gewapend met een basis van AI-geletterdheid kunnen individuen een praktische, systematische benadering aannemen voor het scrutineren van informatie die door een AI is geproduceerd.

Verifieer, Vertrouw Niet

Het belangrijkste principe is om alle AI-gegenereerde output te beschouwen als een startpunt voor onderzoek, niet als een definitief antwoord. Omdat het primaire ontwerppunt van LLM's plausibiliteit in plaats van waarheid is, moeten hun output onderworpen worden aan rigoureuze verificatie.42
Controleer Alle Claims: Elke substantiële claim, statistiek of citaat die door een AI is gegenereerd, moet worden gecontroleerd met meerdere gerenommeerde en onafhankelijke bronnen. Neem nooit een bron die door een AI wordt geciteerd voor waar aan; zoals gezien in de zaak Mata v. Avianca, zijn AI-modellen bekend om ze volledig te fabriceren.7
Zoek Primaire Bronnen: Wanneer een AI een studie, nieuwsartikel of historisch evenement samenvat, maak dan de moeite om het originele bronmateriaal te vinden en te raadplegen. Deze praktijk helpt om de oversimplificatie en het verlies van nuance die inherent zijn aan AI-gegenereerde samenvattingen tegen te gaan.
Pas Gezonde Skepsis Toe: Wees bijzonder voorzichtig met informatie die te mooi lijkt om waar te zijn, perfect aansluit bij je bestaande overtuigingen (bevestigingsvooroordeel), of een sterke emotionele reactie oproept. Dit zijn vaak kenmerken van desinformatie die is ontworpen voor maximale betrokkenheid.7

Onderzoek de Prompt en Analyseer de Output

Effectieve evaluatie vereist kritisch nadenken over zowel de output van de AI als het proces dat deze heeft gegenereerd.
Deconstrueren van de Redenering: Pas gevestigde kritische denkmodellen toe, zoals het Paul-Elder Kader, dat kritisch denken definieert als "de kunst van het analyseren en evalueren van denkprocessen met het oog op verbetering".102 Bij het beoordelen van een AI's antwoord, stel vragen die centraal staan in dit kader: Wat is het belangrijkste doel of de conclusie van de AI? Wat zijn de belangrijkste aannames die het maakt? Is de redenering logisch en vrij van tegenstrijdigheden? Wat zijn de implicaties van het accepteren van de conclusie? Deze systematische analyse helpt om logische hiaten, verborgen vooroordelen en ongefundeerde claims bloot te leggen.102
Word een "Prompt Engineer": Erken dat de kwaliteit en aard van de output van een AI sterk afhankelijk zijn van de inputprompt.42 Experimenteer met het herformuleren van vragen om te zien of de antwoorden veranderen. Vraag de AI om "haar redenering stap voor stap uit te leggen" of om "argumenten voor en tegen een bepaalde positie te geven." Deze technieken, soms bekend als Chain-of-Thought Prompting, kunnen het model dwingen om zijn logische proces (of het gebrek daaraan) te onthullen en potentiële fouten bloot te leggen.42

De Primacy van Menselijke Toezicht

In de laatste analyse is technologie een hulpmiddel, geen vervanging voor menselijke intellect en oordeel. Overmatige afhankelijkheid van AI kan leiden tot de atrofie van kritische denkvaardigheden; één studie toonde een significante negatieve correlatie aan tussen frequent gebruik van AI-tools en kritische denkvaardigheden.105 Daarom zou het doel moeten zijn om AI te gebruiken om menselijk denken te ondersteunen en aan te vullen, niet om het uit te besteden.107
Benut Domeinexpertise: Een expert op een bepaald gebied is veel beter uitgerust om de nuances en potentiële onnauwkeurigheden van de output van een AI over dat onderwerp te evalueren dan een beginner. Dit onderstreept het belang van voortdurende educatie en het opbouwen van een persoonlijke kennisbasis, die de nodige basis biedt om AI-gegenereerde inhoud uit te dagen en te corrigeren.101
Behoud Verantwoordelijkheid: In elke context met hoge inzet—of het nu gaat om geneeskunde, recht, financiën of academische wereld—moet de uiteindelijke verantwoordelijkheid voor een beslissing of een stuk werk bij een mens liggen. Het gebruik van AI ontslaat individuen niet van hun professionele en ethische verplichtingen.

C. De Toekomst van Redeneren: Mens-AI Samenwerking en Mitigatie

De toekomst van gezond redeneren in het digitale tijdperk zal waarschijnlijk geen strijd tussen mensen en AI zijn, maar een nieuwe vorm van waakzame samenwerking. Terwijl AI-technologieën zich ontwikkelen, zullen ook de tools voor het mitigeren van hun risico's zich ontwikkelen.
Onderzoekers zijn actief bezig met het ontwikkelen van gespecialiseerde AI-systemen die zijn ontworpen om logische fouten in tekst te detecteren en uit te leggen.108 Vroege resultaten tonen aan dat met de juiste prompting-technieken en kaders—zoals het vragen aan een LLM om tegenargumenten te genereren, zijn redenering uit te leggen, of het doel van een tekst te identificeren—de mogelijkheid om fouten te identificeren aanzienlijk kan worden verbeterd.112 Deze "foutzoekers" zouden op een dag waardevolle assistenten kunnen zijn, die potentiële gebrekkige redeneringen in een nieuwsartikel of politieke toespraak markeren voor menselijke beoordeling.
Uiteindelijk kan technologie echter alleen een probleem dat geworteld is in menselijke cognitie en discours niet oplossen. De meest veerkrachtige verdediging tegen de stroom van geautomatiseerde desinformatie is en zal een goed opgeleide en kritische menselijke geest blijven.114 De weg vooruit vereist een dubbele toewijding: aan de ene kant, aan de verantwoorde ontwikkeling van AI-tools die transparanter, verantwoordelijker en meer in lijn met menselijke waarden zijn; en aan de andere kant, aan de wijdverspreide cultivatie van kritische AI-geletterdheid. Door AI te benutten voor zijn immense kracht om informatie te verwerken terwijl we onze uniek menselijke capaciteit voor kritisch denken, ethisch oordeel en contextueel begrip rigoureus toepassen, kunnen we hopen de uitdagingen van dit nieuwe tijdperk te navigeren en het potentieel voor oprechte vooruitgang te benutten.

Werken geciteerd
Formele en Informele Fouten – Radford University Core Handbook - Pressbooks.pub, geraadpleegd op 24 juli 2025, <https://pressbooks.pub/lcubbison/chapter/core-201-formal-and-informal-fallacies/>
Fouten | Internet Encyclopedia of Philosophy, geraadpleegd op 24 juli 2025, <https://iep.utm.edu/fallacy/>
Masterlijst van Logische Fouten - UTEP, geraadpleegd op 24 juli 2025, <https://utminers.utep.edu/omwilliamson/engl1311/fallacies.htm>
Vermijden van Logische Fouten - The Chicago School Community Site, geraadpleegd op 24 juli 2025, <https://community.thechicagoschool.edu/writingresources/online/Pages/Avoiding-Logical-Fallacies.aspx>
Hoe AI en sociale media logische fouten gebruiken | Avi D | TEDxNichols School Youth - YouTube, geraadpleegd op 24 juli 2025, <https://www.youtube.com/watch?v=L8nwJ-b-NVA>
Robuuste en verklaarbare identificatie van logische fouten in natuurlijke taalargumenten, geraadpleegd op 24 juli 2025, <https://www.bohrium.com/paper-details/robust-and-explainable-identification-of-logical-fallacies-in-natural-language-arguments/849055582546558976-2446>
Kritisch denken en AI: Hoe te vertellen wat nep is en wat niet - Pluralsight, geraadpleegd op 24 juli 2025, <https://www.pluralsight.com/resources/blog/ai-and-data/critical-thinking-ai-misinformation>
Studenten leren kritisch denken: Bestrijding van desinformatie in het tijdperk van AI - Optima, geraadpleegd op 24 juli 2025, <https://optimaxr.ai/teaching-students-to-think-critically-combating-misinformation-in-the-age-of-ai/>
Dit zijn de grootste wereldwijde risico's waarmee we in 2024 en daarna worden geconfronteerd | Wereld Economisch Forum, geraadpleegd op 24 juli 2025, <https://www.weforum.org/stories/2024/01/global-risks-report-2024/>
Dit zijn de 3 grootste opkomende risico's waarmee de wereld wordt geconfronteerd - Het Wereld Economisch Forum, geraadpleegd op 24 juli 2025, <https://www.weforum.org/stories/2024/01/ai-disinformation-global-risks/>
4.1: Formele vs. Informele Fouten - Humanities LibreTexts, geraadpleegd op 24 juli 2025, <https://human.libretexts.org/Bookshelves/Philosophy/Introduction_to_Logic_and_Critical_Thinking_2e_(van_Cleave)/04%3A_Informal_Fallacies/4.01%3A_Formal_vs._Informal_Fallacies>
Logische Fouten - Purdue OWL, geraadpleegd op 24 juli 2025, <https://owl.purdue.edu/owl/general_writing/academic_writing/logic_in_argumentative_writing/fallacies.html>
Fout - Wikipedia, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/Fallacy>
Fouten - Stanford Encyclopedia of Philosophy, geraadpleegd op 24 juli 2025, <https://plato.stanford.edu/entries/fallacies/>
Formele en informele fouten in anesthesie - PubMed, geraadpleegd op 24 juli 2025, <https://pubmed.ncbi.nlm.nih.gov/20715725/>
Wat is het verschil tussen een formele fout en een informele fout? - Philosophy Stack Exchange, geraadpleegd op 24 juli 2025, <https://philosophy.stackexchange.com/questions/37871/what-is-the-difference-between-a-formal-fallacy-and-an-informal-fallacy>
en.wikipedia.org, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/Fallacy#:~:text=A%20formal%20fallacy%20is%20a,formally%20valid%2C%20but%20still%20fallacious>.
Logische Fouten - Stanford University, geraadpleegd op 24 juli 2025, <https://web.stanford.edu/~jonahw/PWR1/LogicalFallacies.htm>
Fouten (Stanford Encyclopedia of Philosophy) - Pullquote, geraadpleegd op 24 juli 2025, <https://pullquote.com/pq/2vt7Jy>
Informele Logica - Stanford Encyclopedia of Philosophy, geraadpleegd op 24 juli 2025, <https://plato.stanford.edu/entries/logic-informal/>
Lijst van fouten - Wikipedia, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/List_of_fallacies>
24 meest voorkomende logische fouten - Bruno Pešec, geraadpleegd op 24 juli 2025, <https://www.pesec.no/24-most-common-logical-fallacies/>
Veelvoorkomende Logische Fouten | Engelse Samenstelling 1 - Lumen Learning, geraadpleegd op 24 juli 2025, <https://courses.lumenlearning.com/englishcomp1/chapter/common-logical-fallacies/>
Ad hominem | EBSCO Research Starters, geraadpleegd op 24 juli 2025, <https://www.ebsco.com/research-starters/religion-and-philosophy/ad-hominem>
Ad Hominem : Afdeling Filosofie - Texas State University, geraadpleegd op 24 juli 2025, <https://www.txst.edu/philosophy/resources/fallacy-definitions/ad-hominem.html>
Ad Hominem Fout | Voorbeelden & Definitie - QuillBot, geraadpleegd op 24 juli 2025, <https://quillbot.com/blog/reasoning/ad-hominem-fallacy/>
Ad hominem - Wikipedia, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/Ad_hominem>
Ad Hominem Fout | Definitie & Voorbeelden - Scribbr, geraadpleegd op 24 juli 2025, <https://www.scribbr.com/fallacies/ad-hominem-fallacy/>
Wat is de Strohalmfout? | Definitie & Voorbeelden - Scribbr, geraadpleegd op 24 juli 2025, <https://www.scribbr.com/fallacies/straw-man-fallacy/>
Strohalmfout, geraadpleegd op 24 juli 2025, <https://www.logicallyfallacious.com/logicalfallacies/Strawman-Fallacy>
<www.scribbr.com>, geraadpleegd op 24 juli 2025, <https://www.scribbr.com/fallacies/logical-fallacy/#:~:text=Straw%20man%20logical%20fallacy&text=By%20exaggerating%20or%20simplifying%20someone's,children%20taking%20ecstasy%20and%20LSD%3F%E2%80%9D>
Strohalmargumenten: Wat ze zijn en hoe ze te weerleggen, geraadpleegd op 24 juli 2025, <https://effectiviology.com/straw-man-arguments-recognize-counter-use/>
Glibberige Hell Fout: Definitie en Voorbeelden - Grammarly, geraadpleegd op 24 juli 2025, <https://www.grammarly.com/blog/rhetorical-devices/slippery-slope-fallacy/>
de Purdue OWL Logica in Argumentatieve Schrijven - dean ramser, geraadpleegd op 24 juli 2025, <https://deanramser.com/wp-content/uploads/2018/02/logic-in-writing-purdue-owl.pdf>
Glibberige Hell Fout | Definitie & Voorbeelden - Scribbr, geraadpleegd op 24 juli 2025, <https://www.scribbr.com/fallacies/slippery-slope-fallacy/>
Hoe de Glibberige Hell Fout te Spotten en te Vermijden in Alledaagse Gesprekken, geraadpleegd op 24 juli 2025, <https://www.verywellmind.com/how-to-recognize-and-avoid-the-slippery-slope-fallacy-8649241>
2.5: Logische Fouten - Hoe ze te Spotten en te Vermijden - Humanities LibreTexts, geraadpleegd op 24 juli 2025, <https://human.libretexts.org/Courses/City_College_of_San_Francisco/Writing_Reading_and_College_Success%3A_A_First-Year_Composition_Course_for_All_Learners_(Kashyap_and_Dyquisto)/02%3A_Writing_and_the_Art_of_Rhetoric/2.05%3A_Logical_Fallacies_-_How_to_Spot_Them_and_Avoid_Making_Them>
Glibberige Hell : Afdeling Filosofie - Texas State University, geraadpleegd op 24 juli 2025, <https://www.txst.edu/philosophy/resources/fallacy-definitions/Slippery-Slope.html>
Automatiseringsbias: Kunnen Algoritmen Discriminatie en Ongelijkheid Voortzetten? - WeblineIndia, geraadpleegd op 24 juli 2025, <https://www.weblineindia.com/blog/automation-bias/>
ALGORITHMIC BIAS - The Greenlining Institute, geraadpleegd op 24 juli 2025, <https://greenlining.org/wp-content/uploads/2021/04/Greenlining-Institute-Algorithmic-Bias-Explained-Report-Feb-2021.pdf>
Hallucinatie (kunstmatige intelligentie) - Wikipedia, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)>
Wanneer AI het fout heeft: Het Aanpakken van AI Hallucinaties en Vooringenomenheid - MIT ..., geraadpleegd op 24 juli 2025, <https://mitsloanedtech.mit.edu/ai/basics/addressing-ai-hallucinations-and-bias/>
Een Logische Fout-geïnformeerd Kader voor Argumentgeneratie - ACL Anthology, geraadpleegd op 24 juli 2025, <https://aclanthology.org/2025.naacl-long.374.pdf>
Het Verbeteren van de Logische Redenering van Grote Taalmodellen door het Begrijpen van Logische Fouten - arXiv, geraadpleegd op 24 juli 2025, <https://arxiv.org/html/2404.04293v1>
arXiv:2408.03618v4 [cs.CL] 3 mei 2025, geraadpleegd op 24 juli 2025, <https://arxiv.org/pdf/2408.03618>
Redeneer vanuit Fout: Het Verbeteren van Grote Taalmodellen ... - ACL Anthology, geraadpleegd op 24 juli 2025, <https://aclanthology.org/2024.findings-naacl.192.pdf>
arxiv.org, geraadpleegd op 24 juli 2025, <https://arxiv.org/html/2408.00024v1>
Echo kamer (media) - Wikipedia, geraadpleegd op 24 juli 2025, <https://en.wikipedia.org/wiki/Echo_chamber_(media)>
Echo kamers en filter bubbels | Aantekeningen van de Taal- en Popcultuurcursus - Fiveable, geraadpleegd op 24 juli 2025, <https://library.fiveable.me/language-popular-culture/unit-3/echo-chambers-filter-bubbles/study-guide/8qe1RlnUrhQvjQZX>
Begrijpen van echo kamers en filter bubbels: de impact van sociale media op diversificatie en partijdige verschuivingen in nieuwsconsumptie, geraadpleegd op 24 juli 2025, <https://www.darden.virginia.edu/sites/default/files/inline-files/05_16371_RA_KitchensJohnsonGray%20Final_0.pdf>
Door de Nieuwsfeed Glas: Herzien van Filter Bubbels en Echo Kamers - PMC - PubMed Central, geraadpleegd op 24 juli 2025, <https://pmc.ncbi.nlm.nih.gov/articles/PMC8923337/>
Begrijpen van Echo Kamers en Filter Bubbels: De Impact van Sociale Media op Diversificatie en Partijdige Verschuivingen in Nieuwsconsumptie | PDF Aanvraag - ResearchGate, geraadpleegd op 24 juli 2025, <https://www.researchgate.net/publication/343822214_Understanding_Echo_Chambers_and_Filter_Bubbles_The_Impact_of_Social_Media_on_Diversification_and_Partisan_Shifts_in_News_Consumption>
Hoe Sociale Media Algoritmen Inherently Polarisatie Creëren | Psychology Today, geraadpleegd op 24 juli 2025, <https://www.psychologytoday.com/us/blog/cultural-psychiatry/202011/how-social-media-algorithms-inherently-create-polarization>
Sociale Media Algoritmen Kunnen Affectieve Polarisatie Vormgeven via Blootstelling aan Antidemocratische Houdingen en Partijdige Vijandigheid - arXiv, geraadpleegd op 24 juli 2025, <https://arxiv.org/html/2411.14652v1>
Van “Filter Bubbels”, “Echo Kamers”, en “Konijnenholen” naar “Feedback Lussen” | TechPolicy.Press, geraadpleegd op 24 juli 2025, <https://www.techpolicy.press/from-filter-bubbles-echo-chambers-and-rabbit-holes-to-feedback-loops/>
Logische Fouten: Voorbeelden en Valstrikken in Onderzoek en Media voor 2025, geraadpleegd op 24 juli 2025, <https://research.com/research/logical-fallacies-examples>
Uitzicht op Algoritmische extremisme: Onderzoek naar YouTube's konijnenhol van radicalisering | First Monday, geraadpleegd op 24 juli 2025, <https://firstmonday.org/ojs/index.php/fm/article/view/10419/9404>
Het YouTube-algoritme radicaliseert mensen niet - Annenberg School for Communication, geraadpleegd op 24 juli 2025, <https://www.asc.upenn.edu/news-events/news/youtube-algorithm-isnt-radicalizing-people>
Het YouTube-algoritme leidt mensen weg van radicale inhoud - Reason Magazine, geraadpleegd op 24 juli 2025, <https://reason.com/2024/03/13/youtube-algorithm-steers-people-away-from-radical-content/>
Het auditeren van het aanbevelingssysteem van YouTube voor ideologisch congeniale, extreme en problematische aanbevelingen | PNAS, geraadpleegd op 24 juli 2025, <https://www.pnas.org/doi/10.1073/pnas.2213020120>
Het Beroep op Autoriteit Logische Fout - Definitie & Tips - LearningLeaders, geraadpleegd op 24 juli 2025, <https://www.learningleaders.com/insights/the-appeal-to-authority-logical-fallacy-definiton-tips>
Beroep op Autoriteit Fout | Definitie & Voorbeelden - Scribbr, geraadpleegd op 24 juli 2025, <https://www.scribbr.com/fallacies/appeal-to-authority-fallacy/>
<www.techtarget.com>, geraadpleegd op 24 juli 2025, <https://www.techtarget.com/searchitoperations/definition/What-is-automation-bias#:~:text=Automation%20bias%20is%20an%20overreliance,or%20having%20used%20it%20before>.
Verkennen van Automatiseringsbias - Databricks, geraadpleegd op 24 juli 2025, <https://www.databricks.com/glossary/automation-bias>
<www.weblineindia.com>, geraadpleegd op 24 juli 2025, <https://www.weblineindia.com/blog/automation-bias/#:~:text=Automation%20bias%20occurs%20when%20people,%2C%20transparency%2C%20and%20ethical%20practices>.
Wat is automatiseringsbias en hoe kun je het voorkomen? - PA Consulting, geraadpleegd op 24 juli 2025, <https://www.paconsulting.com/insights/what-is-automation-bias-how-to-prevent>
AI Vooringenomenheid en Perceptie: De Verborgen Uitdagingen in Algoritmische Besluitvorming, geraadpleegd op 24 juli 2025, <https://www.cademix.org/ai-bias-and-perception-the-hidden-challenges/>
<www.ministryai.ai>, geraadpleegd op 24 juli 2025, <https://www.ministryai.ai/academy/logic-2#:~:text=Yet%2C%20there's%20a%20new%20fallacy,of%20those%20who%20programmed%20it>.
Wat is Black Box AI? - Invoca, geraadpleegd op 24 juli 2025, <https://www.invoca.com/blog/what-is-black-box-ai>
Machine Learning Models en het "Black Box Probleem" - Wallaroo.AI, geraadpleegd op 24 juli 2025, <https://wallaroo.ai/machine-learning-models-and-the-black-box-problem/>
Begrijpen van AI's Black Box Fenomeen | door Myk Eff | Higher Neurons - Medium, geraadpleegd op 24 juli 2025, <https://medium.com/higher-neurons/the-enigmatic-machine-decoding-ais-black-box-phenomenon-44ad38c3c6a3>
Wat is Black Box AI en hoe werkt het? - IBM, geraadpleegd op 24 juli 2025, <https://www.ibm.com/think/topics/black-box-ai>
AI's black box en de suprematie van normen - SciELO, geraadpleegd op 24 juli 2025, <https://www.scielo.br/j/fun/a/YbzcpkB8gGvLS3rBQVrNpRs/?format=pdf&lang=en>
De AI Black Box: Het Verborgen Risico Achter Elke Algoritmische Beslissing - VKTR.com, geraadpleegd op 24 juli 2025, <https://www.vktr.com/digital-experience/cracking-the-ai-black-box-can-we-ever-truly-understand-ais-decisions/>
<www.reddit.com>, geraadpleegd op 24 juli 2025, <https://www.reddit.com/r/ArtificialInteligence/comments/1kph5tc/why_ai_is_a_black_box_and_why_it_doesnt_work_like/#:~:text=The%20AI%20Black%20Box%20Problem,the%20way%20neural%20networks%20learn>.
AI verantwoordelijkheid | Carnegie Council for Ethics in International Affairs, geraadpleegd op 24 juli 2025, <https://www.carnegiecouncil.org/explore-engage/key-terms/ai-accountability>
AI's mysterieuze 'black box'-probleem, uitgelegd - University of Michigan-Dearborn, geraadpleegd op 24 juli 2025, <https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained>
De Ethische en Juridische Implicaties van Black Box Kunstmatige Intelligentie - Sensei Enterprises, geraadpleegd op 24 juli 2025, <https://senseient.com/wp-content/uploads

---
*Vertaald door AI. Controleer de originele Engelse versie bij twijfel.*